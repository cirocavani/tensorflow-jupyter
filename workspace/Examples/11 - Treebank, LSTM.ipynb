{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://arxiv.org/abs/1409.2329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking treebank dataset...\n",
      "Extracting ./simple-examples/data/ptb.test.txt\n",
      "Extracting ./simple-examples/data/ptb.train.txt\n",
      "Extracting ./simple-examples/data/ptb.valid.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "HOME_DIR = 'treebank'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "\n",
    "print('Unpacking treebank dataset...')\n",
    "\n",
    "TAR_FILE = 'simple-examples.tgz'\n",
    "TAR_PATH = os.path.join(DATA_DIR, TAR_FILE)\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.base import maybe_download\n",
    "maybe_download(TAR_FILE, DATA_DIR, 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz')\n",
    "\n",
    "def extract(tar, filename, dst_path):\n",
    "    print('Extracting', filename)\n",
    "    dst_file = os.path.join(dst_path, os.path.basename(filename))\n",
    "    with open(dst_file, 'wb') as fout:\n",
    "        fin = tar.extractfile(filename)\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "with tarfile.open(TAR_PATH, mode='r:gz') as t:\n",
    "    extract(t, './simple-examples/data/ptb.test.txt', DATA_DIR)\n",
    "    extract(t, './simple-examples/data/ptb.train.txt', DATA_DIR)\n",
    "    extract(t, './simple-examples/data/ptb.valid.txt', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple-examples.tgz', 'ptb.test.txt', 'ptb.train.txt', 'ptb.valid.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples...\n",
      "\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "\n",
      "...\n",
      "\n",
      "Validation samples...\n",
      "\n",
      "consumers may want to move their telephones a little closer to the tv set\n",
      "\n",
      "<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk>\n",
      "\n",
      "two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues\n",
      "\n",
      "...\n",
      "\n",
      "Test samples...\n",
      "\n",
      "no it was n't black monday\n",
      "\n",
      "but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos\n",
      "\n",
      "some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show(filename, lines=3):\n",
    "    with open(filename, 'r') as f:\n",
    "        for _ in range(lines):\n",
    "            print(next(f).strip())\n",
    "            print()\n",
    "\n",
    "TRAIN_DATA = os.path.join(DATA_DIR, 'ptb.train.txt')\n",
    "VALID_DATA = os.path.join(DATA_DIR, 'ptb.valid.txt')\n",
    "TEST_DATA = os.path.join(DATA_DIR, 'ptb.test.txt')\n",
    "\n",
    "print('Train samples...\\n')\n",
    "show(TRAIN_DATA)\n",
    "print('...\\n')\n",
    "print('Validation samples...\\n')\n",
    "show(VALID_DATA)\n",
    "print('...\\n')\n",
    "print('Test samples...\\n')\n",
    "show(TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_text_as_tokens(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().replace('\\n', '<eos>').split()\n",
    "\n",
    "train_tokens = read_text_as_tokens(TRAIN_DATA)\n",
    "valid_tokens = read_text_as_tokens(VALID_DATA)\n",
    "test_tokens = read_text_as_tokens(TEST_DATA)\n",
    "\n",
    "train_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:\n",
      "\n",
      "10,000\n",
      "\n",
      "Most common:\n",
      "\n",
      "50,770\tthe\n",
      "45,020\t<unk>\n",
      "42,068\t<eos>\n",
      "32,481\tN\n",
      "24,400\tof\n",
      "23,638\tto\n",
      "21,196\ta\n",
      "18,000\tin\n",
      "17,474\tand\n",
      "9,784\t's\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "tokens_freq = collections.Counter(train_tokens)\n",
    "\n",
    "print('Vocabulary size:\\n\\n{:,d}\\n'.format(len(tokens_freq)))\n",
    "print('Most common:\\n')\n",
    "for token, freq in tokens_freq.most_common(10):\n",
    "    print('{:,d}\\t{}'.format(freq, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aer=9970\n",
      "banknote=9971\n",
      "berlitz=9972\n",
      "calloway=9973\n",
      "centrust=9974\n",
      "cluett=9975\n",
      "fromstein=9976\n",
      "gitano=9977\n",
      "guterman=9978\n",
      "hydro-quebec=9979\n",
      "ipo=9980\n",
      "kia=9981\n",
      "memotec=9982\n",
      "mlx=9983\n",
      "nahb=9984\n",
      "punts=9985\n",
      "rake=9986\n",
      "regatta=9987\n",
      "rubens=9988\n",
      "sim=9989\n"
     ]
    }
   ],
   "source": [
    "# lower ids for most common tokens (first id = 0)\n",
    "tokens = sorted(tokens_freq.items(), key=lambda tokn: tokn[1], reverse=True)\n",
    "token_to_id = dict((tok, i) for i, (tok, _) in enumerate(tokens, 0))\n",
    "\n",
    "train_data = list(token_to_id[tok] for tok in train_tokens)\n",
    "valid_data = list(token_to_id[tok] for tok in valid_tokens)\n",
    "test_data = list(token_to_id[tok] for tok in test_tokens)\n",
    "\n",
    "for tok, i in zip(train_tokens[:20], train_data[:20]):\n",
    "    print('{}={}'.format(tok, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch reader**\n",
    "\n",
    "Batch blocks.\n",
    "\n",
    "* `data`: array of token ids\n",
    "* `batch_size`: number of blocks for each call\n",
    "* `num_steps`: number of tokens per block for each call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pseudo-values\n",
    "data = train_data[:26]\n",
    "batch_size = 4\n",
    "num_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow runtime\n",
    "session = tf.InteractiveSession()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(26,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980,\n",
       "       9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991,\n",
       "       9992, 9993,    2, 8569], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor = tf.convert_to_tensor(data, dtype=tf.int32)\n",
    "print(data_tensor)\n",
    "data_tensor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Size:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = tf.size(data_tensor)\n",
    "print(data_len)\n",
    "data_len.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"floordiv:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = data_len // batch_size\n",
    "print(num_batches)\n",
    "num_batches.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len_trim = batch_size * num_batches\n",
    "print(data_len_trim)\n",
    "data_len_trim.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(?,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980,\n",
       "       9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991,\n",
       "       9992, 9993], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore tail lesser than batch size (trim)\n",
    "data_tensor_trim = data_tensor[:data_len_trim]\n",
    "print(data_tensor_trim)\n",
    "data_tensor_trim.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(4, ?), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9973, 9974, 9975],\n",
       "       [9976, 9977, 9978, 9979, 9980, 9981],\n",
       "       [9982, 9983, 9984, 9985, 9986, 9987],\n",
       "       [9988, 9989, 9990, 9991, 9992, 9993]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batches = tf.reshape(data_tensor_trim, shape=(batch_size, num_batches))\n",
    "print(data_batches)\n",
    "data_batches.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"floordiv_1:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore last block lesser than num_steps + 1\n",
    "num_blocks = (num_batches - 1) // num_steps\n",
    "print(num_blocks)\n",
    "num_blocks.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_producer_Dequeue:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "i = tf.train.range_input_producer(num_blocks, shuffle=False).dequeue()\n",
    "threads += tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StridedSlice:0\", shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.strided_slice(data_batches, begin=(0, i * num_steps), end=(batch_size, (i + 1) * num_steps))\n",
    "x.set_shape((batch_size, num_steps))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StridedSlice_1:0\", shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.strided_slice(data_batches, begin=(0, i * num_steps + 1), end=(batch_size, (i + 1) * num_steps + 1))\n",
    "y.set_shape((batch_size, num_steps))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9970 9971 9972 9973 9974 9975]\n",
      " [9976 9977 9978 9979 9980 9981]\n",
      " [9982 9983 9984 9985 9986 9987]\n",
      " [9988 9989 9990 9991 9992 9993]]\n",
      "\n",
      "X:\n",
      "\n",
      "[[9970 9971]\n",
      " [9976 9977]\n",
      " [9982 9983]\n",
      " [9988 9989]]\n",
      "\n",
      "Y:\n",
      "\n",
      "[[9971 9972]\n",
      " [9977 9978]\n",
      " [9983 9984]\n",
      " [9989 9990]]\n",
      "\n",
      "X:\n",
      "\n",
      "[[9972 9973]\n",
      " [9978 9979]\n",
      " [9984 9985]\n",
      " [9990 9991]]\n",
      "\n",
      "Y:\n",
      "\n",
      "[[9973 9974]\n",
      " [9979 9980]\n",
      " [9985 9986]\n",
      " [9991 9992]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_batches.eval())\n",
    "print()\n",
    "for k in range(num_blocks.eval()):\n",
    "    x_, y_ = session.run([x, y])\n",
    "    print('X:\\n')\n",
    "    print(x_)\n",
    "    print()\n",
    "    print('Y:\\n')\n",
    "    print(y_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "\n",
      "[[9970 9971]\n",
      " [9976 9977]\n",
      " [9982 9983]\n",
      " [9988 9989]]\n",
      "\n",
      "Y:\n",
      "\n",
      "[[9971 9972]\n",
      " [9977 9978]\n",
      " [9983 9984]\n",
      " [9989 9990]]\n",
      "\n",
      "X:\n",
      "\n",
      "[[9972 9973]\n",
      " [9978 9979]\n",
      " [9984 9985]\n",
      " [9990 9991]]\n",
      "\n",
      "Y:\n",
      "\n",
      "[[9973 9974]\n",
      " [9979 9980]\n",
      " [9985 9986]\n",
      " [9991 9992]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final dataset reader\n",
    "\n",
    "def input_batch(data, batch_size, num_steps, name=None):\n",
    "    with tf.name_scope(name, 'input_batch', [data, batch_size, num_steps]):\n",
    "        data_tensor = tf.convert_to_tensor(data, dtype=tf.int32)\n",
    "\n",
    "        # ignore tail lesser than batch size (trim)\n",
    "        data_len = tf.size(data_tensor)\n",
    "        num_batches = data_len // batch_size\n",
    "        data_len_trim = batch_size * num_batches\n",
    "        data_tensor_trim = data_tensor[:data_len_trim]\n",
    "        data_batches = tf.reshape(data_tensor_trim, shape=(batch_size, num_batches))\n",
    "\n",
    "        # ignore last block lesser than num_steps + 1\n",
    "        num_blocks = (num_batches - 1) // num_steps\n",
    "\n",
    "        i = tf.train.range_input_producer(num_blocks, shuffle=False).dequeue()\n",
    "\n",
    "        x = tf.strided_slice(data_batches, begin=(0, i * num_steps), end=(batch_size, (i + 1) * num_steps))\n",
    "        x.set_shape((batch_size, num_steps))\n",
    "\n",
    "        y = tf.strided_slice(data_batches, begin=(0, i * num_steps + 1), end=(batch_size, (i + 1) * num_steps + 1))\n",
    "        y.set_shape((batch_size, num_steps))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.Session(graph=graph) as session:\n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    _data = train_data[:26]\n",
    "    _batch_size = 4\n",
    "    _num_steps = 2\n",
    "    _epoch_size = ((len(_data) // _batch_size) - 1) // _num_steps\n",
    "    \n",
    "    x, y = input_batch(_data, _batch_size, _num_steps)\n",
    "    threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "    \n",
    "    for _ in range(_epoch_size):\n",
    "        x_, y_ = session.run([x, y])\n",
    "        print('X:\\n')\n",
    "        print(x_)\n",
    "        print()\n",
    "        print('Y:\\n')\n",
    "        print(y_)\n",
    "        print()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "del graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class PTBModel:\n",
    "    \n",
    "    #_logits\n",
    "    #_initial_state\n",
    "    #_final_state\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_data,\n",
    "                 batch_size,\n",
    "                 num_steps,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 vocab_size):        \n",
    "        lstm_cells = [\n",
    "            self._lstm_reg(tf.contrib.rnn.BasicLSTMCell(hidden_size,\n",
    "                                                        forget_bias=0.0,\n",
    "                                                        state_is_tuple=True,\n",
    "                                                        reuse=tf.get_variable_scope().reuse))\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(lstm_cells, state_is_tuple=True)\n",
    "        \n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        embedding = tf.get_variable('embedding', [vocab_size, hidden_size], dtype=tf.float32)\n",
    "        inputs = self._input_reg(tf.nn.embedding_lookup(embedding, input_data))\n",
    "        \n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        self._final_state = state\n",
    "        \n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        softmax_w = tf.get_variable('softmax_w', [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable('softmax_b', [vocab_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self._logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "    \n",
    "    def _lstm_reg(self, lstm_cell):\n",
    "        return lstm_cell\n",
    "    \n",
    "    def _input_reg(self, inputs):\n",
    "        return inputs\n",
    "    \n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self._logits\n",
    "    \n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "class PTBTrain(PTBModel):\n",
    "\n",
    "    #_is_training\n",
    "    #_epoch_size\n",
    "    #_batch_size\n",
    "    #_num_steps\n",
    "    #_keep_prob\n",
    "\n",
    "    #_cost\n",
    "    #_lr\n",
    "    #_train_op\n",
    "    #_new_lr\n",
    "    #_lr_update\n",
    "\n",
    "    def __init__(self,\n",
    "                 is_training,\n",
    "                 input_data,\n",
    "                 targets,\n",
    "                 epoch_size,\n",
    "                 batch_size,\n",
    "                 num_steps,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 vocab_size,\n",
    "                 keep_prob,\n",
    "                 learning_rate,\n",
    "                 max_grad_norm):\n",
    "        self._is_training = is_training\n",
    "        self._epoch_size = epoch_size\n",
    "        self._batch_size = batch_size\n",
    "        self._num_steps = num_steps\n",
    "        self._keep_prob = keep_prob\n",
    "        \n",
    "        PTBModel.__init__(self,\n",
    "                          input_data,\n",
    "                          batch_size,\n",
    "                          num_steps,\n",
    "                          num_layers,\n",
    "                          hidden_size,\n",
    "                          vocab_size)\n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.logits,\n",
    "            targets,\n",
    "            tf.ones((batch_size, num_steps), dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True\n",
    "        )\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        if not is_training:\n",
    "            self._train_op = tf.no_op()\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(learning_rate, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[], name='new_learning_rate')\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "    def _lstm_reg(self, lstm_cell):\n",
    "        if self._is_training and self._keep_prob < 1:\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=self._keep_prob)\n",
    "        return lstm_cell\n",
    "    \n",
    "    def _input_reg(self, inputs):\n",
    "        if self._is_training and self._keep_prob < 1:\n",
    "            return tf.nn.dropout(inputs, self._keep_prob)\n",
    "        return inputs\n",
    "    \n",
    "    def lr(self, session):\n",
    "        return session.run(self._lr)\n",
    "    \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    def run_epoch_train(self, session, verbose=True):\n",
    "        return self._run_epoch(session, verbose)\n",
    "\n",
    "    def run_epoch_eval(self, session, verbose=False):\n",
    "        return self._run_epoch(session, verbose)\n",
    "\n",
    "    def _run_epoch(self, session, verbose):\n",
    "        batch_size = self._batch_size\n",
    "        num_steps = self._num_steps\n",
    "        epoch_size = self._epoch_size\n",
    "        \n",
    "        start_time = time.time()\n",
    "        costs = 0.0\n",
    "        iters = 0\n",
    "        state = session.run(self.initial_state)\n",
    "\n",
    "        for step in range(epoch_size):\n",
    "            fetches = [self._cost, self.final_state, self._train_op]\n",
    "\n",
    "            feed_dict = {}\n",
    "            for k, (c, h) in enumerate(self.initial_state):\n",
    "                feed_dict[c] = state[k].c\n",
    "                feed_dict[h] = state[k].h\n",
    "\n",
    "            batch_cost, state, _ = session.run(fetches, feed_dict)\n",
    "            costs += batch_cost\n",
    "            iters += num_steps\n",
    "\n",
    "            if verbose and step % (epoch_size // 10) == 10:\n",
    "                print('{:.1f}% perplexity: {:,.3f} speed: {:,.0f} wps'.format(\n",
    "                    step * 100.0 / epoch_size,\n",
    "                    np.exp(costs / iters),\n",
    "                    iters * batch_size / (time.time() - start_time)))\n",
    "\n",
    "        return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def params(self, *keys):\n",
    "        return {\n",
    "            k: v\n",
    "            for k, v in self.__class__.__dict__.items()\n",
    "            if not k.startswith('__') and (not keys or k in keys)\n",
    "        }\n",
    "    \n",
    "class TinyConfig(Config):\n",
    "    '''Tiny config, for testing.'''\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 2\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "class SmallConfig(Config):\n",
    "    '''Small config.'''\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = TinyConfig()\n",
    "# cfg = SmallConfig()\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-cfg.init_scale, cfg.init_scale)\n",
    "    \n",
    "    params = cfg.params(\n",
    "        'batch_size',\n",
    "        'num_steps',\n",
    "        'num_layers',\n",
    "        'hidden_size',\n",
    "        'vocab_size',\n",
    "        'keep_prob',\n",
    "        'learning_rate',\n",
    "        'max_grad_norm')\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    num_steps = params['num_steps']\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_input, train_target = input_batch(train_data, batch_size, num_steps, 'train_input')\n",
    "        epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "        with tf.variable_scope('model', reuse=None, initializer=initializer):\n",
    "            m_train = PTBTrain(is_training=True, \n",
    "                               input_data=train_input,\n",
    "                               targets=train_target,\n",
    "                               epoch_size=epoch_size,\n",
    "                               **params)\n",
    "        tf.summary.scalar('training_loss', m_train._cost)\n",
    "        tf.summary.scalar('learning_rate', m_train._lr)\n",
    "    \n",
    "    with tf.name_scope('validation'):\n",
    "        valid_input, valid_target = input_batch(valid_data, batch_size, num_steps, 'validation_input')\n",
    "        epoch_size = ((len(valid_data) // batch_size) - 1) // num_steps\n",
    "        with tf.variable_scope('model', reuse=True, initializer=initializer):\n",
    "            m_valid = PTBTrain(is_training=False,\n",
    "                               input_data=valid_input,\n",
    "                               targets=valid_target,\n",
    "                               epoch_size=epoch_size,\n",
    "                               **params)\n",
    "        tf.summary.scalar('validation_loss', m_valid._cost)\n",
    "        \n",
    "    with tf.name_scope('test'):\n",
    "        test_input, test_target = input_batch(test_data, 1, 1, 'test_input')\n",
    "        epoch_size = len(test_data) - 1\n",
    "        params.update(batch_size=1, num_steps=1)\n",
    "        with tf.variable_scope('model', reuse=True, initializer=initializer):\n",
    "            m_test = PTBTrain(is_training=False,\n",
    "                              input_data=test_input,\n",
    "                              targets=test_target,\n",
    "                              epoch_size=epoch_size,\n",
    "                              **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def remove_dir(path):\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "LOG_DIR = os.path.join(HOME_DIR, 'log')\n",
    "MODEL_DIR = os.path.join(HOME_DIR, 'model')\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, 'ptb_lstm')\n",
    "\n",
    "remove_dir(LOG_DIR)\n",
    "remove_dir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path treebank/log/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:model/global_step/sec: 0\n",
      "\n",
      "Epoch: 1 Learning rate: 1.000000INFO:tensorflow:Recording summary at step 0.\n",
      "\n",
      "0.0% perplexity: 9,180.709 speed: 1,920 wps\n",
      "10.0% perplexity: 840.813 speed: 2,911 wps\n",
      "20.0% perplexity: 781.459 speed: 2,910 wps\n",
      "30.0% perplexity: 758.018 speed: 2,911 wps\n",
      "INFO:tensorflow:model/global_step/sec: 72.7842\n",
      "INFO:tensorflow:Recording summary at step 8734.\n",
      "40.0% perplexity: 746.492 speed: 2,913 wps\n",
      "50.0% perplexity: 741.939 speed: 2,915 wps\n",
      "60.0% perplexity: 717.206 speed: 2,917 wps\n",
      "70.0% perplexity: 686.134 speed: 2,917 wps\n",
      "INFO:tensorflow:model/global_step/sec: 72.9932\n",
      "INFO:tensorflow:Recording summary at step 17493.\n",
      "80.0% perplexity: 661.062 speed: 2,916 wps\n",
      "90.0% perplexity: 638.630 speed: 2,915 wps\n",
      "Epoch: 1 Train Perplexity: 619.249\n",
      "Epoch: 1 Valid Perplexity: 489.841\n",
      "\n",
      "Epoch: 2 Learning rate: 0.500000\n",
      "0.0% perplexity: 600.944 speed: 2,919 wps\n",
      "INFO:tensorflow:model/global_step/sec: 66.5162\n",
      "INFO:tensorflow:Recording summary at step 25475.\n",
      "10.0% perplexity: 458.474 speed: 2,908 wps\n",
      "20.0% perplexity: 469.972 speed: 2,908 wps\n",
      "30.0% perplexity: 468.940 speed: 2,909 wps\n",
      "40.0% perplexity: 469.004 speed: 2,909 wps\n",
      "INFO:tensorflow:model/global_step/sec: 72.7005\n",
      "INFO:tensorflow:Recording summary at step 34199.\n",
      "50.0% perplexity: 470.820 speed: 2,908 wps\n",
      "60.0% perplexity: 464.212 speed: 2,906 wps\n",
      "70.0% perplexity: 460.331 speed: 2,905 wps\n",
      "80.0% perplexity: 458.617 speed: 2,904 wps\n",
      "INFO:tensorflow:Saving checkpoint to path treebank/log/model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 72.4298\n",
      "INFO:tensorflow:Recording summary at step 42892.\n",
      "90.0% perplexity: 455.824 speed: 2,902 wps\n",
      "Epoch: 2 Train Perplexity: 452.923\n",
      "Epoch: 2 Valid Perplexity: 450.691\n",
      "\n",
      "INFO:tensorflow:Recording summary at step 46478.\n",
      "INFO:tensorflow:model/global_step/sec: 29.8881\n",
      "Test Perplexity: 426.998\n",
      "\n",
      "Saving model...\n",
      "CPU times: user 37min 13s, sys: 19.3 s, total: 37min 32s\n",
      "Wall time: 12min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sv = tf.train.Supervisor(graph=graph, logdir=LOG_DIR)\n",
    "with sv.managed_session() as session:\n",
    "    for epoch in range(1, cfg.max_max_epoch+1):\n",
    "        decay = cfg.lr_decay ** max(epoch - cfg.max_epoch, 0.0)\n",
    "        lr_value = cfg.learning_rate * decay\n",
    "        m_train.assign_lr(session, lr_value)\n",
    "        \n",
    "        print()\n",
    "        print('Epoch: {:d} Learning rate: {:.6f}'.format(epoch, m_train.lr(session)))\n",
    "        \n",
    "        train_perplexity = m_train.run_epoch_train(session)\n",
    "        print('Epoch: {:d} Train Perplexity: {:.3f}'.format(epoch, train_perplexity))\n",
    "        \n",
    "        valid_perplexity = m_valid.run_epoch_eval(session)\n",
    "        print('Epoch: {:d} Valid Perplexity: {:.3f}'.format(epoch, valid_perplexity))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    test_perplexity = m_test.run_epoch_eval(session)\n",
    "    print('Test Perplexity: {:.3f}'.format(test_perplexity))\n",
    "    \n",
    "    print()\n",
    "    print('Saving model...')\n",
    "    sv.saver.save(session, MODEL_FILE, global_step=sv.global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
