{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Wide & Deep Learning Tutorial\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading The Census Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "HOME_DIR = 'census'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "\n",
    "CENSUS_TRAINING = \"adult.data\"\n",
    "CENSUS_TEST = \"adult.test\"\n",
    "\n",
    "TRAINING_FILE = os.path.join(DATA_DIR, CENSUS_TRAINING)\n",
    "TEST_FILE = os.path.join(DATA_DIR, CENSUS_TEST)\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.base import maybe_download\n",
    "maybe_download(CENSUS_TRAINING, DATA_DIR, 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\n",
    "maybe_download(CENSUS_TEST, DATA_DIR, 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test')\n",
    "\n",
    "MODEL_DIR = os.path.join(HOME_DIR, 'model', 'wide_n_deep')\n",
    "\n",
    "if os.path.isdir(MODEL_DIR):\n",
    "    shutil.rmtree(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined column fnlwgt:2 (assuming CATEGORICAL type)\n",
      "Undefined column income_bracket:14 (assuming CATEGORICAL type)\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\"\n",
    "]\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"native_country\"\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\"\n",
    "]\n",
    "\n",
    "record_defaults = []\n",
    "\n",
    "for i, colname in enumerate(COLUMNS):\n",
    "    if colname in CATEGORICAL_COLUMNS:\n",
    "        record_defaults.append([\"\"])\n",
    "    elif colname in CONTINUOUS_COLUMNS:\n",
    "        record_defaults.append([0.0])\n",
    "    else:\n",
    "        print(\"Undefined column {}:{} (assuming CATEGORICAL type)\".format(colname, i))\n",
    "        record_defaults.append([\"\"])\n",
    "\n",
    "def read_census_csv(filename_queue, skip_header=False):\n",
    "    reader = tf.TextLineReader(skip_header_lines=int(skip_header))\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    cols = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "    label_raw = cols.pop() # income_bracket\n",
    "    label = tf.py_func(lambda x: int(\">50K\" in x), [label_raw], [tf.int64])[0]\n",
    "    label.set_shape([])\n",
    "\n",
    "    cols.append(label)\n",
    "    return cols\n",
    "\n",
    "def input_pipeline(filenames, batch_size, skip_header=False):\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    input_cols = read_census_csv(filename_queue, skip_header)\n",
    "    \n",
    "    example = tf.train.batch(input_cols, batch_size=batch_size)\n",
    "    label = example.pop()\n",
    "    \n",
    "    feature_cols = dict(zip(COLUMNS, example))\n",
    "    \n",
    "    indices = [[i, 0] for i in range(batch_size)]\n",
    "    for colname in CATEGORICAL_COLUMNS:\n",
    "        tensor = feature_cols[colname]\n",
    "        feature_cols[colname] = tf.SparseTensor(indices, tensor, [batch_size, 1])\n",
    "\n",
    "    return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_pipeline([TRAINING_FILE], 128, False)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_pipeline([TEST_FILE], 128, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Base Feature Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical base columns\n",
    "\n",
    "workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\"workclass\", hash_bucket_size=100)\n",
    "education = tf.contrib.layers.sparse_column_with_hash_bucket(\"education\", hash_bucket_size=1000)\n",
    "marital_status = tf.contrib.layers.sparse_column_with_hash_bucket(\"marital_status\", hash_bucket_size=100)\n",
    "occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\"occupation\", hash_bucket_size=1000)\n",
    "relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\"relationship\", hash_bucket_size=100)\n",
    "native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "gender = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"gender\",\n",
    "    keys=[\"female\", \"male\"])\n",
    "\n",
    "race = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"race\",\n",
    "    keys=[\"Amer-Indian-Eskimo\", \"Asian-Pac-Islander\", \"Black\", \"Other\", \"White\"])\n",
    "\n",
    "# Continuous base columns\n",
    "\n",
    "age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Wide Model: Linear Model with Crossed Feature Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_SparseColumn(column_name='gender', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('female', 'male'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='workclass', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='marital_status', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _SparseColumn(column_name='relationship', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string),\n",
       " _BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)),\n",
       " _CrossedColumn(columns=(_SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=10000, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _CrossedColumn(columns=(_SparseColumn(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=10000, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='race', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), num_oov_buckets=0, vocab_size=5, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_buckets = tf.contrib.layers.bucketized_column(\n",
    "    age,\n",
    "    boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "education_x_occupation = tf.contrib.layers.crossed_column(\n",
    "    [education, occupation],\n",
    "    hash_bucket_size=int(1e4))\n",
    "\n",
    "native_country_x_occupation = tf.contrib.layers.crossed_column(\n",
    "    [native_country, occupation],\n",
    "    hash_bucket_size=int(1e4))\n",
    "\n",
    "age_buckets_x_race_x_occupation = tf.contrib.layers.crossed_column(\n",
    "    [age_buckets, race, occupation],\n",
    "    hash_bucket_size=int(1e6))\n",
    "\n",
    "wide_columns = [\n",
    "    gender,\n",
    "    native_country,\n",
    "    education,\n",
    "    occupation,\n",
    "    workclass,\n",
    "    marital_status,\n",
    "    relationship,\n",
    "    age_buckets,\n",
    "    education_x_occupation,\n",
    "    native_country_x_occupation,\n",
    "    age_buckets_x_race_x_occupation]\n",
    "\n",
    "wide_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Deep Model: Neural Network with Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='workclass', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768e97d0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee230>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='marital_status', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee398>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='gender', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('female', 'male'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee5f0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='relationship', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee6e0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='race', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), num_oov_buckets=0, vocab_size=5, default_value=-1), combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee7d0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee8c0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function _initializer at 0x7f70768ee9b0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None),\n",
       " _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32),\n",
       " _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32),\n",
       " _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32),\n",
       " _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32),\n",
       " _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_columns = [\n",
    "    tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(marital_status, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(race, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(native_country, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "    age,\n",
    "    education_num,\n",
    "    capital_gain,\n",
    "    capital_loss,\n",
    "    hours_per_week]\n",
    "\n",
    "deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Wide and Deep Models into One**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=MODEL_DIR,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Evaluating The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9296875\n",
      "accuracy/baseline_target_mean: 0.2109375\n",
      "accuracy/threshold_0.500000_mean: 0.9296875\n",
      "auc: 0.963696360588\n",
      "global_step: 200\n",
      "labels/actual_target_mean: 0.2109375\n",
      "labels/prediction_mean: 0.220046311617\n",
      "loss: 3.95447969437\n",
      "precision/positive_threshold_0.500000_mean: 0.909090936184\n",
      "recall/positive_threshold_0.500000_mean: 0.740740716457\n"
     ]
    }
   ],
   "source": [
    "m.fit(input_fn=train_input_fn, steps=200)\n",
    "\n",
    "results = m.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "for key in sorted(results):\n",
    "    print('{}: {}'.format(key, results[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 0.10 (CPU, Python 2)",
   "language": "python",
   "name": "tensorflow-py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
