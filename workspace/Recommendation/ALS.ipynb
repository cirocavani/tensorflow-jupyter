{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Collaborative Filtering with Matrix Factorization (ALS)\n",
    "\n",
    "PoC\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens Dataset\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv.\n",
    "\n",
    "(README for more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "DATA_DIR = 'movielens'\n",
    "DATASET_URL = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "DATASET_FILENAME = DATASET_URL.split('/')[-1]\n",
    "DATASET_PACKAGE = os.path.join(DATA_DIR, DATASET_FILENAME)\n",
    "DATASET_PATH = os.path.join(DATA_DIR, DATASET_FILENAME[:-4])\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "if not os.path.isfile(DATASET_PACKAGE):\n",
    "    print('Downloading {}...'.format(DATASET_FILENAME))\n",
    "    r = requests.get(DATASET_URL, stream=True)\n",
    "    with open(DATASET_PACKAGE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Done!')\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print('Unpacking {}...'.format(DATASET_PACKAGE))\n",
    "    with zipfile.ZipFile(DATASET_PACKAGE, 'r') as f:\n",
    "        f.extractall(DATA_DIR)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "\n",
      "Users: 671\n",
      "Items: 9,066\n",
      "Ratings: 100,004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "\n",
    "Rating = collections.namedtuple('Rating', ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "class Dataset(collections.namedtuple('Dataset', ['users', 'items', 'ratings'])):\n",
    "\n",
    "    #users: set[str]\n",
    "    #items: set[str]\n",
    "    #ratings: list[Rating]\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __str__(self):\n",
    "        out = 'Users: {:,d}\\n'.format(self.n_users)\n",
    "        out += 'Items: {:,d}\\n'.format(self.n_items)\n",
    "        out += 'Ratings: {:,d}\\n'.format(self.n_ratings)\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    @property\n",
    "    def n_items(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    @property\n",
    "    def n_ratings(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def user_ratings(self, user_id):\n",
    "        return list(r for r in self.ratings if r.user_id == user_id)\n",
    "\n",
    "    def item_ratings(self, item_id):\n",
    "        return list(r for r in self.ratings if r.item_id == item_id)\n",
    "\n",
    "    def filter_ratings(self, users, items):\n",
    "        return list(((r.user_id, r.item_id), r.rating)\n",
    "                    for r in self.ratings\n",
    "                    if r.user_id in users\n",
    "                    and r.item_id in items)\n",
    "\n",
    "\n",
    "def new_dataset(ratings):\n",
    "    users = set(r.user_id for r in ratings)\n",
    "    items = set(r.item_id for r in ratings)\n",
    "    return Dataset(users, items, ratings)\n",
    "\n",
    "\n",
    "def load_movielens_ratings(dataset_path):\n",
    "    ratings_csv = os.path.join(dataset_path, 'ratings.csv')\n",
    "    if not os.path.isfile(ratings_csv):\n",
    "        raise Exception('File not found: \\'{}\\''.format(ratings_csv))\n",
    "    ratings = list()\n",
    "    with open(ratings_csv, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for user_id, item_id, rating, timestamp in reader:\n",
    "            ratings.append(Rating(user_id,\n",
    "                                  item_id,\n",
    "                                  float(rating),\n",
    "                                  int(timestamp)))\n",
    "    return ratings\n",
    "\n",
    "def load_movielens(dataset_path):\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        raise Exception('Path not found: \\'{}\\''.format(dataset_path))\n",
    "\n",
    "    ratings = load_movielens_ratings(dataset_path)\n",
    "    dataset = new_dataset(ratings)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "small_dataset = load_movielens('movielens/ml-latest-small')\n",
    "\n",
    "print('Dataset\\n\\n{}'.format(small_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "\n",
      "Users: 435\n",
      "Items: 5,668\n",
      "Ratings: 64,002\n",
      "\n",
      "Validation\n",
      "\n",
      "Users: 136\n",
      "Items: 4,112\n",
      "Ratings: 16,001\n",
      "\n",
      "Test\n",
      "\n",
      "Users: 147\n",
      "Items: 4,753\n",
      "Ratings: 20,001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_by_time(dataset, train_ratio=0.80):\n",
    "    ratings = sorted(dataset.ratings, key=lambda r: r.timestamp)\n",
    "    size = int(len(ratings) * train_ratio)\n",
    "    train_ratings = ratings[:size]\n",
    "    test_ratings = ratings[size:]\n",
    "    return new_dataset(train_ratings), \\\n",
    "            new_dataset(test_ratings)\n",
    "\n",
    "train_valid_data, test_data = split_by_time(small_dataset)\n",
    "train_data, valid_data = split_by_time(train_valid_data)\n",
    "\n",
    "print('Train\\n\\n{}'.format(train_data))\n",
    "print('Validation\\n\\n{}'.format(valid_data))\n",
    "print('Test\\n\\n{}'.format(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for train: 64,002\n"
     ]
    }
   ],
   "source": [
    "train_eval = list(((r.user_id, r.item_id), r.rating) for r in train_data.ratings)\n",
    "print('Evaluation ratings for train: {:,d}'.format(len(train_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train and validation: 2,424\n"
     ]
    }
   ],
   "source": [
    "# only items in train will be available for validation\n",
    "valid_items = train_data.items & valid_data.items\n",
    "print('Items in train and validation: {:,d}'.format(len(valid_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in validation with train items: 135\n"
     ]
    }
   ],
   "source": [
    "# users from validation that has any item from train\n",
    "valid_users = set(r.user_id for r in valid_data.ratings if r.item_id in train_data.items)\n",
    "print('Users in validation with train items: {:,d}'.format(len(valid_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in train and validation: 23\n"
     ]
    }
   ],
   "source": [
    "# only users in train are available for validation\n",
    "valid_users &= train_data.users\n",
    "print('Users in train and validation: {:,d}'.format(len(valid_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for validation: 944\n"
     ]
    }
   ],
   "source": [
    "valid_eval = valid_data.filter_ratings(valid_users, valid_items)\n",
    "print('Evaluation ratings for validation: {:,d}'.format(len(valid_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Cluster.HEAVY: 'heavy'>,\n",
       " <Cluster.MODERATE: 'moderate'>,\n",
       " <Cluster.LIGHT: 'light'>,\n",
       " <Cluster.ACCIDENTAL: 'accidental'>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Cluster(Enum):\n",
    "    HEAVY = 'heavy'\n",
    "    MODERATE = 'moderate'\n",
    "    LIGHT = 'light'\n",
    "    ACCIDENTAL = 'accidental'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    \n",
    "list(Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id=624, (train, valid) = (679, 91), moderate\n",
      "user_id=547, (train, valid) = (893, 158), heavy\n",
      "user_id=16, (train, valid) = (19, 7), accidental\n",
      "user_id=580, (train, valid) = (585, 144), moderate\n",
      "user_id=587, (train, valid) = (338, 35), moderate\n",
      "user_id=77, (train, valid) = (238, 56), moderate\n",
      "user_id=529, (train, valid) = (318, 32), moderate\n",
      "user_id=15, (train, valid) = (827, 53), moderate\n",
      "user_id=105, (train, valid) = (421, 17), moderate\n",
      "user_id=561, (train, valid) = (248, 43), moderate\n",
      "user_id=388, (train, valid) = (597, 6), accidental\n",
      "user_id=596, (train, valid) = (438, 3), accidental\n",
      "user_id=380, (train, valid) = (548, 52), moderate\n",
      "user_id=356, (train, valid) = (12, 3), accidental\n",
      "user_id=430, (train, valid) = (186, 93), moderate\n",
      "user_id=292, (train, valid) = (263, 1), accidental\n",
      "user_id=384, (train, valid) = (349, 31), moderate\n",
      "user_id=427, (train, valid) = (162, 4), accidental\n",
      "user_id=150, (train, valid) = (358, 6), accidental\n",
      "user_id=648, (train, valid) = (123, 48), moderate\n",
      "user_id=355, (train, valid) = (269, 18), moderate\n",
      "user_id=432, (train, valid) = (52, 8), accidental\n",
      "user_id=353, (train, valid) = (389, 35), moderate\n"
     ]
    }
   ],
   "source": [
    "valid_user_clusters = collections.defaultdict(list)\n",
    "\n",
    "for user_id in valid_users:\n",
    "    n_train = len(list(r for r in train_data.user_ratings(user_id) if r.item_id in valid_items))\n",
    "    n_valid = len(list(r for r in valid_data.user_ratings(user_id) if r.item_id in valid_items))\n",
    "    \n",
    "    cluster = None\n",
    "    if n_train < 10 or n_valid < 10 \\\n",
    "        or n_train + n_valid < 30:\n",
    "        cluster = Cluster.ACCIDENTAL\n",
    "    elif n_train + n_valid > 1000:\n",
    "        cluster = Cluster.HEAVY\n",
    "    elif n_train + n_valid > 100:\n",
    "        cluster = Cluster.MODERATE\n",
    "    else:\n",
    "        cluster = Cluster.LIGHT\n",
    "    \n",
    "    valid_user_clusters[cluster].append(user_id)\n",
    "    print('user_id={}, (train, valid) = ({}, {}), {}'.format(user_id, n_train, n_valid, cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heavy 1\n",
      "moderate 14\n",
      "light 0\n",
      "accidental 8\n"
     ]
    }
   ],
   "source": [
    "for cluster in Cluster:\n",
    "    users = valid_user_clusters[cluster]\n",
    "    print(cluster, len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for heavy: 158\n",
      "Evaluation ratings for moderate: 748\n",
      "Evaluation ratings for accidental: 38\n"
     ]
    }
   ],
   "source": [
    "valid_clusters = dict()\n",
    "\n",
    "for cluster in Cluster:\n",
    "    users = valid_user_clusters[cluster]\n",
    "    if not users:\n",
    "        continue\n",
    "    eval_data = valid_data.filter_ratings(users, valid_items)\n",
    "    if not eval_data:\n",
    "        continue\n",
    "    valid_clusters[cluster] = eval_data\n",
    "    print('Evaluation ratings for {}: {:,d}'.format(cluster, len(eval_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train and test: 2,332\n"
     ]
    }
   ],
   "source": [
    "# only items in train will be available for test\n",
    "test_items = train_data.items & test_data.items\n",
    "print('Items in train and test: {:,d}'.format(len(test_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in test with train items: 145\n"
     ]
    }
   ],
   "source": [
    "# users from test that has any item from train\n",
    "test_users = set(r.user_id for r in test_data.ratings if r.item_id in train_data.items)\n",
    "print('Users in test with train items: {:,d}'.format(len(test_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in train and test: 5\n"
     ]
    }
   ],
   "source": [
    "# only users in train are available for test\n",
    "test_users &= train_data.users\n",
    "print('Users in train and test: {:,d}'.format(len(test_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for test: 278\n"
     ]
    }
   ],
   "source": [
    "test_eval = test_data.filter_ratings(test_users, test_items)\n",
    "print('Evaluation ratings for test: {:,d}'.format(len(test_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow WALS\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map User <-> index\n",
    "# Map Item <-> index\n",
    "IndexMapping = collections.namedtuple('IndexMapping', ['users_to_idx',\n",
    "                                                       'users_from_idx',\n",
    "                                                       'items_to_idx',\n",
    "                                                       'items_from_idx'])\n",
    "\n",
    "def map_index(values):\n",
    "    values_from_idx = dict(enumerate(values))\n",
    "    values_to_idx = dict((value, idx) for idx, value in values_from_idx.items())\n",
    "    return values_to_idx, values_from_idx\n",
    "\n",
    "def new_mapping(dataset):\n",
    "    users_to_idx, users_from_idx = map_index(dataset.users)\n",
    "    items_to_idx, items_from_idx = map_index(dataset.items)\n",
    "    return IndexMapping(users_to_idx, users_from_idx, items_to_idx, items_from_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.203\n",
      "RMSE: 0.802\n",
      "RMSE: 0.641\n",
      "RMSE: 0.593\n",
      "RMSE: 0.569\n",
      "RMSE: 0.554\n",
      "RMSE: 0.544\n",
      "RMSE: 0.536\n",
      "RMSE: 0.530\n",
      "RMSE: 0.526\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.factorization import WALSModel\n",
    "\n",
    "class ALSRecommenderModel:\n",
    "    \n",
    "    def __init__(self, user_factors, item_factors, mapping):\n",
    "        self.user_factors = user_factors\n",
    "        self.item_factors = item_factors\n",
    "        self.mapping = mapping\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for user_id, item_id in x:\n",
    "            if user_id not in self.mapping.users_to_idx \\\n",
    "                or item_id not in self.mapping.items_to_idx:\n",
    "                yield (user_id, item_id), 0.0\n",
    "                continue\n",
    "            i = self.mapping.users_to_idx[user_id]\n",
    "            j = self.mapping.items_to_idx[item_id]\n",
    "            u = self.user_factors[i]\n",
    "            v = self.item_factors[j]\n",
    "            r = np.dot(u, v)\n",
    "            yield (user_id, item_id), r\n",
    "    \n",
    "    def recommend(self, user_id, num_items=10, items_exclude=set()):\n",
    "        i = self.mapping.users_to_idx[user_id]\n",
    "        u = self.user_factors[i]\n",
    "        V = self.item_factors\n",
    "        P = np.dot(V, u)\n",
    "        rank = sorted(enumerate(P), key=lambda p: p[1], reverse=True)\n",
    "\n",
    "        top = list()\n",
    "        k = 0\n",
    "        while k < len(rank) and len(top) < num_items:\n",
    "            j, r = rank[k]\n",
    "            k += 1\n",
    "\n",
    "            item_id = self.mapping.items_from_idx[j]\n",
    "            if item_id in items_exclude:\n",
    "                continue\n",
    "\n",
    "            top.append((item_id, r))\n",
    "\n",
    "        return top        \n",
    "    \n",
    "class ALSRecommender:\n",
    "    \n",
    "    def __init__(self, num_factors=10, num_iters=10, reg=1e-1):\n",
    "        self.num_factors = num_factors\n",
    "        self.num_iters = num_iters\n",
    "        self.regularization = reg\n",
    "\n",
    "    def fit(self, dataset, verbose=False):\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "            input_matrix, mapping = self.sparse_input(dataset)\n",
    "            model = self.als_model(dataset)\n",
    "            self.train(model, input_matrix, verbose)\n",
    "            row_factor = model.row_factors[0].eval()\n",
    "            col_factor = model.col_factors[0].eval()\n",
    "            return ALSRecommenderModel(row_factor, col_factor, mapping)\n",
    "\n",
    "    def sparse_input(self, dataset):\n",
    "        mapping = new_mapping(dataset)\n",
    "\n",
    "        indices = [(mapping.users_to_idx[r.user_id],\n",
    "                    mapping.items_to_idx[r.item_id])\n",
    "                   for r in dataset.ratings]\n",
    "        values = [r.rating for r in dataset.ratings]\n",
    "        shape = (dataset.n_users, dataset.n_items)\n",
    "\n",
    "        return tf.SparseTensor(indices, values, shape), mapping\n",
    "    \n",
    "    def als_model(self, dataset):\n",
    "        return WALSModel(\n",
    "            dataset.n_users,\n",
    "            dataset.n_items,\n",
    "            self.num_factors,\n",
    "            regularization=self.regularization,\n",
    "            unobserved_weight=0)\n",
    "\n",
    "    def train(self, model, input_matrix, verbose=False):\n",
    "        rmse_op = self.rmse_op(model, input_matrix) if verbose else None\n",
    "\n",
    "        row_update_op = model.update_row_factors(sp_input=input_matrix)[1]\n",
    "        col_update_op = model.update_col_factors(sp_input=input_matrix)[1]\n",
    "\n",
    "        model.initialize_op.run()\n",
    "        model.worker_init.run()\n",
    "        for _ in range(self.num_iters):\n",
    "            # Update Users\n",
    "            model.row_update_prep_gramian_op.run()\n",
    "            model.initialize_row_update_op.run()\n",
    "            row_update_op.run()\n",
    "            # Update Items\n",
    "            model.col_update_prep_gramian_op.run()\n",
    "            model.initialize_col_update_op.run()\n",
    "            col_update_op.run()\n",
    "\n",
    "            if verbose:\n",
    "                print('RMSE: {:,.3f}'.format(rmse_op.eval()))\n",
    "\n",
    "    def approx_sparse(self, model, indices, shape):\n",
    "        row_factors = tf.nn.embedding_lookup(\n",
    "            model.row_factors,\n",
    "            tf.range(model._input_rows),\n",
    "            partition_strategy=\"div\")\n",
    "        col_factors = tf.nn.embedding_lookup(\n",
    "            model.col_factors,\n",
    "            tf.range(model._input_cols),\n",
    "            partition_strategy=\"div\")\n",
    "\n",
    "        row_indices, col_indices = tf.split(indices,\n",
    "                                            axis=1,\n",
    "                                            num_or_size_splits=2)\n",
    "        gathered_row_factors = tf.gather(row_factors, row_indices)\n",
    "        gathered_col_factors = tf.gather(col_factors, col_indices)\n",
    "        approx_vals = tf.squeeze(tf.matmul(gathered_row_factors,\n",
    "                                           gathered_col_factors,\n",
    "                                           adjoint_b=True))\n",
    "\n",
    "        return tf.SparseTensor(indices=indices,\n",
    "                               values=approx_vals,\n",
    "                               dense_shape=shape)\n",
    "\n",
    "    def rmse_op(self, model, input_matrix):\n",
    "        approx_matrix = self.approx_sparse(model, input_matrix.indices, input_matrix.dense_shape)\n",
    "        err = tf.sparse_add(input_matrix, approx_matrix * (-1))\n",
    "        err2 = tf.square(err)\n",
    "        n = input_matrix.values.shape[0].value\n",
    "        return tf.sqrt(tf.sparse_reduce_sum(err2) / n)\n",
    "\n",
    "\n",
    "als = ALSRecommender()\n",
    "als_model = als.fit(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 39292 4.0 2.14771\n",
      "561 2916 3.0 4.14338\n",
      "561 4643 2.5 3.76398\n",
      "561 33679 4.0 3.10178\n",
      "561 5991 4.5 2.4092\n",
      "561 2804 3.0 3.10694\n",
      "561 5956 4.0 4.69892\n",
      "561 4025 3.0 2.50858\n",
      "561 2701 3.5 2.92227\n",
      "561 368 4.0 3.83185\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    x, y  = valid_eval[k]\n",
    "    _,  y_hat = list(als_model.transform([x]))[0]\n",
    "    print(*x, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.526\n",
      "RMSE (validation): 1.789\n",
      "RMSE for heavy: 2.339\n",
      "RMSE for moderate: 1.651\n",
      "RMSE for accidental: 1.758\n"
     ]
    }
   ],
   "source": [
    "def _rmse(model, data):\n",
    "    x, y = zip(*data)\n",
    "    y_hat = list(r_hat for _, r_hat in model.transform(x))\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(y, y_hat))))\n",
    "\n",
    "def eval_rmse(model):\n",
    "    rmse = _rmse(model, train_eval)\n",
    "    print('RMSE (train): {:,.3f}'.format(rmse))\n",
    "    \n",
    "    rmse = _rmse(model, valid_eval)\n",
    "    print('RMSE (validation): {:,.3f}'.format(rmse))\n",
    "\n",
    "    for cluster in Cluster:\n",
    "        eval_data = valid_clusters.get(cluster, None)\n",
    "        if not eval_data:\n",
    "            continue\n",
    "        rmse = _rmse(model, eval_data)\n",
    "        print('RMSE for {}: {:,.3f}'.format(cluster, rmse))\n",
    "\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "RMSE: 1.946\n",
      "RMSE: 0.761\n",
      "RMSE: 0.627\n",
      "RMSE: 0.585\n",
      "RMSE: 0.563\n",
      "RMSE: 0.549\n",
      "RMSE: 0.539\n",
      "RMSE: 0.531\n",
      "RMSE: 0.525\n",
      "RMSE: 0.520\n",
      "\n",
      "Evaluation...\n",
      "\n",
      "RMSE (train): 0.520\n",
      "RMSE (validation): 1.666\n",
      "RMSE for heavy: 2.241\n",
      "RMSE for moderate: 1.506\n",
      "RMSE for accidental: 1.833\n"
     ]
    }
   ],
   "source": [
    "als = ALSRecommender(num_factors=10, num_iters=10, reg=0.1)\n",
    "print('Training...\\n')\n",
    "als_model = als.fit(train_data, verbose=True)\n",
    "print('\\nEvaluation...\\n')\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items for 547:\n",
      "\n",
      "[1] 48516, 5.00\n",
      "[1] 3310, 5.00\n",
      "[1] 39183, 5.00\n",
      "[2] 357, 4.50\n",
      "[2] 165, 4.50\n",
      "[2] 1183, 4.50\n",
      "[2] 30749, 4.50\n",
      "[2] 4235, 4.50\n",
      "[2] 3809, 4.50\n",
      "[2] 3989, 4.50\n",
      "[2] 954, 4.50\n",
      "[2] 3022, 4.50\n",
      "[2] 48774, 4.50\n",
      "[2] 48696, 4.50\n",
      "[2] 46578, 4.50\n",
      "[2] 45210, 4.50\n",
      "[2] 2245, 4.50\n",
      "[2] 2150, 4.50\n",
      "[2] 1288, 4.50\n",
      "[2] 4349, 4.50\n",
      "\n",
      "Recommendations for 547:\n",
      "\n",
      "[1] 940, 8.29, -\n",
      "[2] 5378, 7.02, -\n",
      "[3] 2202, 6.44, -\n",
      "[4] 446, 6.43, -\n",
      "[5] 383, 6.42, -\n",
      "[6] 1701, 6.41, -\n",
      "[7] 125, 6.37, -\n",
      "[8] 1125, 6.27, -\n",
      "[9] 2099, 6.23, -\n",
      "[10] 1627, 5.92, -\n"
     ]
    }
   ],
   "source": [
    "# valid_clusters: dict[cluster: Cluster, list[((user_id: str, item_id: str), rating: float)]]\n",
    "# 0 -> first user-item-pair-and-rating, 0 -> user-item-pair, 0 -> user\n",
    "user_id = valid_clusters[Cluster.HEAVY][0][0][0] \n",
    "\n",
    "user_items = sorted([(r.item_id, r.rating)\n",
    "                     for r in valid_data.ratings\n",
    "                     if r.user_id == user_id \\\n",
    "                         and r.item_id in train_data.items],\n",
    "                    key=lambda r: r[1],\n",
    "                    reverse=True)\n",
    "\n",
    "items_exclude = set(r.item_id for r in train_data.ratings if r.user_id == user_id)\n",
    "\n",
    "rec_items = als_model.recommend(user_id, items_exclude=items_exclude)\n",
    "\n",
    "user_top = dict()\n",
    "p_rating = None\n",
    "p = 0\n",
    "print('Top items for {}:\\n'.format(user_id))\n",
    "for i, (item_id, rating) in enumerate(user_items):\n",
    "    if p_rating is None or p_rating > rating:\n",
    "        p_rating = rating\n",
    "        p += 1\n",
    "    user_top[item_id] = p\n",
    "    if i < 20:\n",
    "        print('[{}] {}, {:,.2f}'.format(p, item_id, rating))\n",
    "print()\n",
    "\n",
    "p_rating = None\n",
    "p = 0\n",
    "print('Recommendations for {}:\\n'.format(user_id))\n",
    "for item_id, rating in rec_items:\n",
    "    if p_rating is None or p_rating > rating:\n",
    "        p_rating = rating\n",
    "        p += 1\n",
    "    print('[{}] {}, {:,.2f}, {}'.format(p, item_id, rating, user_top.get(item_id, '-')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ALS\n",
    "\n",
    "http://spark.apache.org/docs/2.1.1/ml-collaborative-filtering.html\n",
    "\n",
    "http://spark.apache.org/docs/2.1.1/api/python/pyspark.ml.html#module-pyspark.ml.recommendation\n",
    "\n",
    "https://github.com/apache/spark/blob/v2.1.1/examples/src/main/python/ml/als_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME=os.path.abspath('../../software/spark-2.1.1-bin-hadoop2.7')\n",
    "\n",
    "if not os.path.isdir(SPARK_HOME):\n",
    "    raise Exception('File not found: {}'.format(SPARK_HOME))\n",
    "\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '4g'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.append(os.path.join(SPARK_HOME, 'python', 'lib', 'py4j-0.10.4-src.zip'))\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import atexit\n",
    "atexit.register(lambda: spark.stop())\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|383 |21  |3.0   |\n",
      "|383 |47  |5.0   |\n",
      "|383 |1079|3.0   |\n",
      "|409 |16  |4.0   |\n",
      "|409 |21  |5.0   |\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_plain = list((int(user_id), int(item_id), rating)\n",
    "                   for (user_id, item_id), rating in train_eval)\n",
    "train_df = spark.createDataFrame(train_plain, ['user', 'item', 'rating'])\n",
    "train_df.printSchema()\n",
    "train_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+-----+------+\n",
      "|user|item |rating|\n",
      "+----+-----+------+\n",
      "|561 |39292|4.0   |\n",
      "|561 |2916 |3.0   |\n",
      "|561 |4643 |2.5   |\n",
      "|561 |33679|4.0   |\n",
      "|561 |5991 |4.5   |\n",
      "+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_plain = list((int(user_id), int(item_id), rating)\n",
    "                   for (user_id, item_id), rating in valid_eval)\n",
    "valid_df = spark.createDataFrame(valid_plain, ['user', 'item', 'rating'])\n",
    "valid_df.printSchema()\n",
    "valid_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|547 |7396|4.0   |\n",
      "|547 |1734|3.5   |\n",
      "|529 |7158|4.5   |\n",
      "|529 |3967|3.5   |\n",
      "|529 |3911|4.0   |\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_plain = list((int(user_id), int(item_id), rating)\n",
    "                  for (user_id, item_id), rating in test_eval)\n",
    "test_df = spark.createDataFrame(test_plain, ['user', 'item', 'rating'])\n",
    "test_df.printSchema()\n",
    "test_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_clusters_spark = dict()\n",
    "for cluster, data in valid_clusters.items():\n",
    "    eval_data = list((int(user_id), int(item_id), rating) for (user_id, item_id), rating in data)\n",
    "    eval_df = spark.createDataFrame(eval_data, ['user', 'item', 'rating'])\n",
    "    valid_clusters_spark[cluster] = eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.597\n",
      "RMSE (validation): 1.013\n",
      "RMSE for heavy: 1.123\n",
      "RMSE for moderate: 0.972\n",
      "RMSE for accidental: 1.279\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS as SparkALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def _rmse_spark(model, df):\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                    labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "    pred = model.transform(df)\n",
    "    return evaluator.evaluate(pred)\n",
    "\n",
    "def eval_rmse_spark(model):\n",
    "    rmse = _rmse_spark(model, train_df)\n",
    "    print('RMSE (train): {:,.3f}'.format(rmse))\n",
    "\n",
    "    rmse = _rmse_spark(model, valid_df)\n",
    "    print('RMSE (validation): {:,.3f}'.format(rmse))\n",
    "\n",
    "    for cluster in Cluster:\n",
    "        eval_df = valid_clusters_spark.get(cluster, None)\n",
    "        if not eval_df:\n",
    "            continue\n",
    "        rmse = _rmse_spark(model, eval_df)\n",
    "        print('RMSE for {}: {:,.3f}'.format(cluster, rmse))\n",
    "\n",
    "spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)\n",
    "spark_model = spark_als.fit(train_df)\n",
    "eval_rmse_spark(spark_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.597\n",
      "RMSE (validation): 1.013\n",
      "RMSE for heavy: 1.123\n",
      "RMSE for moderate: 0.972\n",
      "RMSE for accidental: 1.279\n"
     ]
    }
   ],
   "source": [
    "spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)\n",
    "spark_model = spark_als.fit(train_df)\n",
    "eval_rmse_spark(spark_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Params: 5 5 0.001\n",
      "RMSE: 1.693\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.001\n",
      "RMSE: 1.417\n",
      "best update!\n",
      "\n",
      "Params: 20 5 0.001\n",
      "RMSE: 1.752\n",
      "\n",
      "Params: 5 5 0.1\n",
      "RMSE: 1.222\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.1\n",
      "RMSE: 1.384\n",
      "\n",
      "Params: 20 5 0.1\n",
      "RMSE: 1.559\n",
      "\n",
      "Params: 5 5 1\n",
      "RMSE: 1.498\n",
      "\n",
      "Params: 10 5 1\n",
      "RMSE: 1.744\n",
      "\n",
      "Params: 20 5 1\n",
      "RMSE: 2.016\n",
      "\n",
      "Best Params:\n",
      "\n",
      "n_factors=5, n_iters=5, reg=0.1, RMSE=1.222\n"
     ]
    }
   ],
   "source": [
    "default_params = dict(num_factors=[5, 10, 20, 50, 100, 200],\n",
    "                      num_iters=[5, 10, 25],\n",
    "                      reg = [1e-5, 1e-3, 1e-1, 0.0, 1])\n",
    "\n",
    "small_params = dict(num_factors=[5, 10, 20],\n",
    "                    num_iters=[5],\n",
    "                    reg = [1e-3, 1e-1, 1])\n",
    "\n",
    "def grid_search(eval_func, params=default_params, verbose=False):\n",
    "    best_rmse = None\n",
    "    best_params = None\n",
    "    for reg in params['reg']:\n",
    "        for num_iters in params['num_iters']:\n",
    "            for num_factors in params['num_factors']:\n",
    "                if verbose:\n",
    "                    print('\\nParams:', num_factors, num_iters, reg)\n",
    "                try:\n",
    "                    rmse = eval_func(num_factors, num_iters, reg)\n",
    "                except:\n",
    "                    rmse = None\n",
    "                if verbose:\n",
    "                    print('RMSE:', '{:,.3f}'.format(rmse) if rmse is not None else '-')\n",
    "                if rmse is not None and (best_rmse is None or rmse < best_rmse):\n",
    "                    if verbose:\n",
    "                        print('best update!')\n",
    "                    best_rmse = rmse\n",
    "                    best_params = (num_factors, num_iters, reg)\n",
    "    return best_params, best_rmse\n",
    "\n",
    "def tf_eval(num_factors, num_iters, reg):\n",
    "    als = ALSRecommender(num_factors=num_factors, num_iters=num_iters, reg=reg)\n",
    "    model = als.fit(train_data)\n",
    "    return _rmse(model, valid_eval)\n",
    "\n",
    "tf_params, tf_score = grid_search(tf_eval, params=small_params, verbose=True)\n",
    "print()\n",
    "print('Best Params:\\n\\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}'.format(*tf_params, tf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Params: 5 5 0.001\n",
      "RMSE: 1.327\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.001\n",
      "RMSE: 1.412\n",
      "\n",
      "Params: 20 5 0.001\n",
      "RMSE: 1.862\n",
      "\n",
      "Params: 5 5 0.1\n",
      "RMSE: 0.990\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.1\n",
      "RMSE: 1.020\n",
      "\n",
      "Params: 20 5 0.1\n",
      "RMSE: 1.028\n",
      "\n",
      "Params: 5 5 1\n",
      "RMSE: 1.257\n",
      "\n",
      "Params: 10 5 1\n",
      "RMSE: 1.259\n",
      "\n",
      "Params: 20 5 1\n",
      "RMSE: 1.257\n",
      "\n",
      "Best Params:\n",
      "\n",
      "n_factors=5, n_iters=5, reg=0.1, RMSE=0.990\n"
     ]
    }
   ],
   "source": [
    "def spark_eval(num_factors, num_iters, reg):\n",
    "    als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)\n",
    "    model = als.fit(train_df)\n",
    "    return _rmse_spark(model, valid_df)\n",
    "\n",
    "spark_params, spark_score = grid_search(spark_eval, params=small_params, verbose=True)\n",
    "print()\n",
    "print('Best Params:\\n\\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}'.format(*spark_params, spark_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow RMSE for test: 1.279\n"
     ]
    }
   ],
   "source": [
    "als = ALSRecommender(*tf_params)\n",
    "model = als.fit(train_data)\n",
    "rmse = _rmse(model, test_eval)\n",
    "print('TensorFlow RMSE for test: {:,.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark RMSE for test: 1.151\n"
     ]
    }
   ],
   "source": [
    "num_factors, num_iters, reg = spark_params\n",
    "als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)\n",
    "model = als.fit(train_df)\n",
    "rmse = _rmse_spark(model, test_df)\n",
    "print('Spark RMSE for test: {:,.3f}'.format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
