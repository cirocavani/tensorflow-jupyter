{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Collaborative Filtering with Matrix Factorization (ALS)\n",
    "\n",
    "PoC\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens Dataset\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv.\n",
    "\n",
    "(README for more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "DATA_DIR = 'movielens'\n",
    "DATASET_URL = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "DATASET_FILENAME = DATASET_URL.split('/')[-1]\n",
    "DATASET_PACKAGE = os.path.join(DATA_DIR, DATASET_FILENAME)\n",
    "DATASET_PATH = os.path.join(DATA_DIR, DATASET_FILENAME[:-4])\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "if not os.path.isfile(DATASET_PACKAGE):\n",
    "    print('Downloading {}...'.format(DATASET_FILENAME))\n",
    "    r = requests.get(DATASET_URL, stream=True)\n",
    "    with open(DATASET_PACKAGE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Done!')\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print('Unpacking {}...'.format(DATASET_PACKAGE))\n",
    "    with zipfile.ZipFile(DATASET_PACKAGE, 'r') as f:\n",
    "        f.extractall(DATA_DIR)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 671\n",
      "Items: 9,066\n",
      "Ratings: 100,004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "\n",
    "Rating = collections.namedtuple('Rating', ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "class Dataset(collections.namedtuple('Dataset', ['users', 'items', 'ratings'])):\n",
    "\n",
    "    #users: set[str]\n",
    "    #items: set[str]\n",
    "    #ratings: list[Rating]\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __str__(self):\n",
    "        out = 'Users: {:,d}\\n'.format(self.n_users)\n",
    "        out += 'Items: {:,d}\\n'.format(self.n_items)\n",
    "        out += 'Ratings: {:,d}\\n'.format(self.n_ratings)\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    @property\n",
    "    def n_items(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    @property\n",
    "    def n_ratings(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def user_ratings(self, user_id):\n",
    "        return list(r for r in self.ratings if r.user_id == user_id)\n",
    "\n",
    "    def item_ratings(self, item_id):\n",
    "        return list(r for r in self.ratings if r.item_id == item_id)\n",
    "\n",
    "def new_dataset(ratings):\n",
    "    users = set(r.user_id for r in ratings)\n",
    "    items = set(r.item_id for r in ratings)\n",
    "    return Dataset(users, items, ratings)\n",
    "\n",
    "\n",
    "def load_movielens_ratings(dataset_path):\n",
    "    ratings_csv = os.path.join(dataset_path, 'ratings.csv')\n",
    "    if not os.path.isfile(ratings_csv):\n",
    "        raise Exception('File not found: \\'{}\\''.format(ratings_csv))\n",
    "    ratings = list()\n",
    "    with open(ratings_csv, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for user_id, item_id, rating, timestamp in reader:\n",
    "            ratings.append(Rating(user_id,\n",
    "                                  item_id,\n",
    "                                  float(rating),\n",
    "                                  int(timestamp)))\n",
    "    return ratings\n",
    "\n",
    "def load_movielens(dataset_path):\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        raise Exception('Path not found: \\'{}\\''.format(dataset_path))\n",
    "\n",
    "    ratings = load_movielens_ratings(dataset_path)\n",
    "    dataset = new_dataset(ratings)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "small_dataset = load_movielens('movielens/ml-latest-small')\n",
    "print(small_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "\n",
      "Users: 671\n",
      "Items: 9,066\n",
      "Ratings: 100,004\n",
      "\n",
      "Train\n",
      "\n",
      "Users: 547\n",
      "Items: 7,356\n",
      "Ratings: 80,003\n",
      "\n",
      "Test\n",
      "\n",
      "Users: 147\n",
      "Items: 4,753\n",
      "Ratings: 20,001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_by_time(dataset, train_ratio=0.80):\n",
    "    ratings = sorted(dataset.ratings, key=lambda r: r.timestamp)\n",
    "    size = int(len(ratings) * train_ratio)\n",
    "    train_ratings = ratings[:size]\n",
    "    test_ratings = ratings[size:]\n",
    "    return new_dataset(train_ratings), \\\n",
    "            new_dataset(test_ratings)\n",
    "\n",
    "train_data, test_data = split_by_time(small_dataset)\n",
    "\n",
    "print('Dataset\\n\\n{}'.format(small_dataset))\n",
    "print('Train\\n\\n{}'.format(train_data))\n",
    "print('Test\\n\\n{}'.format(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train and test: 3,043\n"
     ]
    }
   ],
   "source": [
    "# only items in train will be available for test evaluation\n",
    "common_items = train_data.items & test_data.items\n",
    "print('Items in train and test: {:,d}'.format(len(common_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in test with train items: 146\n"
     ]
    }
   ],
   "source": [
    "# users from test that has any item from train\n",
    "test_users = set(r.user_id for r in test_data.ratings if r.item_id in train_data.items)\n",
    "print('Users in test with train items: {:,d}'.format(len(test_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in train and test: 22\n"
     ]
    }
   ],
   "source": [
    "# only users in train are available for test\n",
    "common_users = train_data.users & test_users\n",
    "print('Users in train and test: {:,d}'.format(len(common_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Cluster.HEAVY: 'heavy'>,\n",
       " <Cluster.MODERATE: 'moderate'>,\n",
       " <Cluster.LIGHT: 'light'>,\n",
       " <Cluster.ACCIDENTAL: 'accidental'>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Cluster(Enum):\n",
    "    HEAVY = 'heavy'\n",
    "    MODERATE = 'moderate'\n",
    "    LIGHT = 'light'\n",
    "    ACCIDENTAL = 'accidental'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    \n",
    "list(Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id=359, (train, test) = (12, 32), light\n",
      "user_id=480, (train, test) = (220, 132), moderate\n",
      "user_id=73, (train, test) = (881, 336), heavy\n",
      "user_id=652, (train, test) = (5, 165), accidental\n",
      "user_id=648, (train, test) = (187, 2), accidental\n",
      "user_id=78, (train, test) = (171, 87), moderate\n",
      "user_id=624, (train, test) = (912, 63), moderate\n",
      "user_id=163, (train, test) = (30, 39), light\n",
      "user_id=199, (train, test) = (300, 37), moderate\n",
      "user_id=15, (train, test) = (946, 276), heavy\n",
      "user_id=68, (train, test) = (112, 2), accidental\n",
      "user_id=275, (train, test) = (33, 140), moderate\n",
      "user_id=599, (train, test) = (85, 73), moderate\n",
      "user_id=529, (train, test) = (314, 85), moderate\n",
      "user_id=157, (train, test) = (205, 83), moderate\n",
      "user_id=547, (train, test) = (1052, 63), heavy\n",
      "user_id=303, (train, test) = (207, 6), accidental\n",
      "user_id=380, (train, test) = (789, 39), moderate\n",
      "user_id=458, (train, test) = (18, 58), light\n",
      "user_id=637, (train, test) = (13, 8), accidental\n",
      "user_id=48, (train, test) = (296, 81), moderate\n",
      "user_id=257, (train, test) = (37, 54), light\n"
     ]
    }
   ],
   "source": [
    "users_clusters = collections.defaultdict(list)\n",
    "\n",
    "for user_id in common_users:\n",
    "    n_train = len(list(r for r in train_data.user_ratings(user_id) if r.item_id in common_items))\n",
    "    n_test = len(list(r for r in test_data.user_ratings(user_id) if r.item_id in common_items))\n",
    "    \n",
    "    cluster = None\n",
    "    if n_train < 10 or n_test < 10 \\\n",
    "        or n_train + n_test < 30:\n",
    "        cluster = Cluster.ACCIDENTAL\n",
    "    elif n_train + n_test > 1000:\n",
    "        cluster = Cluster.HEAVY\n",
    "    elif n_train + n_test > 100:\n",
    "        cluster = Cluster.MODERATE\n",
    "    else:\n",
    "        cluster = Cluster.LIGHT\n",
    "    \n",
    "    users_clusters[cluster].append(user_id)\n",
    "    print('user_id={}, (train, test) = ({}, {}), {}'.format(user_id, n_train, n_test, cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heavy 3\n",
      "moderate 10\n",
      "light 4\n",
      "accidental 5\n"
     ]
    }
   ],
   "source": [
    "for cluster in Cluster:\n",
    "    users = users_clusters[cluster]\n",
    "    print(cluster, len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for heavy: 675\n",
      "Evaluation ratings for moderate: 820\n",
      "Evaluation ratings for light: 183\n",
      "Evaluation ratings for accidental: 183\n"
     ]
    }
   ],
   "source": [
    "def eval_pairs(users):\n",
    "    return list(((r.user_id, r.item_id), r.rating)\n",
    "                for r in test_data.ratings\n",
    "                if r.user_id in users\n",
    "                and r.item_id in common_items)\n",
    "\n",
    "eval_clusters = dict()\n",
    "\n",
    "for cluster in Cluster:\n",
    "    users = users_clusters[cluster]\n",
    "    pairs = eval_pairs(users)\n",
    "    eval_clusters[cluster] = pairs\n",
    "    print('Evaluation ratings for {}: {:,d}'.format(cluster, len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for test: 1,861\n"
     ]
    }
   ],
   "source": [
    "eval_test = eval_pairs(common_users)\n",
    "print('Evaluation ratings for test: {:,d}'.format(len(eval_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for train: 80,003\n"
     ]
    }
   ],
   "source": [
    "eval_train = list(((r.user_id, r.item_id), r.rating) for r in train_data.ratings)\n",
    "print('Evaluation ratings for train: {:,d}'.format(len(eval_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IndexMapping = collections.namedtuple('IndexMapping', ['users_to_idx',\n",
    "                                                       'users_from_idx',\n",
    "                                                       'items_to_idx',\n",
    "                                                       'items_from_idx'])\n",
    "\n",
    "def map_index(values):\n",
    "    values_from_idx = dict(enumerate(values))\n",
    "    values_to_idx = dict((value, idx) for idx, value in values_from_idx.items())\n",
    "    return values_to_idx, values_from_idx\n",
    "\n",
    "def new_mapping(dataset):\n",
    "    users_to_idx, users_from_idx = map_index(dataset.users)\n",
    "    items_to_idx, items_from_idx = map_index(dataset.items)\n",
    "    return IndexMapping(users_to_idx, users_from_idx, items_to_idx, items_from_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow WALS\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.281\n",
      "RMSE: 0.825\n",
      "RMSE: 0.649\n",
      "RMSE: 0.601\n",
      "RMSE: 0.577\n",
      "RMSE: 0.561\n",
      "RMSE: 0.550\n",
      "RMSE: 0.542\n",
      "RMSE: 0.536\n",
      "RMSE: 0.531\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.factorization import WALSModel\n",
    "\n",
    "class ALSRecommenderModel:\n",
    "    \n",
    "    def __init__(self, user_factors, item_factors, mapping):\n",
    "        self.user_factors = user_factors\n",
    "        self.item_factors = item_factors\n",
    "        self.mapping = mapping\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for user_id, item_id in x:\n",
    "            if user_id not in self.mapping.users_to_idx \\\n",
    "                or item_id not in self.mapping.items_to_idx:\n",
    "                yield (user_id, item_id), 0.0\n",
    "                continue\n",
    "            i = self.mapping.users_to_idx[user_id]\n",
    "            j = self.mapping.items_to_idx[item_id]\n",
    "            u = self.user_factors[i]\n",
    "            v = self.item_factors[j]\n",
    "            r = np.dot(u, v)\n",
    "            yield (user_id, item_id), r\n",
    "    \n",
    "    def recommend(self, user_id, num_items=10, items_exclude=set()):\n",
    "        i = self.mapping.users_to_idx[user_id]\n",
    "        u = self.user_factors[i]\n",
    "        V = self.item_factors\n",
    "        P = np.dot(V, u)\n",
    "        rank = sorted(enumerate(P), key=lambda p: p[1], reverse=True)\n",
    "\n",
    "        top = list()\n",
    "        k = 0\n",
    "        while k < len(rank) and len(top) < num_items:\n",
    "            j, r = rank[k]\n",
    "            k += 1\n",
    "\n",
    "            item_id = self.mapping.items_from_idx[j]\n",
    "            if item_id in items_exclude:\n",
    "                continue\n",
    "\n",
    "            top.append((item_id, r))\n",
    "\n",
    "        return top        \n",
    "    \n",
    "class ALSRecommender:\n",
    "    \n",
    "    def __init__(self, num_factors=10, num_iters=10, reg=1e-1):\n",
    "        self.num_factors = num_factors\n",
    "        self.num_iters = num_iters\n",
    "        self.regularization = reg\n",
    "\n",
    "    def fit(self, dataset, verbose=False):\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "            input_matrix, mapping = self.sparse_input(dataset)\n",
    "            model = self.als_model(dataset)\n",
    "            self.train(model, input_matrix, verbose)\n",
    "            row_factor = model.row_factors[0].eval()\n",
    "            col_factor = model.col_factors[0].eval()\n",
    "            return ALSRecommenderModel(row_factor, col_factor, mapping)\n",
    "\n",
    "    def sparse_input(self, dataset):\n",
    "        mapping = new_mapping(dataset)\n",
    "\n",
    "        indices = [(mapping.users_to_idx[r.user_id],\n",
    "                    mapping.items_to_idx[r.item_id])\n",
    "                   for r in dataset.ratings]\n",
    "        values = [r.rating for r in dataset.ratings]\n",
    "        shape = (dataset.n_users, dataset.n_items)\n",
    "\n",
    "        return tf.SparseTensor(indices, values, shape), mapping\n",
    "    \n",
    "    def als_model(self, dataset):\n",
    "        return WALSModel(\n",
    "            dataset.n_users,\n",
    "            dataset.n_items,\n",
    "            self.num_factors,\n",
    "            regularization=self.regularization,\n",
    "            unobserved_weight=0)\n",
    "\n",
    "    def train(self, model, input_matrix, verbose=False):\n",
    "        rmse_op = self.rmse_op(model, input_matrix) if verbose else None\n",
    "\n",
    "        row_update_op = model.update_row_factors(sp_input=input_matrix)[1]\n",
    "        col_update_op = model.update_col_factors(sp_input=input_matrix)[1]\n",
    "\n",
    "        model.initialize_op.run()\n",
    "        model.worker_init.run()\n",
    "        for _ in range(self.num_iters):\n",
    "            # Update Users\n",
    "            model.row_update_prep_gramian_op.run()\n",
    "            model.initialize_row_update_op.run()\n",
    "            row_update_op.run()\n",
    "            # Update Items\n",
    "            model.col_update_prep_gramian_op.run()\n",
    "            model.initialize_col_update_op.run()\n",
    "            col_update_op.run()\n",
    "\n",
    "            if verbose:\n",
    "                print('RMSE: {:,.3f}'.format(rmse_op.eval()))\n",
    "\n",
    "    def approx_sparse(self, model, indices, shape):\n",
    "        row_factors = tf.nn.embedding_lookup(\n",
    "            model.row_factors,\n",
    "            tf.range(model._input_rows),\n",
    "            partition_strategy=\"div\")\n",
    "        col_factors = tf.nn.embedding_lookup(\n",
    "            model.col_factors,\n",
    "            tf.range(model._input_cols),\n",
    "            partition_strategy=\"div\")\n",
    "\n",
    "        row_indices, col_indices = tf.split(indices,\n",
    "                                            axis=1,\n",
    "                                            num_or_size_splits=2)\n",
    "        gathered_row_factors = tf.gather(row_factors, row_indices)\n",
    "        gathered_col_factors = tf.gather(col_factors, col_indices)\n",
    "        approx_vals = tf.squeeze(tf.matmul(gathered_row_factors,\n",
    "                                           gathered_col_factors,\n",
    "                                           adjoint_b=True))\n",
    "\n",
    "        return tf.SparseTensor(indices=indices,\n",
    "                               values=approx_vals,\n",
    "                               dense_shape=shape)\n",
    "\n",
    "    def rmse_op(self, model, input_matrix):\n",
    "        approx_matrix = self.approx_sparse(model, input_matrix.indices, input_matrix.dense_shape)\n",
    "        err = tf.sparse_add(input_matrix, approx_matrix * (-1))\n",
    "        err2 = tf.square(err)\n",
    "        n = input_matrix.values.shape[0].value\n",
    "        return tf.sqrt(tf.sparse_reduce_sum(err2) / n)\n",
    "\n",
    "\n",
    "als = ALSRecommender()\n",
    "als_model = als.fit(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 6385 4.5 5.37492\n",
      "359 1295 4.5 1.39123\n",
      "359 3258 3.5 4.17737\n",
      "359 318 4.0 5.48446\n",
      "359 858 3.5 5.13797\n",
      "359 527 4.0 4.98466\n",
      "359 912 3.0 4.75844\n",
      "359 922 3.5 4.23665\n",
      "359 44555 2.5 3.70919\n",
      "359 1193 4.0 4.95885\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    x, y  = eval_test[k]\n",
    "    _,  y_hat = list(als_model.transform([x]))[0]\n",
    "    print(*x, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.531\n",
      "RMSE (test): 1.890\n",
      "RMSE for heavy: 1.855\n",
      "RMSE for moderate: 1.924\n",
      "RMSE for light: 1.555\n",
      "RMSE for accidental: 2.155\n"
     ]
    }
   ],
   "source": [
    "def _rmse(model, data):\n",
    "    x, y = zip(*data)\n",
    "    y_hat = list(r_hat for _, r_hat in model.transform(x))\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(y, y_hat))))\n",
    "\n",
    "def eval_rmse(model):\n",
    "    rmse = _rmse(model, eval_train)\n",
    "    print('RMSE (train): {:,.3f}'.format(rmse))\n",
    "    \n",
    "    rmse = _rmse(model, eval_test)\n",
    "    print('RMSE (test): {:,.3f}'.format(rmse))\n",
    "\n",
    "    for cluster in Cluster:\n",
    "        eval_data = eval_clusters[cluster]\n",
    "        rmse = _rmse(model, eval_data)\n",
    "        print('RMSE for {}: {:,.3f}'.format(cluster, rmse))\n",
    "\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "RMSE: 2.033\n",
      "RMSE: 0.781\n",
      "RMSE: 0.639\n",
      "RMSE: 0.597\n",
      "RMSE: 0.575\n",
      "RMSE: 0.560\n",
      "RMSE: 0.549\n",
      "RMSE: 0.541\n",
      "RMSE: 0.534\n",
      "RMSE: 0.529\n",
      "\n",
      "Evaluation...\n",
      "\n",
      "RMSE (train): 0.529\n",
      "RMSE (test): 1.652\n",
      "RMSE for heavy: 1.663\n",
      "RMSE for moderate: 1.650\n",
      "RMSE for light: 1.426\n",
      "RMSE for accidental: 1.824\n"
     ]
    }
   ],
   "source": [
    "als = ALSRecommender(num_factors=10, num_iters=10, reg=0.1)\n",
    "print('Training...\\n')\n",
    "als_model = als.fit(train_data, verbose=True)\n",
    "print('\\nEvaluation...\\n')\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test top items for 73:\n",
      "\n",
      "[1] 58559, 5.00\n",
      "[1] 1201, 5.00\n",
      "[1] 27773, 5.00\n",
      "[1] 32, 5.00\n",
      "[1] 4369, 5.00\n",
      "[1] 215, 5.00\n",
      "[2] 36, 4.50\n",
      "[2] 1218, 4.50\n",
      "[2] 26547, 4.50\n",
      "[2] 87192, 4.50\n",
      "[2] 7099, 4.50\n",
      "[2] 48774, 4.50\n",
      "[2] 3328, 4.50\n",
      "[2] 2948, 4.50\n",
      "[2] 2692, 4.50\n",
      "[2] 27022, 4.50\n",
      "[2] 2144, 4.50\n",
      "[2] 74458, 4.50\n",
      "[2] 2467, 4.50\n",
      "[2] 27317, 4.50\n",
      "\n",
      "Recommendations for 73:\n",
      "\n",
      "[1] 994, 5.41, -\n",
      "[2] 1635, 5.36, -\n",
      "[3] 2022, 5.30, -\n",
      "[4] 3060, 5.29, -\n",
      "[5] 1931, 5.24, -\n",
      "[6] 1253, 5.13, -\n",
      "[7] 7256, 5.11, -\n",
      "[8] 2729, 5.05, -\n",
      "[9] 1066, 5.02, -\n",
      "[10] 7771, 5.02, -\n"
     ]
    }
   ],
   "source": [
    "# eval_clusters: dict[cluster: Cluster, list[((user_id: str, item_id: str), rating: float)]]\n",
    "# 0 -> first user-item-pair-and-rating, 0 -> user-item-pair, 0 -> user\n",
    "user_id = eval_clusters[Cluster.HEAVY][0][0][0] \n",
    "\n",
    "user_items = sorted([(r.item_id, r.rating)\n",
    "                     for r in test_data.ratings\n",
    "                     if r.user_id == user_id \\\n",
    "                         and r.item_id in train_data.items],\n",
    "                    key=lambda r: r[1],\n",
    "                    reverse=True)\n",
    "\n",
    "items_exclude = set(r.item_id for r in train_data.ratings if r.user_id == user_id)\n",
    "\n",
    "rec_items = als_model.recommend(user_id, items_exclude=items_exclude)\n",
    "\n",
    "user_top = dict()\n",
    "p_rating = None\n",
    "p = 0\n",
    "print('Test top items for {}:\\n'.format(user_id))\n",
    "for i, (item_id, rating) in enumerate(user_items):\n",
    "    if p_rating is None or p_rating > rating:\n",
    "        p_rating = rating\n",
    "        p += 1\n",
    "    user_top[item_id] = p\n",
    "    if i < 20:\n",
    "        print('[{}] {}, {:,.2f}'.format(p, item_id, rating))\n",
    "print()\n",
    "\n",
    "p_rating = None\n",
    "p = 0\n",
    "print('Recommendations for {}:\\n'.format(user_id))\n",
    "for item_id, rating in rec_items:\n",
    "    if p_rating is None or p_rating > rating:\n",
    "        p_rating = rating\n",
    "        p += 1\n",
    "    print('[{}] {}, {:,.2f}, {}'.format(p, item_id, rating, user_top.get(item_id, '-')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ALS\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/ml-collaborative-filtering.html\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#module-pyspark.ml.recommendation\n",
    "\n",
    "https://github.com/apache/spark/blob/v2.1.0/examples/src/main/python/ml/als_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME=os.path.abspath('../../software/spark-2.1.0-bin-hadoop2.7')\n",
    "\n",
    "if not os.path.isdir(SPARK_HOME):\n",
    "    raise Exception('File not found: {}'.format(SPARK_HOME))\n",
    "\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '4g'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.append(os.path.join(SPARK_HOME, 'python', 'lib', 'py4j-0.10.4-src.zip'))\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import atexit\n",
    "atexit.register(lambda: spark.stop())\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|383 |21  |3.0   |\n",
      "|383 |47  |5.0   |\n",
      "|383 |1079|3.0   |\n",
      "|409 |16  |4.0   |\n",
      "|409 |21  |5.0   |\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_raw = [(int(r.user_id), int(r.item_id), r.rating) for r in train_data.ratings]\n",
    "train_df = spark.createDataFrame(train_raw, ['user', 'item', 'rating'])\n",
    "train_df.printSchema()\n",
    "train_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|359 |6385|4.5   |\n",
      "|359 |1295|4.5   |\n",
      "|359 |3258|3.5   |\n",
      "|359 |318 |4.0   |\n",
      "|359 |858 |3.5   |\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_raw = [(int(r.user_id), int(r.item_id), r.rating)\n",
    "            for r in test_data.ratings\n",
    "            if r.user_id in common_users \\\n",
    "                and r.item_id in common_items]\n",
    "test_df = spark.createDataFrame(test_raw, ['user', 'item', 'rating'])\n",
    "test_df.printSchema()\n",
    "test_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_clusters_spark = dict()\n",
    "for cluster, data in eval_clusters.items():\n",
    "    eval_data = [(int(user_id), int(item_id), rating) for (user_id, item_id), rating in data]\n",
    "    eval_df = spark.createDataFrame(eval_data, ['user', 'item', 'rating'])\n",
    "    eval_clusters_spark[cluster] = eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.605\n",
      "RMSE (test): 1.007\n",
      "RMSE for heavy: 1.043\n",
      "RMSE for moderate: 0.960\n",
      "RMSE for light: 1.075\n",
      "RMSE for accidental: 1.009\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS as SparkALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def eval_rmse_spark(model):\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                    labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "    pred_df = model.transform(train_df)\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    print('RMSE (train): {:,.3f}'.format(rmse))\n",
    "\n",
    "    pred_df = model.transform(test_df)\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    print('RMSE (test): {:,.3f}'.format(rmse))\n",
    "\n",
    "    for cluster in Cluster:\n",
    "        eval_df = eval_clusters_spark[cluster]\n",
    "        pred_df = model.transform(eval_df)\n",
    "        rmse = evaluator.evaluate(pred_df)\n",
    "        print('RMSE for {}: {:,.3f}'.format(cluster, rmse))\n",
    "\n",
    "spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)\n",
    "spark_model = spark_als.fit(train_df)\n",
    "eval_rmse_spark(spark_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.605\n",
      "RMSE (test): 1.007\n",
      "RMSE for heavy: 1.043\n",
      "RMSE for moderate: 0.960\n",
      "RMSE for light: 1.075\n",
      "RMSE for accidental: 1.009\n"
     ]
    }
   ],
   "source": [
    "spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)\n",
    "spark_model = spark_als.fit(train_df)\n",
    "eval_rmse_spark(spark_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Params: 5 5 1e-05\n",
      "Score: 3.791\n",
      "best update!\n",
      "\n",
      "Params: 10 5 1e-05\n",
      "Score: 12.805\n",
      "\n",
      "Params: 20 5 1e-05\n",
      "Score: 32.945\n",
      "\n",
      "Params: 50 5 1e-05\n",
      "Score: 923.598\n",
      "\n",
      "Params: 5 5 0.001\n",
      "Score: 19.489\n",
      "\n",
      "Params: 10 5 0.001\n",
      "Score: 3.581\n",
      "best update!\n",
      "\n",
      "Params: 20 5 0.001\n",
      "Score: 4.634\n",
      "\n",
      "Params: 50 5 0.001\n",
      "Score: 6.230\n",
      "\n",
      "Params: 5 5 0.1\n",
      "Score: 1.870\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.1\n",
      "Score: 1.557\n",
      "best update!\n",
      "\n",
      "Params: 20 5 0.1\n",
      "Score: 1.724\n",
      "\n",
      "Params: 50 5 0.1\n",
      "Score: 2.196\n",
      "\n",
      "Params: 5 5 1\n",
      "Score: 1.526\n",
      "best update!\n",
      "\n",
      "Params: 10 5 1\n",
      "Score: 1.763\n",
      "\n",
      "Params: 20 5 1\n",
      "Score: 2.047\n",
      "\n",
      "Params: 50 5 1\n",
      "Score: 2.377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5, 5, 1), 1.5261158099815511)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = dict(num_factors=[5, 10, 20, 50, 100, 200],\n",
    "                      num_iters=[5, 10, 25],\n",
    "                      reg = [1e-5, 1e-3, 1e-1, 0.0, 1])\n",
    "\n",
    "small_params = dict(num_factors=[5, 10, 20, 50],\n",
    "                    num_iters=[5],\n",
    "                    reg = [1e-5, 1e-3, 1e-1, 1])\n",
    "\n",
    "def grid_search(eval_func, params=default_params, verbose=False):\n",
    "    best_score = None\n",
    "    best_params = None\n",
    "    for reg in params['reg']:\n",
    "        for num_iters in params['num_iters']:\n",
    "            for num_factors in params['num_factors']:\n",
    "                if verbose:\n",
    "                    print('\\nParams:', num_factors, num_iters, reg)\n",
    "                try:\n",
    "                    score = eval_func(num_factors, num_iters, reg)\n",
    "                except:\n",
    "                    score = None\n",
    "                if verbose:\n",
    "                    print('Score:', '{:,.3f}'.format(score) if score is not None else '-')\n",
    "                if score is not None and (best_score is None or score < best_score):\n",
    "                    if verbose:\n",
    "                        print('best update!')\n",
    "                    best_score = score\n",
    "                    best_params = (num_factors, num_iters, reg)\n",
    "    return best_params, best_score\n",
    "\n",
    "def tf_eval(num_factors, num_iters, reg):\n",
    "    als = ALSRecommender(num_factors=num_factors, num_iters=num_iters, reg=reg)\n",
    "    model = als.fit(train_data)\n",
    "    return _rmse(model, eval_test)\n",
    "\n",
    "grid_search(tf_eval, params=small_params, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Params: 5 5 1e-05\n",
      "Score: 3.510\n",
      "best update!\n",
      "\n",
      "Params: 10 5 1e-05\n",
      "Score: 2.218\n",
      "best update!\n",
      "\n",
      "Params: 20 5 1e-05\n",
      "Score: 2.032\n",
      "best update!\n",
      "\n",
      "Params: 50 5 1e-05\n",
      "Score: 2.727\n",
      "\n",
      "Params: 5 5 0.001\n",
      "Score: 1.611\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.001\n",
      "Score: 1.457\n",
      "best update!\n",
      "\n",
      "Params: 20 5 0.001\n",
      "Score: 1.662\n",
      "\n",
      "Params: 50 5 0.001\n",
      "Score: 2.306\n",
      "\n",
      "Params: 5 5 0.1\n",
      "Score: 0.994\n",
      "best update!\n",
      "\n",
      "Params: 10 5 0.1\n",
      "Score: 1.000\n",
      "\n",
      "Params: 20 5 0.1\n",
      "Score: 1.006\n",
      "\n",
      "Params: 50 5 0.1\n",
      "Score: 1.023\n",
      "\n",
      "Params: 5 5 1\n",
      "Score: 1.311\n",
      "\n",
      "Params: 10 5 1\n",
      "Score: 1.311\n",
      "\n",
      "Params: 20 5 1\n",
      "Score: 1.311\n",
      "\n",
      "Params: 50 5 1\n",
      "Score: 1.311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5, 5, 0.1), 0.9938391167559549)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spark_eval(num_factors, num_iters, reg):\n",
    "    als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)\n",
    "    model = als.fit(train_df)\n",
    "    pred_df = model.transform(test_df)\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                    labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "    return evaluator.evaluate(pred_df)\n",
    "\n",
    "grid_search(spark_eval, params=small_params, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.0 (CPU, Python 3)",
   "language": "python",
   "name": "tensorflow-1.0-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
