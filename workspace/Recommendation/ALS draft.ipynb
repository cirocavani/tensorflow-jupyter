{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Collaborative Filtering with Matrix Factorization (ALS)\n",
    "\n",
    "PoC\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens Dataset\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv.\n",
    "\n",
    "(README for more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "DATA_DIR = 'movielens'\n",
    "DATASET_URL = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "DATASET_FILENAME = DATASET_URL.split('/')[-1]\n",
    "DATASET_PACKAGE = os.path.join(DATA_DIR, DATASET_FILENAME)\n",
    "DATASET_PATH = os.path.join(DATA_DIR, DATASET_FILENAME[:-4])\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "if not os.path.isfile(DATASET_PACKAGE):\n",
    "    print('Downloading {}...'.format(DATASET_FILENAME))\n",
    "    r = requests.get(DATASET_URL, stream=True)\n",
    "    with open(DATASET_PACKAGE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Done!')\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print('Unpacking {}...'.format(DATASET_PACKAGE))\n",
    "    with zipfile.ZipFile(DATASET_PACKAGE, 'r') as f:\n",
    "        f.extractall(DATA_DIR)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['links.csv',\n",
       " 'movies.csv',\n",
       " 'ratings.csv',\n",
       " 'README.txt',\n",
       " 'tags.csv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n",
      "1,31,2.5,1260759144\n",
      "1,1029,3.0,1260759179\n",
      "1,1061,3.0,1260759182\n",
      "1,1129,2.0,1260759185\n",
      "1,1172,4.0,1260759205\n",
      "1,1263,2.0,1260759151\n",
      "1,1287,2.0,1260759187\n",
      "1,1293,2.0,1260759148\n",
      "1,1339,3.5,1260759125\n"
     ]
    }
   ],
   "source": [
    "RATINGS_CSV = os.path.join(DATASET_PATH, 'ratings.csv')\n",
    "\n",
    "with open(RATINGS_CSV, 'r') as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 100,004\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "Rating = collections.namedtuple('Rating', ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "ratings = list()\n",
    "with open(RATINGS_CSV, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # skip header\n",
    "    for user_id, item_id, rating, timestamp in reader:\n",
    "        ratings.append(Rating(user_id, item_id, float(rating), int(timestamp)))\n",
    "\n",
    "ratings = sorted(ratings, key=lambda r: r.timestamp)\n",
    "\n",
    "print('Ratings: {:,}'.format(len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Index:\n",
      "i=0, user_id=1\n",
      "i=1, user_id=2\n",
      "i=2, user_id=3\n",
      "i=3, user_id=4\n",
      "i=4, user_id=5\n",
      "i=5, user_id=6\n",
      "i=6, user_id=7\n",
      "i=7, user_id=8\n",
      "i=8, user_id=9\n",
      "i=9, user_id=10\n"
     ]
    }
   ],
   "source": [
    "users_from_idx = sorted(set(r.user_id for r in ratings), key=int)\n",
    "users_from_idx = dict(enumerate(users_from_idx))\n",
    "users_to_idx = dict((user_id, idx) for idx, user_id in users_from_idx.items())\n",
    "print('User Index:')\n",
    "for i in range(10):\n",
    "    print('i={}, user_id={}'.format(i, users_from_idx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Index:\n",
      "j=0, item_id=1\n",
      "j=1, item_id=2\n",
      "j=2, item_id=3\n",
      "j=3, item_id=4\n",
      "j=4, item_id=5\n",
      "j=5, item_id=6\n",
      "j=6, item_id=7\n",
      "j=7, item_id=8\n",
      "j=8, item_id=9\n",
      "j=9, item_id=10\n"
     ]
    }
   ],
   "source": [
    "items_from_idx = sorted(set(r.item_id for r in ratings), key=int)\n",
    "items_from_idx = dict(enumerate(items_from_idx))\n",
    "items_to_idx = dict((item_id, idx) for idx, item_id in items_from_idx.items())\n",
    "print('Item Index:')\n",
    "for j in range(10):\n",
    "    print('j={}, item_id={}'.format(j, items_from_idx[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=Tensor(\"SparseTensor/indices:0\", shape=(100004, 2), dtype=int64), values=Tensor(\"SparseTensor/values:0\", shape=(100004,), dtype=float32), dense_shape=Tensor(\"SparseTensor/dense_shape:0\", shape=(2,), dtype=int64))\n",
      "\n",
      "Total values: 6,083,286\n"
     ]
    }
   ],
   "source": [
    "indices = [(users_to_idx[r.user_id], items_to_idx[r.item_id]) for r in ratings]\n",
    "values = [r.rating for r in ratings]\n",
    "n_rows = len(users_from_idx)\n",
    "n_cols = len(items_from_idx)\n",
    "shape = (n_rows, n_cols)\n",
    "\n",
    "P = tf.SparseTensor(indices, values, shape)\n",
    "\n",
    "print(P)\n",
    "print()\n",
    "print('Total values: {:,}'.format(n_rows * n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.factorization import WALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WALSModel in module tensorflow.contrib.factorization.python.ops.factorization_ops:\n",
      "\n",
      "class WALSModel(builtins.object)\n",
      " |  A model for Weighted Alternating Least Squares matrix factorization.\n",
      " |  \n",
      " |  It minimizes the following loss function over U, V:\n",
      " |   \\\\( ||W \\odot (A - U V^T) ||_F^2 + \\lambda (||U||_F^2 + ||V||_F^2) )\\\\\n",
      " |    where,\n",
      " |    A: input matrix,\n",
      " |    W: weight matrix,\n",
      " |    U, V: row_factors and column_factors matrices,\n",
      " |    \\\\(\\lambda)\\\\: regularization.\n",
      " |  Also we assume that W is of the following special form:\n",
      " |  \\\\( W_{ij} = W_0 + R_i * C_j )\\\\  if \\\\(A_{ij} \\ne 0)\\\\,\n",
      " |  \\\\(W_{ij} = W_0)\\\\ otherwise.\n",
      " |  where,\n",
      " |  \\\\(W_0)\\\\: unobserved_weight,\n",
      " |  \\\\(R_i)\\\\: row_weights,\n",
      " |  \\\\(C_j)\\\\: col_weights.\n",
      " |  \n",
      " |  Note that the current implementation supports two operation modes: The default\n",
      " |  mode is for the condition where row_factors and col_factors can individually\n",
      " |  fit into the memory of each worker and these will be cached. When this\n",
      " |  condition can't be met, setting use_factors_weights_cache to False allows the\n",
      " |  larger problem sizes with slight performance penalty as this will avoid\n",
      " |  creating the worker caches and instead the relevant weight and factor values\n",
      " |  are looked up from parameter servers at each step.\n",
      " |  \n",
      " |  A typical usage example (pseudocode):\n",
      " |  \n",
      " |    with tf.Graph().as_default():\n",
      " |      # Set up the model object.\n",
      " |      model = tf.contrib.factorization.WALSModel(....)\n",
      " |  \n",
      " |      # To be run only once as part of session initialization. In distributed\n",
      " |      # training setting, this should only be run by the chief trainer and all\n",
      " |      # other trainers should block until this is done.\n",
      " |      model_init_op = model.initialize_op\n",
      " |  \n",
      " |      # To be run once per worker after session is available, prior to\n",
      " |      # the gramian_prep_ops for row(column) can be run.\n",
      " |      worker_init_op = model.worker_init\n",
      " |  \n",
      " |      # To be run once per interation sweep before the row(column) update\n",
      " |      # initialize ops can be run. Note that in the distributed training\n",
      " |      # situations, this should only be run by the chief trainer. All other\n",
      " |      # trainers need to block until this is done.\n",
      " |      row_update_gramian_prep_op = model.row_update_prep_gramian_op\n",
      " |      col_update_gramian_prep_op = model.col_update_prep_gramian_op\n",
      " |  \n",
      " |      # To be run once per worker per iteration sweep. Must be run before\n",
      " |      # any actual update ops can be run.\n",
      " |      init_row_update_op = model.initialize_row_update_op\n",
      " |      init_col_update_op = model.initialize_col_update_op\n",
      " |  \n",
      " |      # Ops to upate row(column). This can either take the entire sparse tensor\n",
      " |      # or slices of sparse tensor. For distributed trainer, each trainer\n",
      " |      # handles just part of the matrix.\n",
      " |      row_update_op = model.update_row_factors(\n",
      " |           sp_input=matrix_slices_from_queue_for_worker_shard)[1]\n",
      " |      col_update_op = model.update_col_factors(\n",
      " |           sp_input=transposed_matrix_slices_from_queue_for_worker_shard,\n",
      " |           transpose_input=True)[1]\n",
      " |  \n",
      " |      ...\n",
      " |  \n",
      " |      # model_init_op is passed to Supervisor. Chief trainer runs it. Other\n",
      " |      # trainers wait.\n",
      " |      sv = tf.Supervisor(is_chief=is_chief,\n",
      " |                         ...,\n",
      " |                         init_op=tf.group(..., model_init_op, ...), ...)\n",
      " |      ...\n",
      " |  \n",
      " |      with sv.managed_session(...) as sess:\n",
      " |        # All workers/trainers run it after session becomes available.\n",
      " |        worker_init_op.run(session=sess)\n",
      " |  \n",
      " |        ...\n",
      " |  \n",
      " |        while i in iterations:\n",
      " |  \n",
      " |          # All trainers need to sync up here.\n",
      " |          while not_all_ready:\n",
      " |            wait\n",
      " |  \n",
      " |          # Row update sweep.\n",
      " |          if is_chief:\n",
      " |            row_update_gramian_prep_op.run(session=sess)\n",
      " |          else:\n",
      " |            wait_for_chief\n",
      " |  \n",
      " |          # All workers run upate initialization.\n",
      " |          init_row_update_op.run(session=sess)\n",
      " |  \n",
      " |          # Go through the matrix.\n",
      " |          reset_matrix_slices_queue_for_worker_shard\n",
      " |          while_matrix_slices:\n",
      " |            row_update_op.run(session=sess)\n",
      " |  \n",
      " |          # All trainers need to sync up here.\n",
      " |          while not_all_ready:\n",
      " |            wait\n",
      " |  \n",
      " |          # Column update sweep.\n",
      " |          if is_chief:\n",
      " |            col_update_gramian_prep_op.run(session=sess)\n",
      " |          else:\n",
      " |            wait_for_chief\n",
      " |  \n",
      " |          # All workers run upate initialization.\n",
      " |          init_col_update_op.run(session=sess)\n",
      " |  \n",
      " |          # Go through the matrix.\n",
      " |          reset_transposed_matrix_slices_queue_for_worker_shard\n",
      " |          while_transposed_matrix_slices:\n",
      " |            col_update_op.run(session=sess)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_rows, input_cols, n_components, unobserved_weight=0.1, regularization=None, row_init='random', col_init='random', num_row_shards=1, num_col_shards=1, row_weights=1, col_weights=1, use_factors_weights_cache=True)\n",
      " |      Creates model for WALS matrix factorization.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_rows: total number of rows for input matrix.\n",
      " |        input_cols: total number of cols for input matrix.\n",
      " |        n_components: number of dimensions to use for the factors.\n",
      " |        unobserved_weight: weight given to unobserved entries of matrix.\n",
      " |        regularization: weight of L2 regularization term. If None, no\n",
      " |          regularization is done.\n",
      " |        row_init: initializer for row factor. Can be a tensor or numpy constant.\n",
      " |          If set to \"random\", the value is initialized randomly.\n",
      " |        col_init: initializer for column factor. See row_init for details.\n",
      " |        num_row_shards: number of shards to use for row factors.\n",
      " |        num_col_shards: number of shards to use for column factors.\n",
      " |        row_weights: Must be in one of the following three formats: None, a list\n",
      " |          of lists of non-negative real numbers (or equivalent iterables) or a\n",
      " |          single non-negative real number.\n",
      " |          - When set to None, w_ij = unobserved_weight, which simplifies to ALS.\n",
      " |          Note that col_weights must also be set to \"None\" in this case.\n",
      " |          - If it is a list of lists of non-negative real numbers, it needs to be\n",
      " |          in the form of [[w_0, w_1, ...], [w_k, ... ], [...]], with the number of\n",
      " |          inner lists matching the number of row factor shards and the elements in\n",
      " |          each inner list are the weights for the rows of the corresponding row\n",
      " |          factor shard. In this case,  w_ij = unonbserved_weight +\n",
      " |                                              row_weights[i] * col_weights[j].\n",
      " |          - If this is a single non-negative real number, this value is used for\n",
      " |          all row weights and w_ij = unobserved_weight + row_weights *\n",
      " |                                     col_weights[j].\n",
      " |          Note that it is allowed to have row_weights as a list while col_weights\n",
      " |          a single number or vice versa.\n",
      " |        col_weights: See row_weights.\n",
      " |        use_factors_weights_cache: When True, the factors and weights will be\n",
      " |          cached on the workers before the updates start. Defaults to True.\n",
      " |  \n",
      " |  project_col_factors(self, sp_input=None, transpose_input=False, projection_weights=None)\n",
      " |      Projects the column factors.\n",
      " |      \n",
      " |      This computes the column embedding v_j for an observed column a_j by solving\n",
      " |      one iteration of the update equations.\n",
      " |      \n",
      " |      Args:\n",
      " |        sp_input: A SparseTensor representing a set of columns. Please note that\n",
      " |          the row indices of this SparseTensor must match the model row feature\n",
      " |          indexing while the column indices are ignored. The returned results will\n",
      " |          be in the same ordering as the input columns.\n",
      " |        transpose_input: If true, the input will be logically transposed and the\n",
      " |          columns corresponding to the transposed input are projected.\n",
      " |        projection_weights: The column weights to be used for the projection. If\n",
      " |          None then 1.0 is used. This can be either a scaler or a rank-1 tensor\n",
      " |          with the number of elements matching the number of columns to be\n",
      " |          projected. Note that the row weights will be determined by the\n",
      " |          underlying WALS model.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Projected column factors.\n",
      " |  \n",
      " |  project_row_factors(self, sp_input=None, transpose_input=False, projection_weights=None)\n",
      " |      Projects the row factors.\n",
      " |      \n",
      " |      This computes the row embedding u_i for an observed row a_i by solving\n",
      " |      one iteration of the update equations.\n",
      " |      \n",
      " |      Args:\n",
      " |        sp_input: A SparseTensor representing a set of rows. Please note that the\n",
      " |          column indices of this SparseTensor must match the model column feature\n",
      " |          indexing while the row indices are ignored. The returned results will be\n",
      " |          in the same ordering as the input rows.\n",
      " |        transpose_input: If true, the input will be logically transposed and the\n",
      " |          rows corresponding to the transposed input are projected.\n",
      " |        projection_weights: The row weights to be used for the projection. If None\n",
      " |          then 1.0 is used. This can be either a scaler or a rank-1 tensor with\n",
      " |          the number of elements matching the number of rows to be projected.\n",
      " |          Note that the column weights will be determined by the underlying WALS\n",
      " |          model.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Projected row factors.\n",
      " |  \n",
      " |  update_col_factors(self, sp_input=None, transpose_input=False)\n",
      " |      Updates the column factors.\n",
      " |      \n",
      " |      Args:\n",
      " |        sp_input: A SparseTensor representing a subset of columns of the full\n",
      " |          input. Please refer to comments for update_row_factors for\n",
      " |          restrictions.\n",
      " |        transpose_input: If true, the input will be logically transposed and the\n",
      " |          columns corresponding to the transposed input are updated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple consisting of the following two elements:\n",
      " |        new_values: New values for the column factors.\n",
      " |        update_op: An op that assigns the newly computed values to the column\n",
      " |          factors.\n",
      " |  \n",
      " |  update_row_factors(self, sp_input=None, transpose_input=False)\n",
      " |      Updates the row factors.\n",
      " |      \n",
      " |      Args:\n",
      " |        sp_input: A SparseTensor representing a subset of rows of the full input\n",
      " |          in any order. Please note that this SparseTensor must retain the\n",
      " |          indexing as the original input.\n",
      " |        transpose_input: If true, the input will be logically transposed and the\n",
      " |          rows corresponding to the transposed input are updated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple consisting of the following two elements:\n",
      " |        new_values: New values for the row factors.\n",
      " |        update_op: An op that assigns the newly computed values to the row\n",
      " |          factors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  scatter_update(factor, indices, values, sharding_func) from builtins.type\n",
      " |      Helper function for doing sharded scatter update.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  col_factors\n",
      " |      Returns a list of tensors corresponding to column factor shards.\n",
      " |  \n",
      " |  col_update_prep_gramian_op\n",
      " |      Op to form the gramian before starting col updates.\n",
      " |      \n",
      " |      Must be run before initialize_col_update_op and should only be run by one\n",
      " |      trainer (usually the chief) when doing distributed training.\n",
      " |  \n",
      " |  col_weights\n",
      " |      Returns a list of tensors corresponding to col weight shards.\n",
      " |  \n",
      " |  initialize_col_update_op\n",
      " |      Op to initialize worker state before starting column updates.\n",
      " |  \n",
      " |  initialize_op\n",
      " |      Returns an op for initializing tensorflow variables.\n",
      " |  \n",
      " |  initialize_row_update_op\n",
      " |      Op to initialize worker state before starting row updates.\n",
      " |  \n",
      " |  row_factors\n",
      " |      Returns a list of tensors corresponding to row factor shards.\n",
      " |  \n",
      " |  row_update_prep_gramian_op\n",
      " |      Op to form the gramian before starting row updates.\n",
      " |      \n",
      " |      Must be run before initialize_row_update_op and should only be run by one\n",
      " |      trainer (usually the chief) when doing distributed training.\n",
      " |  \n",
      " |  row_weights\n",
      " |      Returns a list of tensors corresponding to row weight shards.\n",
      " |  \n",
      " |  worker_init\n",
      " |      Op to initialize worker state once before starting any updates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WALSModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.223\n",
      "RMSE: 0.813\n",
      "RMSE: 0.644\n",
      "RMSE: 0.597\n",
      "RMSE: 0.573\n",
      "RMSE: 0.559\n",
      "RMSE: 0.549\n",
      "RMSE: 0.542\n",
      "RMSE: 0.537\n",
      "RMSE: 0.533\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 10\n",
    "reg = 1e-1\n",
    "\n",
    "model = WALSModel(\n",
    "    n_rows,\n",
    "    n_cols,\n",
    "    k,\n",
    "    regularization=reg,\n",
    "    unobserved_weight=0)\n",
    "\n",
    "row_factors = tf.nn.embedding_lookup(\n",
    "    model.row_factors,\n",
    "    tf.range(model._input_rows),\n",
    "    partition_strategy=\"div\")\n",
    "col_factors = tf.nn.embedding_lookup(\n",
    "    model.col_factors,\n",
    "    tf.range(model._input_cols),\n",
    "    partition_strategy=\"div\")\n",
    "\n",
    "row_indices, col_indices = tf.split(P.indices,\n",
    "                                    axis=1,\n",
    "                                    num_or_size_splits=2)\n",
    "gathered_row_factors = tf.gather(row_factors, row_indices)\n",
    "gathered_col_factors = tf.gather(col_factors, col_indices)\n",
    "approx_vals = tf.squeeze(tf.matmul(gathered_row_factors,\n",
    "                                   gathered_col_factors,\n",
    "                                   adjoint_b=True))\n",
    "P_approx = tf.SparseTensor(indices=P.indices,\n",
    "                           values=approx_vals,\n",
    "                           dense_shape=P.dense_shape)\n",
    "\n",
    "E = tf.sparse_add(P, P_approx * (-1))\n",
    "E2 = tf.square(E)\n",
    "n_P = P.values.shape[0].value\n",
    "rmse_op = tf.sqrt(tf.sparse_reduce_sum(E2) / n_P)\n",
    "\n",
    "row_update_op = model.update_row_factors(sp_input=P)[1]\n",
    "col_update_op = model.update_col_factors(sp_input=P)[1]\n",
    "\n",
    "model.initialize_op.run()\n",
    "model.worker_init.run()\n",
    "for _ in range(n):\n",
    "    # Update Users\n",
    "    model.row_update_prep_gramian_op.run()\n",
    "    model.initialize_row_update_op.run()\n",
    "    row_update_op.run()\n",
    "    # Update Items\n",
    "    model.col_update_prep_gramian_op.run()\n",
    "    model.initialize_col_update_op.run()\n",
    "    col_update_op.run()\n",
    "\n",
    "    print('RMSE: {:,.3f}'.format(rmse_op.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User factors shape: (671, 10)\n",
      "Item factors shape: (9066, 10)\n"
     ]
    }
   ],
   "source": [
    "user_factors = model.row_factors[0].eval()\n",
    "item_factors = model.col_factors[0].eval()\n",
    "\n",
    "print('User factors shape:', user_factors.shape)\n",
    "print('Item factors shape:', item_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most havy user 547: 2,391\n"
     ]
    }
   ],
   "source": [
    "c = collections.Counter(r.user_id for r in ratings)\n",
    "user_id, n_ratings = c.most_common(1)[0]\n",
    "print('Most havy user {}: {:,d}'.format(user_id, n_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5-rating from 547:\n",
      "\n",
      "Rating(user_id='547', item_id='163949', rating=5.0, timestamp=1476419239)\n"
     ]
    }
   ],
   "source": [
    "r = next(r for r in reversed(ratings) if r.user_id == user_id and r.rating == 5.0)\n",
    "print('Last 5-rating from {}:\\n'.format(user_id))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating ratings 'predictions'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors for user 547:\n",
      "\n",
      "[ 0.17587891 -0.09659031 -0.1773065  -0.198863    0.17917389  0.12711532\n",
      " -1.15592897  0.12968333  0.27661979 -0.48321217]\n",
      "\n",
      "Factors for item 163949:\n",
      "\n",
      "[ 0.45741466 -0.25120869 -0.46113154 -0.51719838  0.46598965  0.33059701\n",
      " -3.00630188  0.33727586  0.71942407 -1.2567178 ]\n",
      "\n",
      "Approx. rating: 4.740, diff=0.260, 94.798%\n"
     ]
    }
   ],
   "source": [
    "i = users_to_idx[r.user_id]\n",
    "j = items_to_idx[r.item_id]\n",
    "\n",
    "u = user_factors[i]\n",
    "print('Factors for user {}:\\n'.format(r.user_id))\n",
    "print(u)\n",
    "print()\n",
    "\n",
    "v = item_factors[j]\n",
    "print('Factors for item {}:\\n'.format(r.item_id))\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "p = np.dot(u, v)\n",
    "print('Approx. rating: {:,.3f}, diff={:,.3f}, {:,.3%}'.format(p, r.rating - p, p/r.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User preference shape: (9066,)\n",
      "\n",
      "Top 10 items:\n",
      "\n",
      "[1] 2622 6.54\n",
      "[2] 1959 6.10\n",
      "[3] 2594 5.94\n",
      "[4] 2912 5.91\n",
      "[5] 2287 5.91\n",
      "[6] 1920 5.73\n",
      "[7] 585 5.68\n",
      "[8] 282 5.64\n",
      "[9] 4016 5.51\n",
      "[10] 2126 5.39\n"
     ]
    }
   ],
   "source": [
    "V = item_factors\n",
    "\n",
    "user_P = np.dot(V, u)\n",
    "print('User preference shape:', user_P.shape)\n",
    "print()\n",
    "\n",
    "user_items = set(ur.item_id for ur in ratings if ur.user_id == user_id)\n",
    "\n",
    "user_ranking_idx = sorted(enumerate(user_P), key=lambda p: p[1], reverse=True)\n",
    "user_ranking_raw = ((items_from_idx[j], p) for j, p in user_ranking_idx)\n",
    "user_ranking = [(item_id, p) for item_id, p in user_ranking_raw if item_id not in user_items]\n",
    "\n",
    "top10 = user_ranking[:10]\n",
    "\n",
    "print('Top 10 items:\\n')\n",
    "for k, (item_id, p) in enumerate(top10):\n",
    "    print('[{}] {} {:,.2f}'.format(k+1, item_id, p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.0 (CPU, Python 3)",
   "language": "python",
   "name": "tensorflow-1.0-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
