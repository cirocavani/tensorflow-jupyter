{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual LSTM Encoder for Dialog Response Generation**\n",
    "\n",
    "http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/\n",
    "\n",
    "https://github.com/dennybritz/chatbot-retrieval\n",
    "\n",
    "https://github.com/rkadlec/ubuntu-ranking-dataset-creator\n",
    "\n",
    "https://arxiv.org/abs/1506.08909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/39225039\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(share_id, filename):\n",
    "    GOOGLE_DRIVE_URL = \"https://drive.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(GOOGLE_DRIVE_URL, params={'id': share_id}, stream=True)\n",
    "    \n",
    "    token = None\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            token = value\n",
    "            break\n",
    "\n",
    "    if not token:\n",
    "        raise Exception('Token not found')\n",
    "        \n",
    "    params = {'id': share_id, 'confirm': token}\n",
    "    response = session.get(GOOGLE_DRIVE_URL, params=params, stream=True)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(32768):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "HOME_DIR = 'ubuntu'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "DATASET_FILENAME = 'udc.tar.gz'\n",
    "DATASET_PACKAGE = os.path.join(DATA_DIR, DATASET_FILENAME)\n",
    "SHARE_ID = '0B_bZck-ksdkpVEtVc1R6Y01HMWM'\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "VALID_CSV = os.path.join(DATA_DIR, 'valid.csv')\n",
    "TEST_CSV = os.path.join(DATA_DIR, 'test.csv')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "train_missing = not os.path.isfile(TRAIN_CSV)\n",
    "valid_missing = not os.path.isfile(VALID_CSV)\n",
    "test_missing = not os.path.isfile(TEST_CSV)\n",
    "\n",
    "missing = train_missing or valid_missing or test_missing\n",
    "\n",
    "if missing and not os.path.isfile(DATASET_PACKAGE):\n",
    "    print('Downloading {}...'.format(DATASET_FILENAME))\n",
    "    download_file_from_google_drive(SHARE_ID, DATASET_PACKAGE)\n",
    "    print('Done!')\n",
    "\n",
    "def extract(tar, filename, dst_path):\n",
    "    print('Extracting', filename)\n",
    "    dst_file = os.path.join(dst_path, os.path.basename(filename))\n",
    "    with open(dst_file, 'wb') as fout:\n",
    "        fin = tar.extractfile(filename)\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "if missing:\n",
    "    with tarfile.open(DATASET_PACKAGE, mode='r:gz') as t:\n",
    "        if train_missing:\n",
    "            extract(t, './data/train.csv', DATA_DIR)\n",
    "        if valid_missing:\n",
    "            extract(t, './data/valid.csv', DATA_DIR)\n",
    "        if test_missing:\n",
    "            extract(t, './data/test.csv', DATA_DIR)\n",
    "\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(file, lines=3):\n",
    "    with open(file, 'r') as f:\n",
    "        for _ in range(lines):\n",
    "            print(next(f).strip())\n",
    "            print()\n",
    "\n",
    "print('Train samples...\\n')\n",
    "show(TRAIN_CSV)\n",
    "print('...\\n')\n",
    "print('Validation samples...\\n')\n",
    "show(VALID_CSV)\n",
    "print('...\\n')\n",
    "print('Test samples...\\n')\n",
    "show(TEST_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.contrib.learn.preprocessing.VocabularyProcessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration**\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* `max_document_length`\n",
    "* `min_frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('figure', figsize=(16.0, 8.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df.Label = train_df.Label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train_df.Label.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_context_len = train_df.Context.str.split().str.len()\n",
    "context_stats = train_context_len.describe()\n",
    "print(context_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "Q1 = context_stats['25%']\n",
    "Q3 = context_stats['75%']\n",
    "\n",
    "max_len = math.ceil((Q3 + 1.5 * (Q3 - Q1)) / 10) * 10\n",
    "\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_len.hist(bins=100)\n",
    "plt.axvline(max_len, color='r')\n",
    "plt.title('Training Context Length Statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_utterance_len = train_df.Utterance.str.split().str.len()\n",
    "print(train_utterance_len.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utterance_len.hist(bins=100)\n",
    "plt.axvline(max_len, color='r')\n",
    "plt.title('Training Utterance Length Statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_context_len\n",
    "del train_utterance_len\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import collections\n",
    "\n",
    "tokens_freq = collections.Counter()\n",
    "for _, (c, u, _) in train_df.iterrows():\n",
    "    context_tokens = c.split()\n",
    "    utterance_tokens = u.split()\n",
    "    tokens_freq.update(context_tokens)\n",
    "    tokens_freq.update(utterance_tokens)\n",
    "\n",
    "print(len(tokens_freq))\n",
    "print()\n",
    "for token, freq in tokens_freq.most_common(10):\n",
    "    print('{:,d}\\t{}'.format(freq, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_5 = list((token, freq) for token, freq in tokens_freq.items() if freq >= 5)\n",
    "len(tokens_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_5_df = pd.Series(data=list(freq for _, freq in tokens_5))\n",
    "tokens_5_stats = tokens_5_df.describe()\n",
    "print(tokens_5_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = tokens_5_stats['25%']\n",
    "Q3 = tokens_5_stats['75%']\n",
    "\n",
    "high_freq = Q3 + 1.5 * (Q3 - Q1)\n",
    "\n",
    "high_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_5_df[tokens_5_df <= high_freq].hist(bins=50)\n",
    "plt.title('Training Tokens Statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del tokens_freq\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_iterator(filename, cols=[]):\n",
    "    with open(filename, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            if not cols:\n",
    "                yield row\n",
    "            else:\n",
    "                for j in cols:\n",
    "                    yield row[j]\n",
    "\n",
    "def train_iterator():\n",
    "    return csv_iterator(TRAIN_CSV, cols=[0, 1])\n",
    "\n",
    "train_iter = train_iterator()\n",
    "for k in range(1, 3):\n",
    "    print('[', k, '] Context\\n')\n",
    "    print(next(train_iter), '\\n')\n",
    "    print('[', k, '] Utterance\\n')\n",
    "    print(next(train_iter), '\\n')\n",
    "del train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tokenizer(sentences):\n",
    "    return (sentence.split() for sentence in sentences)\n",
    "\n",
    "vocab = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "    max_document_length=160, min_frequency=5, tokenizer_fn=tokenizer)\n",
    "\n",
    "vocab.fit(train_iterator())\n",
    "\n",
    "print('Vocabulary size: {:,d}'.format(len(vocab.vocabulary_)))\n",
    "print('Document Length: {:,d}'.format(vocab.max_document_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence0 = next(train_iterator())\n",
    "vector0 = next(vocab.transform([sentence0]))\n",
    "\n",
    "print('Sentence (tokens={:,d}):\\n'.format(len(sentence0.split())))\n",
    "print(sentence0, '\\n')\n",
    "print('Vector (length={:,d}):\\n'.format(len(vector0)))\n",
    "print(vector0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle will not serialize `tokenizer` function (must be defined before restoring the vocabulary object)\n",
    "VOCABULARY_FILE = os.path.join(DATA_DIR, 'vocabulary.bin')\n",
    "\n",
    "if os.path.isfile(VOCABULARY_FILE):\n",
    "    os.remove(VOCABULARY_FILE)\n",
    "\n",
    "vocab.save(VOCABULARY_FILE)\n",
    "\n",
    "os.path.isfile(VOCABULARY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export data (TFRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyAdapter:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self._vocab = vocab\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._vocab.vocabulary_)\n",
    "\n",
    "    @property\n",
    "    def vector_length(self):\n",
    "        return self._vocab.max_document_length\n",
    "    \n",
    "    def transform(self, sentence):\n",
    "        return next(self._vocab.transform([sentence]))\n",
    "    \n",
    "    def tokens(self, sentence):\n",
    "        return next(self._vocab._tokenizer([sentence]))\n",
    "    \n",
    "vocab_ = VocabularyAdapter(vocab)\n",
    "\n",
    "tokens0 = vocab_.tokens(sentence0)\n",
    "vector0 = vocab_.transform(sentence0)\n",
    "\n",
    "print('Vocabulary size:\\n\\n{:,d}\\n'.format(vocab_.size))\n",
    "print('Vector length:\\n\\n{:,d}\\n'.format(vocab_.vector_length))\n",
    "print('Sentence:\\n')\n",
    "print(sentence0, '\\n')\n",
    "print('Tokens (length={:,d}):\\n'.format(len(tokens0)))\n",
    "print(tokens0, '\\n')\n",
    "print('Vector (length={:,d}):\\n'.format(len(vector0)))\n",
    "print(vector0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_example(vocab_, **kwargs):\n",
    "    example = tf.train.Example()\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        if isinstance(value, str):\n",
    "            vector = vocab_.transform(value)\n",
    "            length = min(vocab_.vector_length, len(vocab_.tokens(value)))\n",
    "            example.features.feature[key].int64_list.value.extend(vector)\n",
    "            example.features.feature[key + '_len'].int64_list.value.extend([length])\n",
    "        elif isinstance(value, int):\n",
    "            example.features.feature[key].int64_list.value.extend([value])\n",
    "        else:\n",
    "            raise Exception('Unknown: {}:{}'.format(key, type(value)))\n",
    "    \n",
    "    return example\n",
    "\n",
    "with open(TRAIN_CSV, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # skip header\n",
    "    \n",
    "    for c, u, l in reader:\n",
    "        example = create_example(vocab_, context=c, utterance=u, label=int(l))\n",
    "    \n",
    "        example_str = str(example)\n",
    "        #print(example_str)\n",
    "        print(example_str[:107])\n",
    "        print('  ...')\n",
    "        print(example_str[-106:-1])\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VALID_CSV, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # skip header\n",
    "    \n",
    "    for c, u, *d in reader:\n",
    "        d_ = dict(('distractor_{}'.format(i), u_) for i, u_ in enumerate(d))\n",
    "        example = create_example(vocab_, context=c, utterance=u, **d_)\n",
    "        \n",
    "        example_str = str(example)\n",
    "        #print(example_str)\n",
    "        print(example_str[:107])\n",
    "        print('  ...')\n",
    "        print(example_str[-106:-1])\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith('.tfrecords'):\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        print('Removing {}...'.format(path))\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_examples(vocab_, filename):\n",
    "    with open(filename, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for c, u, l in reader:\n",
    "            yield create_example(vocab_, context=c, utterance=u, label=int(l))\n",
    "\n",
    "def eval_examples(vocab_, filename):\n",
    "    with open(filename, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for c, u, *d in reader:\n",
    "            d_ = dict(('distractor_{}'.format(i), u_) for i, u_ in enumerate(d))\n",
    "            yield create_example(vocab_, context=c, utterance=u, **d_)\n",
    "\n",
    "def save_tfrecords(input_examples, filename):\n",
    "    print(\"Saving TFRecords at {}...\".format(filename))\n",
    "    n = 0\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for x in input_examples:\n",
    "            writer.write(x.SerializeToString())\n",
    "            n += 1\n",
    "    print('Total records: {:,d}'.format(n))\n",
    "\n",
    "TRAIN_TFR = os.path.join(DATA_DIR, 'train.tfrecords')\n",
    "%time save_tfrecords(train_examples(vocab_, TRAIN_CSV), TRAIN_TFR)\n",
    "\n",
    "VALID_TFR = os.path.join(DATA_DIR, 'valid.tfrecords')\n",
    "%time save_tfrecords(eval_examples(vocab_, VALID_CSV), VALID_TFR)\n",
    "\n",
    "TEST_TFR = os.path.join(DATA_DIR, 'test.tfrecords')\n",
    "%time save_tfrecords(eval_examples(vocab_, TEST_CSV), TEST_TFR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
