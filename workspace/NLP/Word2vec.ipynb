{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = 'wikipedia'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "TEXT_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "TEXT_FILENAME = TEXT_URL.split('/')[-1]\n",
    "TEXT_FILE = os.path.join(DATA_DIR, TEXT_FILENAME)\n",
    "\n",
    "text_missing = not os.path.isfile(TEXT_FILE)\n",
    "\n",
    "if text_missing:\n",
    "    print('Downloading {}...'.format(TEXT_FILENAME))\n",
    "    r = requests.get(TEXT_URL, stream=True)\n",
    "    with open(TEXT_FILE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=32768):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti...\n",
      "\n",
      "(99,998,000 chars)\n",
      "\n",
      "...ousand defenders had set all the buildings but the food storerooms ablaze and committed mass suicide rather than face certain capture or defeat by their enemies because the jewish religion discourages the act of suicide however the defenders were reported to have drawn lots and slain each other in turn down to the last man who would be the only one to actually take his own life as per josephus account the argument is made that the storerooms were left standing to show that the defenders retained the ability to live and chose the time of their death this account of the siege of masada was apparently related to josephus by two women who survived the suicide by hiding inside a cistern along with five children and repeated elazar ben yair s final exortation to his followers prior to the mass suicide verbatim to the romans the site today platform access to the fortress the site of masada was identified in one eight four two and extensively excavated in one nine six three one nine six five b\n"
     ]
    }
   ],
   "source": [
    "def load_raw_text_from_zip(file):\n",
    "    with zipfile.ZipFile(file) as f:\n",
    "        return f.read(f.namelist()[0]).decode('utf-8')\n",
    "\n",
    "raw_text = load_raw_text_from_zip(TEXT_FILE)\n",
    "\n",
    "print('{}...\\n\\n({:,d} chars)\\n\\n...{}'.format(\n",
    "    raw_text[:1000], len(raw_text) - 2000, raw_text[-1000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words (total):\n",
      "\n",
      "17,005,207\n",
      "\n",
      "Words (unique):\n",
      "\n",
      "253,854\n",
      "\n",
      "Most common:\n",
      "\n",
      "the (1,061,396)\n",
      "of (593,677)\n",
      "and (416,629)\n",
      "one (411,764)\n",
      "in (372,201)\n",
      "a (325,873)\n",
      "to (316,376)\n",
      "zero (264,975)\n",
      "nine (250,430)\n",
      "two (192,644)\n",
      "is (183,153)\n",
      "as (131,815)\n",
      "eight (125,285)\n",
      "for (118,445)\n",
      "s (116,710)\n",
      "five (115,789)\n",
      "three (114,775)\n",
      "was (112,807)\n",
      "by (111,831)\n",
      "that (109,510)\n",
      "\n",
      "Least common:\n",
      "\n",
      "jebe (1)\n",
      "mncs (1)\n",
      "intitially (1)\n",
      "privolnoye (1)\n",
      "gennadi (1)\n",
      "gorbachyova (1)\n",
      "democratised (1)\n",
      "clandenstine (1)\n",
      "buildups (1)\n",
      "gorby (1)\n",
      "kajn (1)\n",
      "gorbacheva (1)\n",
      "mikhailgorbachev (1)\n",
      "englander (1)\n",
      "workmans (1)\n",
      "erniest (1)\n",
      "metzada (1)\n",
      "metzuda (1)\n",
      "fretensis (1)\n",
      "exortation (1)\n"
     ]
    }
   ],
   "source": [
    "words = raw_text.split()\n",
    "words_freq = collections.Counter(words).most_common()\n",
    "\n",
    "print('Words (total):\\n\\n{:,d}\\n'.format(len(words)))\n",
    "print('Words (unique):\\n\\n{:,d}\\n'.format(len(words_freq)))\n",
    "print('Most common:\\n')\n",
    "for word, freq in words_freq[:20]:\n",
    "    print('{} ({:,d})'.format(word, freq))\n",
    "print('\\nLeast common:\\n')\n",
    "for word, freq in words_freq[-20:]:\n",
    "    print('{} ({:,d})'.format(word, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words 10+: 47,134\n"
     ]
    }
   ],
   "source": [
    "words_10plus = sum(1 for _, freq in words_freq if freq >= 10)\n",
    "\n",
    "print('Words 10+: {:,d}'.format(words_10plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aggadic', 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 50_000\n",
    "\n",
    "words_freq[vocabulary_size - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for the vocabulary: 49,999\n"
     ]
    }
   ],
   "source": [
    "words_vocab = words_freq[:(vocabulary_size-1)]\n",
    "\n",
    "print('Words for the vocabulary: {:,d}'.format(len(words_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n"
     ]
    }
   ],
   "source": [
    "UNK_ID = 0\n",
    "word_to_id = dict((word, word_id) for word_id, (word, _) in enumerate(words_vocab, UNK_ID+1))\n",
    "word_to_id['UNK'] = UNK_ID\n",
    "word_from_id = dict((word_id, word) for word, word_id in word_to_id.items())\n",
    "\n",
    "print('Vocabulary size: {:d}'.format(len(word_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK words: 203,855\n",
      "UNK frequency: 418,391\n"
     ]
    }
   ],
   "source": [
    "words_to_unk = words_freq[(vocabulary_size-1):]\n",
    "unk_freq = sum(freq for _, freq in words_to_unk)\n",
    "\n",
    "print('UNK words: {:,d}'.format(len(words_to_unk)))\n",
    "print('UNK frequency: {:,d}'.format(unk_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file size: 418,684 bytes\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_FILE = os.path.join(HOME_DIR, 'vocabulary.txt')\n",
    "\n",
    "with open(VOCABULARY_FILE, 'w') as f:\n",
    "    for word_id in range(vocabulary_size):\n",
    "        f.write(word_from_id[word_id] + '\\n')\n",
    "\n",
    "print('Vocabulary file size: {:,d} bytes'.format(os.stat(VOCABULARY_FILE).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50,000\n"
     ]
    }
   ],
   "source": [
    "with open(VOCABULARY_FILE, newline='') as f:\n",
    "    word_from_id_ = dict((word_id, word.strip()) for word_id, word in enumerate(f))\n",
    "    word_to_id_ = dict((word, word_id) for word_id, word in word_from_id_.items())\n",
    "\n",
    "# print(word_from_id_)\n",
    "print('Vocabulary size: {:,d}'.format(len(word_to_id_)))\n",
    "assert word_to_id_ == word_to_id\n",
    "assert word_from_id_ == word_from_id\n",
    "del word_to_id_, word_from_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "\n",
      "17,005,207\n",
      "\n",
      "Text (IDs):\n",
      "\n",
      "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
      "\n",
      "Text (Words):\n",
      "\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data = list(word_to_id.get(word, UNK_ID) for word in words)\n",
    "\n",
    "print('Size:\\n\\n{:,d}\\n'.format(len(data)))\n",
    "print('Text (IDs):\\n\\n{}\\n'.format(data[:10]))\n",
    "print('Text (Words):\\n\\n{}'.format(list(word_from_id[word_id] for word_id in data[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_window(window_words, target_index):\n",
    "    words = list(window_words)\n",
    "    del words[target_index]\n",
    "    return words\n",
    "\n",
    "def input_cbow(data, batch_size, window_size):\n",
    "    if window_size % 2 == 0 or window_size < 3 \\\n",
    "        or window_size > (len(data) - batch_size) / 2:\n",
    "        # {window_size} must be odd: (n words left) target (n words right)\n",
    "        raise Exception(\n",
    "            'Invalid parameters: window_size must be a small odd number')\n",
    "\n",
    "    num_words = len(data)\n",
    "    num_windows = num_words - window_size + 1\n",
    "    num_batches = num_windows // batch_size\n",
    "    target_index = window_size // 2\n",
    "    \n",
    "    words = collections.deque(data[window_size:])\n",
    "    window_words = collections.deque(data[:window_size], maxlen=window_size)\n",
    "    \n",
    "    for n in range(num_batches):\n",
    "        batch = np.ndarray(shape=(batch_size, window_size-1), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch[i,:] = context_window(window_words, target_index)\n",
    "            labels[i, 0] = window_words[target_index]\n",
    "            window_words.append(words.popleft())\n",
    "\n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      "\n",
      " anarchism originated as a term of \n",
      "\n",
      "Batch 1\n",
      "\n",
      "[anarchism, as] -> originated\n",
      "[originated, a] -> as\n",
      "\n",
      "Batch 2\n",
      "\n",
      "[as, term] -> a\n",
      "[a, of] -> term\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "window_size = 3\n",
    "num_iters = 2\n",
    "num_words = window_size + num_iters * batch_size - 1\n",
    "text = ' '.join(word_from_id[word_id] for word_id in data[:num_words])\n",
    "print('Text\\n\\n', text, '\\n')\n",
    "\n",
    "data_iter = input_cbow(data, batch_size, window_size)\n",
    "for k in range(1, num_iters+1):\n",
    "    print('Batch {}\\n'.format(k))\n",
    "    batch_context, batch_target = next(data_iter)\n",
    "    for i in range(batch_size):\n",
    "        context_words = ', '.join(\n",
    "            word_from_id[word_id] for word_id in batch_context[i, :])\n",
    "        target_word = word_from_id[batch_target[i, 0]]\n",
    "        print('[{}] -> {}'.format(context_words, target_word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f24573f4160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "context_size = 2\n",
    "vocabulary_size = 20\n",
    "embedding_size = 3\n",
    "num_sampled = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(4, 2), dtype=int32) \n",
      "\n",
      "[[ 1 13]\n",
      " [12 19]\n",
      " [ 2  6]\n",
      " [ 6 10]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(np.random.randint(low=0,\n",
    "                                  high=vocabulary_size,\n",
    "                                  size=(batch_size, context_size),\n",
    "                                  dtype=np.int32))\n",
    "\n",
    "print(X, '\\n')\n",
    "print(X.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(4, 1), dtype=int32) \n",
      "\n",
      "[[12]\n",
      " [14]\n",
      " [ 4]\n",
      " [19]]\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(np.random.randint(low=0,\n",
    "                                  high=vocabulary_size,\n",
    "                                  size=(batch_size, 1),\n",
    "                                  dtype=np.int32))\n",
    "\n",
    "print(y, '\\n')\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(20, 3) dtype=float32_ref> \n",
      "\n",
      "[[-0.72385198 -0.67064154  0.54728121]\n",
      " [ 0.52267903  0.14309864 -0.21202599]\n",
      " [ 0.83861756 -0.33369949  0.9575246 ]\n",
      " [ 0.96486568  0.56053215  0.49033678]\n",
      " [-0.50572079 -0.51958585 -0.73312479]\n",
      " [ 0.91628253 -0.02707585  0.9486317 ]\n",
      " [-0.06927377 -0.83878195  0.54863465]\n",
      " [-0.44515863 -0.57463926  0.65154487]\n",
      " [ 0.19133538 -0.53356606  0.61336142]\n",
      " [-0.60742682  0.64677942 -0.10579451]\n",
      " [-0.90827733  0.05203509 -0.94087756]\n",
      " [-0.62333119  0.57111442 -0.38464847]\n",
      " [-0.5805648   0.63791192 -0.90240276]\n",
      " [-0.72723395  0.40751234 -0.75468791]\n",
      " [-0.71756595  0.06584837 -0.73213542]\n",
      " [ 0.64148563  0.12542801 -0.12217065]\n",
      " [ 0.04314694 -0.74552041  0.77889448]\n",
      " [ 0.96567845 -0.91742957  0.55220294]\n",
      " [ 0.12808689 -0.78407633  0.60224926]\n",
      " [ 0.85479677  0.51144832  0.7075665 ]]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.random_uniform(shape=(vocabulary_size, embedding_size),\n",
    "#                     minval=-1.0, maxval=1.0)\n",
    "embeddings = tf.Variable(\n",
    "    2 * np.random.rand(vocabulary_size, embedding_size) - 1, dtype=tf.float32)\n",
    "\n",
    "embeddings.initializer.run()\n",
    "\n",
    "print(embeddings, '\\n')\n",
    "print(embeddings.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(4, 2, 3), dtype=float32) \n",
      "\n",
      "[[[ 0.52267903  0.14309864 -0.21202599]\n",
      "  [-0.72723395  0.40751234 -0.75468791]]\n",
      "\n",
      " [[-0.5805648   0.63791192 -0.90240276]\n",
      "  [ 0.85479677  0.51144832  0.7075665 ]]\n",
      "\n",
      " [[ 0.83861756 -0.33369949  0.9575246 ]\n",
      "  [-0.06927377 -0.83878195  0.54863465]]\n",
      "\n",
      " [[-0.06927377 -0.83878195  0.54863465]\n",
      "  [-0.90827733  0.05203509 -0.94087756]]]\n"
     ]
    }
   ],
   "source": [
    "X_embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "\n",
    "print(X_embed, '\\n')\n",
    "print(X_embed.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(4, 3), dtype=float32) \n",
      "\n",
      "[[-0.10227746  0.27530548 -0.48335695]\n",
      " [ 0.13711599  0.57468009 -0.09741813]\n",
      " [ 0.3846719  -0.58624071  0.75307965]\n",
      " [-0.48877555 -0.39337343 -0.19612145]]\n"
     ]
    }
   ],
   "source": [
    "X_avg = tf.reduce_mean(X_embed, axis=1)\n",
    "\n",
    "print(X_avg, '\\n')\n",
    "print(X_avg.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first dimension of each verctor of first context:\n",
      "\n",
      " [ 0.52267903 -0.72723395] \n",
      "\n",
      "first dimension avarage:\n",
      "\n",
      " -0.102277\n"
     ]
    }
   ],
   "source": [
    "c0_w0 = X_embed[0,:,0].eval()\n",
    "print('first dimension of each verctor of first context:\\n\\n', c0_w0, '\\n')\n",
    "print('first dimension avarage:\\n\\n', np.mean(c0_w0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_1:0' shape=(20, 3) dtype=float32_ref> \n",
      "\n",
      "[[-0.95111889  0.03992119  0.60304403]\n",
      " [-0.25201097  1.22195446  0.91662031]\n",
      " [ 0.59678656  1.06454647  0.11580566]\n",
      " [-0.52788448 -0.00793689 -0.57282221]\n",
      " [ 0.47062469  1.08456814  0.19608852]\n",
      " [ 0.80619788  0.15125319  0.05031686]\n",
      " [ 0.68311757 -0.86507517 -0.14866897]\n",
      " [-0.41752672  0.63992518 -0.928909  ]\n",
      " [-0.22810122 -0.79593885  0.03342727]\n",
      " [ 0.90696567 -0.15120924 -0.90413463]\n",
      " [-0.54319501  0.76286793 -0.88365149]\n",
      " [ 1.11157513  0.59892714  1.19862056]\n",
      " [ 0.14531991  0.58058619 -0.56140929]\n",
      " [ 1.47388983  0.75362307  0.09494124]\n",
      " [ 0.52572161 -0.39862293 -0.11135586]\n",
      " [-0.42785749 -0.67497361  0.45894542]\n",
      " [-0.24799375  0.48624259  0.1878079 ]\n",
      " [-0.16138858  0.05955968 -0.83776098]\n",
      " [ 0.24626839 -0.42496043  0.37001279]\n",
      " [-0.52473778 -0.04243804 -0.46187654]]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.truncated_normal(shape=(vocabulary_size, embedding_size),\n",
    "#                       stddev=1.0 / np.sqrt(embedding_size))\n",
    "W = tf.Variable(\n",
    "    np.random.randn(vocabulary_size, embedding_size) / np.sqrt(embedding_size),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "W.initializer.run()\n",
    "\n",
    "print(W, '\\n')\n",
    "print(W.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(20,) dtype=float32_ref> \n",
      "\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.zeros(shape=(vocabulary_size,))\n",
    "b = tf.Variable(np.zeros(vocabulary_size), dtype=tf.float32)\n",
    "\n",
    "b.initializer.run()\n",
    "\n",
    "print(b, '\\n')\n",
    "print(b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_2:0\", shape=(4,), dtype=float32) \n",
      "\n",
      "[ 0.30435109  0.51802754  1.2428782   0.18773946]\n"
     ]
    }
   ],
   "source": [
    "sampled_loss = tf.nn.sampled_softmax_loss(weights=W,\n",
    "                                          biases=b,\n",
    "                                          inputs=X_avg,\n",
    "                                          labels=y,\n",
    "                                          num_sampled=num_sampled,\n",
    "                                          num_classes=vocabulary_size)\n",
    "\n",
    "print(sampled_loss, '\\n')\n",
    "print(sampled_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32) \n",
      "\n",
      "0.683698\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(sampled_loss)\n",
    "\n",
    "print(loss, '\\n')\n",
    "print(loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del X, y, embeddings, X_embed, X_avg, c0_w0, W, b, sampled_loss, loss\n",
    "del graph, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cbow(vocabulary_size, embedding_size, num_sampled):\n",
    "    X = tf.placeholder_with_default([[0]], shape=(None, None), name='X')\n",
    "    y = tf.placeholder_with_default([[0]], shape=(None, 1), name='y')\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=(vocabulary_size, embedding_size),\n",
    "                          minval=-1.0, maxval=1.0),\n",
    "        name='embeddings')\n",
    "\n",
    "    X_embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "    X_avg = tf.reduce_mean(X_embed, axis=1)\n",
    "    \n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(shape=(vocabulary_size, embedding_size),\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)),\n",
    "        name='W')\n",
    "    softmax_biases = tf.Variable(\n",
    "        tf.zeros(shape=(vocabulary_size,)),\n",
    "        name='b')\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        sampled_loss = tf.nn.sampled_softmax_loss(weights=softmax_weights,\n",
    "                                                  biases=softmax_biases,\n",
    "                                                  inputs=X_avg,\n",
    "                                                  labels=y,\n",
    "                                                  num_sampled=num_sampled,\n",
    "                                                  num_classes=vocabulary_size)\n",
    "        loss = tf.reduce_mean(sampled_loss, name='mean')\n",
    "\n",
    "    norm = tf.norm(embeddings, axis=1, keep_dims=True)\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    return X, y, normalized_embeddings, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage loss: 0.659\n",
      "\n",
      "[[-0.35204613 -0.49285588 -0.79571134]\n",
      " [ 0.90027946  0.00527231 -0.43528083]\n",
      " [-0.45081681 -0.8457576   0.28540915]\n",
      " [ 0.93553668 -0.06053047  0.34800476]\n",
      " [ 0.62018192  0.77692211 -0.10847333]\n",
      " [-0.05211216  0.72057694  0.691414  ]\n",
      " [ 0.85853469 -0.40509552 -0.31434992]\n",
      " [ 0.65642017 -0.38012969  0.65162414]\n",
      " [ 0.25981882 -0.48941576  0.8324461 ]\n",
      " [ 0.02589937 -0.97443479 -0.22317269]\n",
      " [-0.81627828  0.2518996  -0.51984262]\n",
      " [ 0.69813287  0.71491545  0.03881122]\n",
      " [-0.76648676  0.62046701  0.16588779]\n",
      " [-0.03354356  0.7318964  -0.6805898 ]\n",
      " [-0.8009752   0.15727918 -0.5776695 ]\n",
      " [ 0.07402308 -0.83239329  0.54921955]\n",
      " [ 0.16653079 -0.84062201  0.51538551]\n",
      " [-0.01632886 -0.62661898  0.77915466]\n",
      " [ 0.73106146 -0.66975462  0.13029923]\n",
      " [ 0.16645701  0.67231679  0.72130591]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "context_size = 2\n",
    "vocabulary_size = 20\n",
    "embedding_size = 3\n",
    "num_sampled = 2\n",
    "\n",
    "with tf.Graph().as_default() as graph, \\\n",
    "    tf.Session(graph=graph) as session:\n",
    "\n",
    "    X, y, embeddings, loss_op = model_cbow(vocabulary_size,\n",
    "                                           embedding_size,\n",
    "                                           num_sampled)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    X_batch = np.random.randint(low=0,\n",
    "                                high=vocabulary_size,\n",
    "                                size=(batch_size, context_size),\n",
    "                                dtype=np.int32)\n",
    "    y_batch = np.random.randint(low=0,\n",
    "                                high=vocabulary_size,\n",
    "                                size=(batch_size, 1),\n",
    "                                dtype=np.int32)\n",
    "    feed_data = {X: X_batch, y: y_batch}\n",
    "\n",
    "    loss, embeddings_ = session.run([loss_op, embeddings], feed_dict=feed_data)\n",
    "\n",
    "    print('Avarage loss: {:,.3f}\\n'.format(loss))\n",
    "    print(embeddings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_window(window_words, target_index):\n",
    "    words = list(window_words)\n",
    "    del words[target_index]\n",
    "    return words\n",
    "\n",
    "def context_sample(context_words, sample_size):\n",
    "    return random.sample(context_words, sample_size)\n",
    "\n",
    "def context_skips(window_words, target_index, sample_size, use_sample):\n",
    "    words = context_window(window_words, target_index)\n",
    "    if use_sample:\n",
    "        words = context_sample(words, sample_size) \n",
    "    return words\n",
    "\n",
    "def input_skip_gram(data, batch_size, window_size, num_skips):\n",
    "    if window_size % 2 == 0 or window_size < 3 \\\n",
    "        or window_size > (len(data) - batch_size) / 2:\n",
    "        # {window_size} must be odd: (n words left) target (n words right)\n",
    "        raise Exception(\n",
    "            'Invalid parameters: window_size must be a small odd number')\n",
    "    if num_skips > window_size - 1:\n",
    "        # It is not possible to generate {num_skips} different pairs\n",
    "        # with the second word coming from {window_size - 1} words.\n",
    "        raise Exception(\n",
    "            'Invalid parameters: num_skips={}, window_size={}'.format(\n",
    "                num_skips, window_size))\n",
    "\n",
    "    num_words = len(data)\n",
    "    num_windows = num_words - window_size + 1\n",
    "    num_batches = num_windows * num_skips // batch_size\n",
    "    target_index = window_size // 2\n",
    "    use_sample = num_skips < window_size - 1\n",
    "\n",
    "    words = collections.deque(data[window_size:])\n",
    "    window_words = collections.deque(data[:window_size], maxlen=window_size)\n",
    "    target_word = window_words[target_index]\n",
    "    context_words = context_skips(window_words,\n",
    "                                  target_index,\n",
    "                                  num_skips,\n",
    "                                  use_sample)\n",
    "\n",
    "    for n in range(num_batches):\n",
    "        batch = np.ndarray(shape=(batch_size,), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch[i] = target_word\n",
    "            labels[i, 0] = context_words.pop()\n",
    "            if not context_words:\n",
    "                window_words.append(words.popleft())\n",
    "                target_word = window_words[target_index]\n",
    "                context_words = context_skips(window_words,\n",
    "                                              target_index,\n",
    "                                              num_skips,\n",
    "                                              use_sample)\n",
    "\n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      "\n",
      " anarchism originated as a \n",
      "\n",
      "Batch 1\n",
      "\n",
      "originated -> as\n",
      "originated -> anarchism\n",
      "\n",
      "Batch 2\n",
      "\n",
      "as -> a\n",
      "as -> originated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "window_size = 3\n",
    "num_skips = 2\n",
    "num_iters = 2\n",
    "num_words = window_size + num_iters * batch_size // num_skips - 1\n",
    "text = ' '.join(word_from_id[word_id] for word_id in data[:num_words])\n",
    "print('Text\\n\\n', text, '\\n')\n",
    "\n",
    "data_iter = input_skip_gram(data, batch_size, window_size, num_skips)\n",
    "for k in range(1, num_iters+1):\n",
    "    print('Batch {}\\n'.format(k))\n",
    "    batch_target, batch_context = next(data_iter)\n",
    "    for i in range(batch_size):\n",
    "        target_word = word_from_id[batch_target[i]]\n",
    "        context_word = word_from_id[batch_context[i, 0]]\n",
    "        print('{} -> {}'.format(target_word, context_word))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f2457464e48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "vocabulary_size = 20\n",
    "embedding_size = 3\n",
    "num_sampled = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(4,), dtype=int32) \n",
      "\n",
      "[ 1 13  7 14]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(np.random.randint(low=0,\n",
    "                                  high=vocabulary_size,\n",
    "                                  size=(batch_size,),\n",
    "                                  dtype=np.int32))\n",
    "\n",
    "print(X, '\\n')\n",
    "print(X.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(4, 1), dtype=int32) \n",
      "\n",
      "[[5]\n",
      " [8]\n",
      " [4]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(np.random.randint(low=0,\n",
    "                                  high=vocabulary_size,\n",
    "                                  size=(batch_size, 1),\n",
    "                                  dtype=np.int32))\n",
    "\n",
    "print(y, '\\n')\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(20, 3) dtype=float32_ref> \n",
      "\n",
      "[[-0.06405666 -0.36442509  0.09976698]\n",
      " [ 0.25951034 -0.92962027  0.29985774]\n",
      " [-0.07028455  0.21121742  0.75075257]\n",
      " [-0.0650936  -0.7252804   0.72301978]\n",
      " [-0.32284504  0.03299018 -0.09381296]\n",
      " [-0.39663857 -0.24010691 -0.71467304]\n",
      " [-0.40799075  0.31186756  0.7314164 ]\n",
      " [ 0.47215068 -0.03772468 -0.52421725]\n",
      " [ 0.72830141  0.5357722  -0.0377    ]\n",
      " [-0.81130475  0.77849263  0.3450298 ]\n",
      " [-0.69996762 -0.48169607  0.65136951]\n",
      " [ 0.05303586 -0.98581433  0.8968181 ]\n",
      " [ 0.72462726  0.97945833  0.76254189]\n",
      " [-0.25559229 -0.44913685 -0.3040261 ]\n",
      " [ 0.94620019  0.64193922  0.59860039]\n",
      " [ 0.72233653  0.12625705 -0.33893391]\n",
      " [ 0.84457082  0.03674772 -0.04018029]\n",
      " [ 0.24594459 -0.20061782 -0.84947133]\n",
      " [ 0.81655848 -0.19471201  0.66972214]\n",
      " [ 0.12862396 -0.88693208  0.59677744]]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.random_uniform(shape=(vocabulary_size, embedding_size),\n",
    "#                     minval=-1.0, maxval=1.0)\n",
    "embeddings = tf.Variable(\n",
    "    2 * np.random.rand(vocabulary_size, embedding_size) - 1, dtype=tf.float32)\n",
    "\n",
    "embeddings.initializer.run()\n",
    "\n",
    "print(embeddings, '\\n')\n",
    "print(embeddings.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(4, 3), dtype=float32) \n",
      "\n",
      "[[ 0.25951034 -0.92962027  0.29985774]\n",
      " [-0.25559229 -0.44913685 -0.3040261 ]\n",
      " [ 0.47215068 -0.03772468 -0.52421725]\n",
      " [ 0.94620019  0.64193922  0.59860039]]\n"
     ]
    }
   ],
   "source": [
    "X_embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "\n",
    "print(X_embed, '\\n')\n",
    "print(X_embed.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_1:0' shape=(20, 3) dtype=float32_ref> \n",
      "\n",
      "[[ 1.79186618  0.24223483  0.07708248]\n",
      " [-0.88965774 -0.37394619 -0.14469676]\n",
      " [ 0.2534678  -0.13020152 -0.27742332]\n",
      " [-0.05867928 -0.02351343  0.37994054]\n",
      " [ 0.47904381 -0.90131861 -0.06806456]\n",
      " [-0.47352576  0.84705657 -0.39652976]\n",
      " [-0.38010231 -0.75899756 -0.37158328]\n",
      " [ 0.28823701 -0.59937847 -1.06404757]\n",
      " [ 0.1208643   0.36876652  0.44599587]\n",
      " [ 0.32415268 -0.21567565 -0.09200609]\n",
      " [-0.04196546  0.42018735 -0.81791347]\n",
      " [-0.11105991 -0.78590721 -0.36691234]\n",
      " [-1.52351332 -0.98226297 -0.14049503]\n",
      " [-0.69547641 -1.20471764  0.14849003]\n",
      " [ 0.43098313 -0.44809586 -0.36235431]\n",
      " [-0.44773129 -1.59782124  0.34702504]\n",
      " [-0.57949483 -0.18733098  1.04739034]\n",
      " [-0.07380351 -0.61254019  0.40654027]\n",
      " [ 0.15701263  0.83129191  0.49949175]\n",
      " [ 0.20048323 -0.60173309 -0.37230048]]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.truncated_normal(shape=(vocabulary_size, embedding_size),\n",
    "#                       stddev=1.0 / np.sqrt(embedding_size))\n",
    "W = tf.Variable(\n",
    "    np.random.randn(vocabulary_size, embedding_size) / np.sqrt(embedding_size),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "W.initializer.run()\n",
    "\n",
    "print(W, '\\n')\n",
    "print(W.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(20,) dtype=float32_ref> \n",
      "\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# ~ tf.zeros(shape=(vocabulary_size,))\n",
    "b = tf.Variable(np.zeros(vocabulary_size), dtype=tf.float32)\n",
    "\n",
    "b.initializer.run()\n",
    "\n",
    "print(b, '\\n')\n",
    "print(b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_2:0\", shape=(4,), dtype=float32) \n",
      "\n",
      "[ 1.2450664   0.51244265  0.71858579  1.3334657 ]\n"
     ]
    }
   ],
   "source": [
    "sampled_loss = tf.nn.sampled_softmax_loss(weights=W,\n",
    "                                          biases=b,\n",
    "                                          inputs=X_embed,\n",
    "                                          labels=y,\n",
    "                                          num_sampled=num_sampled,\n",
    "                                          num_classes=vocabulary_size)\n",
    "\n",
    "print(sampled_loss, '\\n')\n",
    "print(sampled_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32) \n",
      "\n",
      "1.06209\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(sampled_loss)\n",
    "\n",
    "print(loss, '\\n')\n",
    "print(loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del X, y, embeddings, X_embed, W, b, sampled_loss, loss\n",
    "del graph, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_skip_gram(vocabulary_size, embedding_size, num_sampled):\n",
    "    X = tf.placeholder_with_default([0], shape=(None,), name='X')\n",
    "    y = tf.placeholder_with_default([[0]], shape=(None, 1), name='y')\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=(vocabulary_size, embedding_size),\n",
    "                          minval=-1.0, maxval=1.0),\n",
    "        name='embeddings')\n",
    "\n",
    "    X_embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(shape=(vocabulary_size, embedding_size),\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)),\n",
    "        name='W')\n",
    "    softmax_biases = tf.Variable(\n",
    "        tf.zeros(shape=(vocabulary_size,)),\n",
    "        name='b')\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        sampled_loss = tf.nn.sampled_softmax_loss(weights=softmax_weights,\n",
    "                                                  biases=softmax_biases,\n",
    "                                                  inputs=X_embed,\n",
    "                                                  labels=y,\n",
    "                                                  num_sampled=num_sampled,\n",
    "                                                  num_classes=vocabulary_size)\n",
    "        loss = tf.reduce_mean(sampled_loss, name='mean')\n",
    "\n",
    "    norm = tf.norm(embeddings, axis=1, keep_dims=True)\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    return X, y, normalized_embeddings, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage loss: 0.848\n",
      "\n",
      "[[ 0.36134785 -0.71822989  0.59462047]\n",
      " [ 0.3036077   0.49467725 -0.81431985]\n",
      " [-0.24205504  0.0727653   0.96753013]\n",
      " [ 0.49863282  0.29093915 -0.8165291 ]\n",
      " [ 0.87326252 -0.36429736 -0.32357374]\n",
      " [ 0.47323585 -0.74552345  0.46930027]\n",
      " [-0.52143198  0.1009413   0.8473013 ]\n",
      " [-0.24573667  0.50146824  0.82954383]\n",
      " [-0.43916667 -0.76209062  0.47576305]\n",
      " [ 0.47012448  0.49093866  0.73345912]\n",
      " [ 0.51530111 -0.67711431  0.52533883]\n",
      " [ 0.87959242  0.45998335 -0.12137818]\n",
      " [ 0.45687237 -0.80557758 -0.37724325]\n",
      " [-0.073541    0.7254191   0.68436748]\n",
      " [-0.59740579 -0.767735   -0.23170929]\n",
      " [ 0.78319734  0.52396584 -0.3347564 ]\n",
      " [ 0.85303044 -0.51144564  0.10374222]\n",
      " [-0.52321196 -0.6087727  -0.59635991]\n",
      " [ 0.58028489 -0.66106236 -0.47567412]\n",
      " [-0.90289181 -0.34156856 -0.260993  ]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "vocabulary_size = 20\n",
    "embedding_size = 3\n",
    "num_sampled = 2\n",
    "\n",
    "with tf.Graph().as_default() as graph, \\\n",
    "    tf.Session(graph=graph) as session:\n",
    "\n",
    "    X, y, embeddings, loss_op = model_skip_gram(vocabulary_size,\n",
    "                                                embedding_size,\n",
    "                                                num_sampled)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    X_batch = np.random.randint(low=0,\n",
    "                                high=vocabulary_size,\n",
    "                                size=(batch_size,),\n",
    "                                dtype=np.int32)\n",
    "    y_batch = np.random.randint(low=0,\n",
    "                                high=vocabulary_size,\n",
    "                                size=(batch_size, 1),\n",
    "                                dtype=np.int32)\n",
    "    feed_data = {X: X_batch, y: y_batch}\n",
    "\n",
    "    loss, embeddings_ = session.run([loss_op, embeddings], feed_dict=feed_data)\n",
    "\n",
    "    print('Avarage loss: {:,.3f}\\n'.format(loss))\n",
    "    print(embeddings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7ff0d18c44a8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(4, 2), dtype=float32) \n",
      "\n",
      "[[ 3.  4.]\n",
      " [ 4.  3.]\n",
      " [-3.  4.]\n",
      " [-4.  3.]]\n"
     ]
    }
   ],
   "source": [
    "v_0 = tf.constant([3, 4], dtype=tf.float32)\n",
    "v_1 = tf.constant([4, 3], dtype=tf.float32)\n",
    "v_2 = tf.constant([-3, 4], dtype=tf.float32)\n",
    "v_3 = tf.constant([-4, 3], dtype=tf.float32)\n",
    "\n",
    "V = tf.stack([v_0, v_1, v_2, v_3])\n",
    "\n",
    "print(V, '\\n')\n",
    "print(V.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"norm/Sqrt:0\", shape=(4, 1), dtype=float32) \n",
      "\n",
      "[[ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "V_norm = tf.norm(V, axis=1, keep_dims=True)\n",
    "\n",
    "print(V_norm, '\\n')\n",
    "print(V_norm.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(4, 2), dtype=float32) \n",
      "\n",
      "[[ 0.60000002  0.80000001]\n",
      " [ 0.80000001  0.60000002]\n",
      " [-0.60000002  0.80000001]\n",
      " [-0.80000001  0.60000002]]\n"
     ]
    }
   ],
   "source": [
    "U = V / V_norm\n",
    "\n",
    "print(U, '\\n')\n",
    "print(U.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DiagPart:0\", shape=(4,), dtype=float32) \n",
      "\n",
      "[ 1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "UU = tf.diag_part(tf.matmul(U, U, transpose_b=True))\n",
    "\n",
    "print(UU, '\\n')\n",
    "print(UU.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(1, 2), dtype=float32) \n",
      "\n",
      "[[ 0.60000002  0.80000001]]\n"
     ]
    }
   ],
   "source": [
    "i = tf.constant([0], dtype=tf.int32)\n",
    "\n",
    "u_i = tf.nn.embedding_lookup(U, i)\n",
    "\n",
    "print(u_i, '\\n')\n",
    "print(u_i.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(1, 4), dtype=float32) \n",
      "\n",
      "[[  1.00000000e+00   9.60000038e-01   2.80000001e-01   7.15255766e-09]]\n"
     ]
    }
   ],
   "source": [
    "S = tf.matmul(u_i, U, transpose_b=True)\n",
    "\n",
    "print(S, '\\n')\n",
    "print(S.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TopKV2:0\", shape=(1, 2), dtype=float32) \n",
      "\n",
      "[[ 1.          0.96000004]] \n",
      "\n",
      "Tensor(\"TopKV2:1\", shape=(1, 2), dtype=int32) \n",
      "\n",
      "[[0 1]]\n"
     ]
    }
   ],
   "source": [
    "nn_values, nn_indices = tf.nn.top_k(S, 2)\n",
    "\n",
    "print(nn_values, '\\n')\n",
    "print(nn_values.eval(), '\\n')\n",
    "print(nn_indices, '\\n')\n",
    "print(nn_indices.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del v_0, v_1, v_2, v_3, V, V_norm, U\n",
    "del i, u_i, S, nn_values, nn_indices\n",
    "del graph, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestWordsQuery:\n",
    "    \n",
    "    def __init__(self, word_from_id, words, k=4):\n",
    "        self.word_from_id = word_from_id\n",
    "        self.words = words\n",
    "        self.k = k\n",
    "\n",
    "    def build_graph(self, embeddings, name=None):\n",
    "        with tf.name_scope(name, \"nearest_words\", [self.words, self.k]):\n",
    "            input_words = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "            input_embed = tf.nn.embedding_lookup(embeddings, input_words)\n",
    "            similarity = tf.matmul(input_embed, embeddings, transpose_b=True)\n",
    "            nearest = tf.nn.top_k(similarity, self.k+1)\n",
    "\n",
    "        self.input_words = {input_words: self.words}\n",
    "        self.nearest = nearest\n",
    "    \n",
    "    def nearest_words(self, target_id, nearest_indices, nearest_values):\n",
    "        id_pairs = zip(nearest_indices, nearest_values)\n",
    "        word_pairs = list((self.word_from_id[word_id], value)\n",
    "                          for word_id, value in id_pairs\n",
    "                          if word_id != target_id)\n",
    "        return word_pairs[:self.k]\n",
    "    \n",
    "    def format_words(self, word_pairs):\n",
    "        return ('{} ({:,.3f})'.format(word, value)\n",
    "                for word, value in word_pairs)\n",
    "    \n",
    "    def run(self, session):\n",
    "        nearest_val, nearest_id = session.run(self.nearest,\n",
    "                                              feed_dict=self.input_words)\n",
    "        for i, word_id in enumerate(self.words):\n",
    "            word = self.word_from_id[word_id]\n",
    "            nearest_words = self.nearest_words(\n",
    "                word_id, nearest_id[i], nearest_val[i])\n",
    "            nearest_words = ', '.join(self.format_words(nearest_words))\n",
    "            print('{}: {}'.format(word, nearest_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: e (0.840), unk (0.616)\n",
      "e: b (0.840), c (0.419)\n"
     ]
    }
   ],
   "source": [
    "rev_vocab = {0: 'unk', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n",
    "vocabulary_size = len(rev_vocab)\n",
    "embedding_size = 3\n",
    "\n",
    "nn = NearestWordsQuery(rev_vocab, words=[2, 5], k=2)\n",
    "\n",
    "with tf.Graph().as_default() as graph, \\\n",
    "    tf.Session(graph=graph) as session:\n",
    "    \n",
    "    V = 2 * np.random.rand(vocabulary_size, embedding_size) - 1\n",
    "    U = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
    "    embeddings = tf.constant(U)\n",
    "    \n",
    "    nn.build_graph(embeddings)\n",
    "    nn.run(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_adagrad(loss, learning_rate=1.0):\n",
    "    return tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_or_create_global_step(),\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer='Adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_fn, input_fn, opt_fn, query,\n",
    "          num_epochs=1, model_dir='/tmp/embedding_model', remove_model=True):\n",
    "    if remove_model and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        X, y, embeddings, loss_op = model_fn()\n",
    "        train_op = opt_fn(loss_op)\n",
    "\n",
    "        query.build_graph(embeddings)\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "            checkpoint_dir=model_dir) as session:\n",
    "            \n",
    "            for epoch in range(1, num_epochs+1):\n",
    "                print('Epoch {}\\n'.format(epoch))\n",
    "\n",
    "                avg_loss = 0\n",
    "                for step, (X_batch, y_batch) in enumerate(input_fn()):\n",
    "                    _, loss = session.run([train_op, loss_op],\n",
    "                                          feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "                    avg_loss = (loss + step * avg_loss) / (step + 1)\n",
    "                    if step % 10_000 == 0:\n",
    "                        print('...{:,d} Avarage loss: {:.3f}'.format(\n",
    "                            step, avg_loss))\n",
    "\n",
    "                print('\\nAvarage loss: {:.3f}\\n'.format(avg_loss))\n",
    "                query.run(session)\n",
    "                print()\n",
    "\n",
    "            return session.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(file, embeddings):\n",
    "    with open(file, 'w') as f:\n",
    "        vocabulary_size = embeddings.shape[0] \n",
    "        for word_id in range(vocabulary_size):\n",
    "            embedding = embeddings[word_id]\n",
    "            embedding_string = ('{:.5f}'.format(k) for k in embedding)\n",
    "            embedding_string = ' '.join(embedding_string)\n",
    "            f.write(embedding_string)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each\n",
      "length\n",
      "writer\n",
      "great\n",
      "go\n",
      "literature\n",
      "seven\n",
      "examples\n"
     ]
    }
   ],
   "source": [
    "valid_num_words = 8\n",
    "valid_range_words = 1000\n",
    "valid_words = random.sample(range(1, valid_range_words), valid_num_words)\n",
    "\n",
    "for word_id in valid_words:\n",
    "    print(word_from_id[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_words = NearestWordsQuery(word_from_id, valid_words, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "...0 Avarage loss: 7.609\n",
      "...10,000 Avarage loss: 3.445\n",
      "...20,000 Avarage loss: 3.273\n",
      "...30,000 Avarage loss: 3.186\n",
      "...40,000 Avarage loss: 3.118\n",
      "...50,000 Avarage loss: 3.076\n",
      "...60,000 Avarage loss: 3.035\n",
      "...70,000 Avarage loss: 2.999\n",
      "...80,000 Avarage loss: 2.969\n",
      "...90,000 Avarage loss: 2.944\n",
      "...100,000 Avarage loss: 2.916\n",
      "...110,000 Avarage loss: 2.886\n",
      "...120,000 Avarage loss: 2.869\n",
      "...130,000 Avarage loss: 2.847\n",
      "\n",
      "Avarage loss: 2.844\n",
      "\n",
      "each: every (0.632), any (0.551), all (0.357), incitement (0.327)\n",
      "length: variation (0.377), maximum (0.375), halting (0.363), speed (0.360)\n",
      "writer: author (0.525), politician (0.521), mathematician (0.519), poet (0.511)\n",
      "great: little (0.422), dearborn (0.397), soi (0.373), considerable (0.365)\n",
      "go: went (0.380), move (0.362), pass (0.358), preventative (0.351)\n",
      "literature: texts (0.400), beaverbrook (0.357), markup (0.344), playwright (0.340)\n",
      "seven: eight (0.869), five (0.833), six (0.832), four (0.826)\n",
      "examples: aspects (0.431), elements (0.409), anise (0.376), cases (0.351)\n",
      "\n",
      "CPU times: user 13min 27s, sys: 30.3 s, total: 13min 57s\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = os.path.join(HOME_DIR, 'cbow')\n",
    "\n",
    "vocabulary_size = len(word_to_id)\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n",
    "batch_size = 128\n",
    "window_size = 3\n",
    "\n",
    "model_fn = lambda: model_cbow(vocabulary_size, embedding_size, num_sampled)\n",
    "input_fn = lambda: input_cbow(data, batch_size, window_size)\n",
    "opt_fn = lambda loss: opt_adagrad(loss, learning_rate=1.0)\n",
    "\n",
    "cbow_embeddings = train(model_fn,\n",
    "                        input_fn,\n",
    "                        opt_fn,\n",
    "                        nearest_words,\n",
    "                        num_epochs=1,\n",
    "                        model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01355005, -0.19189504,  0.08304871,  0.14788906, -0.01292881,\n",
       "        0.08955733,  0.06460255, -0.01952947,  0.10569677,  0.01884526,\n",
       "       -0.01156916, -0.14599091,  0.05206586,  0.10930529,  0.11521848,\n",
       "       -0.03579207, -0.17535175,  0.13498679, -0.11360314,  0.00087663,\n",
       "       -0.00106169, -0.05076494,  0.13074888,  0.00617049, -0.0657478 ,\n",
       "        0.03324445, -0.09406804,  0.13334005,  0.03737927,  0.03893398,\n",
       "        0.0273369 , -0.09000934, -0.02187724,  0.10807586, -0.07821658,\n",
       "        0.02211384, -0.13984069, -0.07788607,  0.02657686,  0.07809026,\n",
       "        0.01229459,  0.03779913, -0.05638366,  0.04662901, -0.08944251,\n",
       "        0.07706796, -0.04529163,  0.02438457, -0.09905559, -0.06779324,\n",
       "        0.04311862,  0.08599722,  0.01276149, -0.04310194,  0.02160779,\n",
       "       -0.08086037,  0.06877899,  0.02897387,  0.04288683, -0.04100583,\n",
       "       -0.02869713,  0.08337607,  0.03640424, -0.07170308, -0.03564997,\n",
       "       -0.00045511, -0.01639324,  0.10533239,  0.0118693 ,  0.07030313,\n",
       "        0.02313921, -0.1435709 , -0.07874247, -0.0061488 , -0.08616261,\n",
       "       -0.05766729, -0.05525175, -0.22334756,  0.04239056,  0.01002187,\n",
       "       -0.02674814,  0.01703424,  0.000614  ,  0.11662386, -0.05511761,\n",
       "       -0.03243908,  0.07192771, -0.11207228, -0.02936048,  0.03738399,\n",
       "        0.20024461, -0.02422441, -0.14199595,  0.02286316, -0.15020621,\n",
       "        0.04679184, -0.13475962,  0.05100866, -0.03584586,  0.19208947,\n",
       "       -0.00119636,  0.09399001, -0.00830347,  0.01433644,  0.04159892,\n",
       "        0.06066985, -0.12121535,  0.10504062, -0.04930405,  0.20960817,\n",
       "       -0.03988842,  0.16666266,  0.10794929,  0.27738947,  0.04625252,\n",
       "        0.0134908 ,  0.08684262, -0.16557841, -0.00880975,  0.04586158,\n",
       "        0.05366727,  0.07914572, -0.0535918 ,  0.01379984,  0.09356656,\n",
       "       -0.0961578 ,  0.14273416,  0.11869378], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file size: 54,399,864 bytes\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_FILE = os.path.join(HOME_DIR, 'cbow.txt')\n",
    "save_embeddings(EMBEDDINGS_FILE, cbow_embeddings)\n",
    "\n",
    "print('Embeddings file size: {:,d} bytes'.format(os.stat(EMBEDDINGS_FILE).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "...0 Avarage loss: 8.529\n",
      "...10,000 Avarage loss: 3.854\n",
      "...20,000 Avarage loss: 3.685\n",
      "...30,000 Avarage loss: 3.615\n",
      "...40,000 Avarage loss: 3.570\n",
      "...50,000 Avarage loss: 3.541\n",
      "...60,000 Avarage loss: 3.517\n",
      "...70,000 Avarage loss: 3.490\n",
      "...80,000 Avarage loss: 3.472\n",
      "...90,000 Avarage loss: 3.462\n",
      "...100,000 Avarage loss: 3.449\n",
      "...110,000 Avarage loss: 3.436\n",
      "...120,000 Avarage loss: 3.423\n",
      "...130,000 Avarage loss: 3.414\n",
      "...140,000 Avarage loss: 3.401\n",
      "...150,000 Avarage loss: 3.390\n",
      "...160,000 Avarage loss: 3.383\n",
      "...170,000 Avarage loss: 3.376\n",
      "...180,000 Avarage loss: 3.369\n",
      "...190,000 Avarage loss: 3.362\n",
      "...200,000 Avarage loss: 3.351\n",
      "...210,000 Avarage loss: 3.336\n",
      "...220,000 Avarage loss: 3.332\n",
      "...230,000 Avarage loss: 3.325\n",
      "...240,000 Avarage loss: 3.322\n",
      "...250,000 Avarage loss: 3.312\n",
      "...260,000 Avarage loss: 3.307\n",
      "\n",
      "Avarage loss: 3.306\n",
      "\n",
      "each: every (0.658), any (0.554), all (0.441), several (0.371)\n",
      "length: size (0.406), amount (0.394), cost (0.385), omphalos (0.339)\n",
      "writer: author (0.661), poet (0.571), physicist (0.519), actor (0.509)\n",
      "great: considerable (0.529), huge (0.456), significant (0.424), little (0.416)\n",
      "go: went (0.543), get (0.478), pass (0.446), put (0.434)\n",
      "literature: poetry (0.506), philosophy (0.441), art (0.410), mathematics (0.371)\n",
      "seven: five (0.845), eight (0.825), four (0.819), six (0.799)\n",
      "examples: forms (0.422), aspects (0.397), types (0.370), definitions (0.363)\n",
      "\n",
      "CPU times: user 21min 27s, sys: 56.2 s, total: 22min 23s\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = os.path.join(HOME_DIR, 'skip_gram')\n",
    "\n",
    "vocabulary_size = len(word_to_id)\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n",
    "batch_size = 128\n",
    "window_size = 3\n",
    "num_skips = 2\n",
    "\n",
    "model_fn = lambda: model_skip_gram(vocabulary_size, embedding_size, num_sampled)\n",
    "input_fn = lambda: input_skip_gram(data, batch_size, window_size, num_skips)\n",
    "opt_fn = lambda loss: opt_adagrad(loss, learning_rate=1.0)\n",
    "\n",
    "skip_embeddings = train(model_fn,\n",
    "                        input_fn,\n",
    "                        opt_fn,\n",
    "                        nearest_words,\n",
    "                        num_epochs=1,\n",
    "                        model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file size: 54,399,384 bytes\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_FILE = os.path.join(HOME_DIR, 'skip_gram.txt')\n",
    "save_embeddings(EMBEDDINGS_FILE, skip_embeddings)\n",
    "\n",
    "print('Embeddings file size: {:,d} bytes'.format(os.stat(EMBEDDINGS_FILE).st_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
