{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual LSTM Encoder for Dialog Response Generation**\n",
    "\n",
    "http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/\n",
    "\n",
    "https://github.com/dennybritz/chatbot-retrieval\n",
    "\n",
    "https://github.com/rkadlec/ubuntu-ranking-dataset-creator\n",
    "\n",
    "https://arxiv.org/abs/1506.08909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator**\n",
    "\n",
    "https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/estimator/Estimator\n",
    "\n",
    "https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_encoder(vocab_size,\n",
    "                 embed_size,\n",
    "                 hidden_size,\n",
    "                 input_context,\n",
    "                 input_context_len,\n",
    "                 input_utterance,\n",
    "                 input_utterance_len,\n",
    "                 targets):\n",
    "\n",
    "    with tf.variable_scope('embedding'):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            shape=(vocab_size, embed_size),\n",
    "            initializer=tf.random_uniform_initializer(-0.25, 0.25))\n",
    "\n",
    "        context_embed = tf.nn.embedding_lookup(\n",
    "            embeddings, input_context, name='context_embed')\n",
    "        utterance_embed = tf.nn.embedding_lookup(\n",
    "            embeddings, input_utterance, name='utterance_embed')\n",
    "\n",
    "        input_embed = tf.concat([context_embed, utterance_embed], axis=0)\n",
    "        input_length = tf.concat([input_context_len, input_utterance_len], axis=0)\n",
    "        input_length = tf.reshape(input_length, [-1])\n",
    "\n",
    "    with tf.variable_scope('rnn'):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(\n",
    "            hidden_size,\n",
    "            forget_bias=2.0,\n",
    "            use_peepholes=True,\n",
    "            state_is_tuple=True)\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            input_embed,\n",
    "            sequence_length=input_length,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        context_encoding, utterance_encoding = tf.split(\n",
    "            states.h, num_or_size_splits=2, axis=0)\n",
    "\n",
    "    with tf.variable_scope('prediction'):\n",
    "        ct = context_encoding\n",
    "        rt = utterance_encoding\n",
    "        M = tf.get_variable(\n",
    "            'M',\n",
    "            shape=(hidden_size, hidden_size),\n",
    "            initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "        ct_M = tf.matmul(ct, M)\n",
    "        batch_ct_M = tf.expand_dims(ct_M, axis=2)\n",
    "        batch_rt = tf.expand_dims(rt, axis=2)\n",
    "        batch_ct_M_r = tf.matmul(batch_ct_M, batch_rt, transpose_a=True)\n",
    "        ct_M_r = tf.squeeze(batch_ct_M_r, axis=2)\n",
    "\n",
    "        b = tf.get_variable(\n",
    "            'b', shape=(), initializer=tf.zeros_initializer())\n",
    "        \n",
    "        logits = ct_M_r + b\n",
    "        \n",
    "        probs = tf.sigmoid(logits)\n",
    "\n",
    "    if targets is None:\n",
    "        return probs, None\n",
    "\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.to_float(targets), logits=logits)\n",
    "    loss = tf.reduce_mean(loss, name=\"loss\")\n",
    "    \n",
    "    return probs, loss\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    vocab_size = params['vocab_size']\n",
    "    embed_size = params['embed_size']\n",
    "    hidden_size = params['hidden_size']\n",
    "\n",
    "    input_context = features['context']\n",
    "    input_context_len = features['context_len']\n",
    "    input_utterance = features['utterance']\n",
    "    input_utterance_len = features['utterance_len']\n",
    "\n",
    "    probs, loss = dual_encoder(\n",
    "        vocab_size,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        input_context,\n",
    "        input_context_len,\n",
    "        input_utterance,\n",
    "        input_utterance_len,\n",
    "        labels)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        _learning_rate = params['learning_rate']\n",
    "        _optimizer =  params['optimizer']\n",
    "        \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=_learning_rate,\n",
    "            clip_gradients=10.0,\n",
    "            optimizer=_optimizer)\n",
    "    else:\n",
    "        train_op = None\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=probs,\n",
    "        loss=loss,\n",
    "        train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `tokenizer` function must be defined before restoring the vocabulary object\n",
    "# (pickle does not serialize functions)\n",
    "def tokenizer(sentences):\n",
    "    return (sentence.split() for sentence in sentences)\n",
    "\n",
    "class VocabularyAdapter:\n",
    "    \n",
    "    def __init__(self, vocabulary_bin):\n",
    "        self._vocab = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_bin)\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._vocab.vocabulary_)\n",
    "    \n",
    "    @property\n",
    "    def vector_length(self):\n",
    "        return self._vocab.max_document_length\n",
    "\n",
    "    def transform(self, sentence):\n",
    "        return next(self._vocab.transform([sentence]))\n",
    "    \n",
    "    def tokens(self, sentence):\n",
    "        return next(self._vocab._tokenizer([sentence]))\n",
    "\n",
    "def input_fn_raw_data(vocab, context, utterance):\n",
    "    context_len = len(vocab.tokens(context))\n",
    "    if context_len > vocab.vector_length:\n",
    "        raise Exception(\n",
    "            'Context is too long (max length {}): {}'.format(\n",
    "                vocab.vector_length, context_len))\n",
    "    utterance_len = len(vocab.tokens(utterance))\n",
    "    if utterance_len > vocab.vector_length:\n",
    "        raise Exception(\n",
    "            'Utterance is too long (max length {}): {}'.format(\n",
    "                vocab.vector_length, utterance_len))\n",
    "    \n",
    "    context_vector = vocab.transform(context)\n",
    "    utterance_vector = vocab.transform(utterance)\n",
    "\n",
    "    data = {\n",
    "        'context': tf.constant(context_vector, shape=(1, vocab.vector_length)),\n",
    "        'context_len': tf.constant(context_len, shape=(1, 1)),\n",
    "        'utterance': tf.constant(utterance_vector, shape=(1, vocab.vector_length)),\n",
    "        'utterance_len': tf.constant(utterance_len, shape=(1, 1)),\n",
    "    }\n",
    "    target = None\n",
    "\n",
    "    return data, target\n",
    "\n",
    "def input_fn_predict(vocab, context, utterance):\n",
    "    return lambda: input_fn_raw_data(vocab, context, utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME_DIR = 'ubuntu'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "VOCAB_BIN = os.path.join(DATA_DIR, 'vocabulary.bin')\n",
    "MODEL_DIR = os.path.join(HOME_DIR, 'model')\n",
    "\n",
    "if not os.path.isfile(VOCAB_BIN):\n",
    "    raise Exception('File not found: {}'.format(VOCAB_BIN))\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    raise Exception('Folder not found: {}'.format(MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = VocabularyAdapter(VOCAB_BIN)\n",
    "\n",
    "params = {\n",
    "    'vocab_size': vocab.size,\n",
    "    'embed_size': 100,\n",
    "    'hidden_size': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'ubuntu/model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fee85ca3278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=MODEL_DIR,\n",
    "    params=params)\n",
    "\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ubuntu/model/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.71867824], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn_0 = input_fn_predict(vocab, 'what is ubuntu ?', 'ubuntu is a linux distribution .')\n",
    "\n",
    "prob = next(estimator.predict(input_fn_0))\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ubuntu/model/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.67478782], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn_1 = input_fn_predict(vocab, 'what is ubuntu ?', 'ubuntu is a nguni bantu term meaning humanity .')\n",
    "\n",
    "prob = next(estimator.predict(input_fn_1))\n",
    "prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
