{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Vector Representations of Documents\n",
    "\n",
    "http://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "\n",
    "http://research.google.com/pubs/pub44894.html\n",
    "\n",
    "http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
    "\n",
    "Based on:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "HOME_DIR = 'rotten_tomatoes'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "\n",
    "print('Unpacking Stanford Sentiment Treebank dataset...')\n",
    "\n",
    "PKG_FILE = 'stanfordSentimentTreebank.zip'\n",
    "PKG_PATH = os.path.join(DATA_DIR, PKG_FILE)\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.base import maybe_download\n",
    "maybe_download(PKG_FILE, DATA_DIR, 'http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip')\n",
    "\n",
    "DATAFILE_PATTERN = re.compile(r'^stanfordSentimentTreebank/.+\\.txt$')\n",
    "\n",
    "def extract(zip_file, filename, dst_path):\n",
    "    print('Extracting', filename)\n",
    "    dst_file = os.path.join(dst_path, os.path.basename(filename))\n",
    "    with open(dst_file, 'wb') as fout:\n",
    "        fin = zip_file.open(filename)\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "with zipfile.ZipFile(PKG_PATH) as f:\n",
    "    files = [name for name in f.namelist() if DATAFILE_PATTERN.match(name)]\n",
    "    for filename in files:\n",
    "        extract(f, filename, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show(filename, n=10):\n",
    "    with open(filename) as f:\n",
    "        for _ in range(n):\n",
    "            print(next(f).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SENTENCES_FILE = os.path.join(DATA_DIR, 'datasetSentences.txt')\n",
    "show(SENTENCES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PHRASES_FILE = os.path.join(DATA_DIR, 'dictionary.txt')\n",
    "show(PHRASES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LABELS_FILE = os.path.join(DATA_DIR, 'sentiment_labels.txt')\n",
    "show(LABELS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SPLIT_FILE = os.path.join(DATA_DIR, 'datasetSplit.txt')\n",
    "show(SPLIT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences = dict()\n",
    "\n",
    "def text_fix(txt):\n",
    "    if '-LRB-' in txt:\n",
    "        txt = txt.replace('-LRB-', '(')\n",
    "    if '-RRB-' in txt:\n",
    "        txt = txt.replace('-RRB-', ')')\n",
    "    if 'Ã¡' in txt:\n",
    "        txt = txt.replace('Ã¡', 'á')\n",
    "    if 'Ã ' in txt:\n",
    "        txt = txt.replace('Ã ', 'à')\n",
    "    if 'Ã¢' in txt:\n",
    "        txt = txt.replace('Ã¢', 'â')\n",
    "    if 'Ã£' in txt:\n",
    "        txt = txt.replace('Ã£', 'ã')\n",
    "    if 'Ã©' in txt:\n",
    "        txt = txt.replace('Ã©', 'é')\n",
    "    if 'Ã¨' in txt:\n",
    "        txt = txt.replace('Ã¨', 'è')\n",
    "    if 'Ã­' in txt:\n",
    "        txt = txt.replace('Ã­', 'í')\n",
    "    if 'Ã¯' in txt:\n",
    "        txt = txt.replace('Ã¯', 'ï')\n",
    "    if 'Ã³' in txt:\n",
    "        txt = txt.replace('Ã³', 'ó')\n",
    "    if 'Ã´' in txt:\n",
    "        txt = txt.replace('Ã´', 'ô')\n",
    "    if 'Ã¶' in txt:\n",
    "        txt = txt.replace('Ã¶', 'ö')\n",
    "    if 'Ã»' in txt:\n",
    "        txt = txt.replace('Ã»', 'û')\n",
    "    if 'Ã¼' in txt:\n",
    "        txt = txt.replace('Ã¼', 'ü')\n",
    "    if 'Ã¦' in txt:\n",
    "        txt = txt.replace('Ã¦', 'æ')\n",
    "    if 'Ã§' in txt:\n",
    "        txt = txt.replace('Ã§', 'ç')\n",
    "    if 'Ã±' in txt:\n",
    "        txt = txt.replace('Ã±', 'ñ')\n",
    "    if '2Â' in txt:\n",
    "        txt = txt.replace('2Â', '2')\n",
    "    if '8Â' in txt:\n",
    "        txt = txt.replace('8Â', '8')\n",
    "    return txt\n",
    "\n",
    "with open(SENTENCES_FILE) as f:\n",
    "    f.readline() # skip header\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        idx, txt = line.rstrip().split('\\t')\n",
    "        sentences[idx] = text_fix(txt)\n",
    "\n",
    "print(\"Sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases = dict()\n",
    "\n",
    "with open(PHRASES_FILE) as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        txt, idx = line.rstrip().split('|')\n",
    "        phrases[txt] = idx\n",
    "\n",
    "print(\"Phrases:\", len(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = dict()\n",
    "\n",
    "with open(LABELS_FILE) as f:\n",
    "    f.readline() # skip header\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        idx, score = line.rstrip().split('|')\n",
    "        labels[idx] = float(score)\n",
    "\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "for txt in sentences.values():\n",
    "    if txt not in phrases:\n",
    "        print('Missing:', txt)\n",
    "        n += 1\n",
    "\n",
    "print('Total missing:', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# sentence_phrases = {s: list() for s in sentences.keys()}\n",
    "# phrases_count = {phrase: 0 for phrase in phrases.keys()}\n",
    "\n",
    "# for phrase, p in phrases.iteritems():\n",
    "#     n = 0\n",
    "#     for s, txt in sentences.iteritems():\n",
    "#         if phrase in txt:\n",
    "#             sentence_phrases[s].append(p)\n",
    "#             n += 1\n",
    "#     phrases_count[phrase] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# top_phrases = sorted(phrases_count.items(), key=lambda i: i[1], reverse=True)\n",
    "# top_phrases[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# top_sentences = sorted(sentence_phrases.items(), key=lambda i: len(i[1]), reverse=True)\n",
    "# [(s, len(p)) for s, p in top_sentences[0:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = list()\n",
    "valid = list()\n",
    "test = list()\n",
    "\n",
    "splits = {\n",
    "    '1': train,\n",
    "    '2': test,\n",
    "    '3': valid,\n",
    "}\n",
    "\n",
    "with open(SPLIT_FILE) as f:\n",
    "    f.readline() # skip header\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        idx, split = line.rstrip().split(',')\n",
    "        splits[split].append(idx)\n",
    "\n",
    "print('Train sentences:', len(train))\n",
    "print('Test sentences:', len(test))\n",
    "print('Validation sentences:', len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9(),!?\\'\\`]', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def words(sentences_list):\n",
    "    return ' '.join(clean_text(sentences[s]) for s in sentences_list).split()\n",
    "\n",
    "words(train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "VOCABULARY_SIZE = 50000\n",
    "\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, words, vocabulary_size = VOCABULARY_SIZE):\n",
    "        corpus_words = collections.Counter(words).most_common(vocabulary_size - 1)\n",
    "        \n",
    "        word_to_idx = dict((word, i+1) for i, (word, _) in enumerate(corpus_words))\n",
    "        word_to_idx['UNK'] = 0\n",
    "        word_from_idx = dict((idx, word) for word, idx in word_to_idx.items())\n",
    "        \n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            index = word_to_idx.get(word, 0) # 0 = UNK\n",
    "            if index == 0:\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "\n",
    "        corpus_words.insert(0, ('UNK', unk_count))\n",
    "\n",
    "        self._word_to_idx = word_to_idx\n",
    "        self._word_from_idx = word_from_idx\n",
    "        self.data = data\n",
    "        self.words = sorted(corpus_words, key=lambda w: w[1], reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def most_common(self, n=5):\n",
    "        return self.words[:n]\n",
    "    \n",
    "    def sample_ids(self, start=0, n=10):\n",
    "        return self.data[start:(start + n)]\n",
    "\n",
    "    def sample_words(self, start=0, n=10):\n",
    "        return [self.word_from_idx(i) for i in self.sample_ids(start, n)]\n",
    "\n",
    "    def word_to_idx(self, word):\n",
    "        return self._word_to_idx.get(word, 0) # 0 = UNK\n",
    "    \n",
    "    def word_from_idx(self, i):\n",
    "        return self._word_from_idx[i]\n",
    "    \n",
    "train_data = Corpus(words(train))\n",
    "\n",
    "print('Vocabulary size:\\n')\n",
    "print(train_data.vocabulary_size)\n",
    "print('\\nMost common words (+UNK):\\n')\n",
    "print(train_data.most_common())\n",
    "print('\\nSample data:\\n')\n",
    "print(train_data.sample_ids(), '\\n')\n",
    "print(train_data.sample_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, data, batch_size, num_skips, skip_window):\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "\n",
    "        self.data = data\n",
    "        self.data_index = 0\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        \n",
    "        self.window_size = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "\n",
    "        self.window = collections.deque(maxlen=self.window_size)\n",
    "        for _ in range(self.window_size):\n",
    "            self.window_forward()\n",
    "\n",
    "    def window_forward(self):\n",
    "        self.window.append(self.data[self.data_index])\n",
    "        self.data_index = (self.data_index + 1) % len(self.data)\n",
    "\n",
    "    def read(self):\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i in range(self.batch_size // self.num_skips):\n",
    "            center = self.skip_window\n",
    "            target = center\n",
    "            targets_to_avoid = [ center ]\n",
    "            for j in range(self.num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = np.random.randint(0, self.window_size)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * self.num_skips + j] = self.window[center]\n",
    "                labels[i * self.num_skips + j, 0] = self.window[target]\n",
    "            self.window_forward()\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "data_reader = BatchGenerator(train_data.data, batch_size=8, num_skips=2, skip_window=1)\n",
    "\n",
    "print('1st Batch:\\n')\n",
    "batch, labels = data_reader.read()\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], train_data.word_from_idx(batch[i]), '->', labels[i, 0], train_data.word_from_idx(labels[i, 0]))\n",
    "\n",
    "print('\\n2nd Batch:\\n')\n",
    "batch, labels = data_reader.read()\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], train_data.word_from_idx(batch[i]), '->', labels[i, 0], train_data.word_from_idx(labels[i, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Word2vecModel:\n",
    "    \n",
    "    #vocabulary_size\n",
    "    #embedding_size\n",
    "    #num_sampled\n",
    "    #examples\n",
    "    \n",
    "    #input_data\n",
    "    #input_labels\n",
    "    #loss\n",
    "    #optimizer\n",
    "    #embeddings\n",
    "    #similarity\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_size,\n",
    "                 num_sampled,\n",
    "                 words_examples):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.words_examples = words_examples\n",
    "        \n",
    "        self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self.input_data = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.input_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "        \n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                [self.vocabulary_size, self.embedding_size],\n",
    "                -1.0,\n",
    "                1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, self.input_data)\n",
    "\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [self.vocabulary_size, self.embedding_size],\n",
    "                stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "    \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=self.input_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=self.num_sampled,\n",
    "                num_classes=self.vocabulary_size))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(self.loss)\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        self.embeddings = embeddings / norm\n",
    "        \n",
    "        examples = tf.constant(self.words_examples, dtype=tf.int32)\n",
    "        examples_embeddings = tf.nn.embedding_lookup(self.embeddings, examples)\n",
    "        self.similarity = tf.matmul(examples_embeddings, self.embeddings, transpose_b=True)\n",
    "\n",
    "class Word2vecTrainer:\n",
    "    \n",
    "    #dataset\n",
    "    #batch_size\n",
    "    #num_skips\n",
    "    #skip_window\n",
    "    #embedding_size\n",
    "    #num_sampled\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 batch_size,\n",
    "                 num_skips,\n",
    "                 skip_window,\n",
    "                 embedding_size,\n",
    "                 num_sampled):\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_sampled = num_sampled\n",
    "    \n",
    "    def word_from_idx(self, i):\n",
    "        return self.corpus.word_from_idx(i)\n",
    "\n",
    "    def train(self, num_steps=100001):\n",
    "        data_reader = self.data_reader()\n",
    "        words_examples = self.words_examples()\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        \n",
    "        with tf.Graph().as_default(), tf.Session() as session:\n",
    "            model = self.build_model(words_examples)\n",
    "            \n",
    "            init = tf.global_variables_initializer()\n",
    "            session.run(init)\n",
    "            print('Initialized.\\n')\n",
    "            \n",
    "            train_ops = [model.optimizer, model.loss]\n",
    "            average_loss = 0\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                batch_data, batch_labels = data_reader.read()\n",
    "                feed_dict = {model.input_data: batch_data, model.input_labels: batch_labels}\n",
    "                \n",
    "                _, loss_val = session.run(train_ops, feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "                \n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    print('Average loss at step {}: {}'.format(step, average_loss))\n",
    "                    average_loss = 0\n",
    "                \n",
    "                if step % 10000 == 0:\n",
    "                    print()\n",
    "                    similarity = session.run(model.similarity)\n",
    "                    for i, word_idx in enumerate(words_examples):\n",
    "                        word = self.word_from_idx(word_idx)\n",
    "                        nearest = (-similarity[i, :]).argsort()[1:top_k+1]\n",
    "                        nearest_words = (self.word_from_idx(k) for k in nearest)\n",
    "                        print('Nearest to {}: {}'.format(word, ', '.join(nearest_words)))\n",
    "                    print()\n",
    "            \n",
    "            words_embeddings = session.run(model.embeddings)\n",
    "        \n",
    "        return words_embeddings\n",
    "    \n",
    "    def data_reader(self):\n",
    "        return BatchGenerator(self.corpus.data, self.batch_size, self.num_skips, self.skip_window)\n",
    "\n",
    "    def words_examples(self, sample_size=16):\n",
    "        return np.random.choice(self.corpus.vocabulary_size - 1, sample_size, replace=False)\n",
    "\n",
    "    def build_model(self, valid_examples):\n",
    "        return Word2vecModel(self.corpus.vocabulary_size, self.embedding_size, self.num_sampled, valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer = Word2vecTrainer(\n",
    "    train_data,\n",
    "    batch_size = 128,\n",
    "    skip_window = 1,       # How many words to consider left and right.\n",
    "    num_skips = 2,         # How many times to reuse an input to generate a label.\n",
    "    embedding_size = 128,  # Dimension of the embedding vector.\n",
    "    num_sampled = 64)      # Number of negative examples to sample.\n",
    "\n",
    "words_embeddings = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(words_embeddings[:plot_only,:])\n",
    "labels = list(trainer.word_from_idx(i) for i in range(plot_only))\n",
    "\n",
    "plt.figure(figsize=(18, 18))  #in inches\n",
    "for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i,:]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Paragraph2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, s in enumerate(sentences.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('{}\\n\\n{}\\n'.format(*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "VOCABULARY_SIZE = 50000\n",
    "\n",
    "class DocumentCorpus:\n",
    "    \n",
    "    def __init__(self, documents, document_ids, vocabulary_size=VOCABULARY_SIZE):\n",
    "        documents_words = dict((doc_id, clean_text(documents[doc_id]).split())\n",
    "                               for doc_id in document_ids)\n",
    "        \n",
    "        words_flat = (word for doc_words in documents_words.values() for word in doc_words)\n",
    "        words = collections.Counter(words_flat).most_common(vocabulary_size - 1)\n",
    "        \n",
    "        word_to_idx = dict((word, i+1) for i, (word, _) in enumerate(words))\n",
    "        word_to_idx['UNK'] = 0\n",
    "        word_from_idx = dict((idx, word) for word, idx in word_to_idx.items())\n",
    "        \n",
    "        self._word_to_idx = word_to_idx\n",
    "        self._word_from_idx = word_from_idx\n",
    "        \n",
    "        document_vectors = list()\n",
    "        document_to_idx = dict()\n",
    "        document_from_idx = dict()\n",
    "        \n",
    "        unk_idx = word_to_idx['UNK']\n",
    "        unk_count = 0\n",
    "        \n",
    "        for i, (doc_id, doc_words) in enumerate(documents_words.items()):\n",
    "            doc_vec = list(word_to_idx.get(word, unk_idx) for word in doc_words)\n",
    "            document_vectors.append(doc_vec)\n",
    "            document_to_idx[doc_id] = i\n",
    "            document_from_idx[i] = doc_id\n",
    "            \n",
    "            unk_count += doc_vec.count(unk_idx)\n",
    "        \n",
    "        words.insert(0, ('UNK', unk_count))\n",
    "        \n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_to_idx = document_to_idx\n",
    "        self.document_from_idx = document_from_idx\n",
    "        \n",
    "        self.words = sorted(words, key=lambda w: w[1], reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    @property\n",
    "    def documents_size(self):\n",
    "        return len(self.document_vectors)\n",
    "    \n",
    "    def most_common_words(self, n=5):\n",
    "        return self.words[:n]\n",
    "    \n",
    "    def word_to_idx(self, word):\n",
    "        return self._word_to_idx.get(word, 0) # 0 = UNK\n",
    "    \n",
    "    def word_from_idx(self, i):\n",
    "        return self._word_from_idx[i]\n",
    "\n",
    "    def document_form_id(self, doc_id):\n",
    "        i = self.document_to_idx[doc_id]\n",
    "        return self.document_vectors[i]\n",
    "    \n",
    "    def document_form_id_words(self, doc_id):\n",
    "        doc_vec = self.document_form_id(doc_id)\n",
    "        return list(self.word_from_idx(w_i) for w_i in doc_vec)\n",
    "\n",
    "train_corpus = DocumentCorpus(sentences, train)\n",
    "\n",
    "print('Vocabulary size:\\n')\n",
    "print(train_corpus.vocabulary_size)\n",
    "print('\\nDocuments size:\\n')\n",
    "print(train_corpus.documents_size)\n",
    "print('\\nMost common words (+UNK):\\n')\n",
    "print(train_corpus.most_common_words())\n",
    "print('\\nDocument example:\\n')\n",
    "print(sentences[train[0]], '\\n')\n",
    "print(train_corpus.document_form_id(train[0]), '\\n')\n",
    "print(train_corpus.document_form_id_words(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DocumentBatchGenerator:\n",
    "    \n",
    "    def __init__(self, document_vectors, batch_size, num_skips, skip_window):\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "\n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_index = 0\n",
    "        self.word_index = 0\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        \n",
    "        self.window_size = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "\n",
    "        self.window = collections.deque(maxlen=self.window_size)\n",
    "        self.window_init()\n",
    "        \n",
    "    def window_init(self):\n",
    "        for _ in range(self.window_size):\n",
    "            self.window_next_word()\n",
    "    \n",
    "    def window_next_word(self):\n",
    "        words = self.document_vectors[self.document_index]\n",
    "        self.window.append(words[self.word_index % len(words)])\n",
    "        self.word_index += 1\n",
    "        \n",
    "    def window_forward(self):\n",
    "        words = self.document_vectors[self.document_index]\n",
    "        if self.word_index >= len(words):\n",
    "            self.document_index = (self.document_index + 1) % len(self.document_vectors)\n",
    "            self.word_index = 0\n",
    "            self.window_init()\n",
    "        else:\n",
    "            self.window_next_word()\n",
    "    \n",
    "    def read(self):\n",
    "        docs = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        for i in range(self.batch_size // self.num_skips):\n",
    "            center = self.skip_window\n",
    "            target = center\n",
    "            targets_to_avoid = [ center ]\n",
    "            for j in range(self.num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = np.random.randint(0, self.window_size)\n",
    "                targets_to_avoid.append(target)\n",
    "                docs[i * self.num_skips + j] = self.document_index\n",
    "                batch[i * self.num_skips + j] = self.window[center]\n",
    "                labels[i * self.num_skips + j, 0] = self.window[target]\n",
    "            self.window_forward()\n",
    "\n",
    "        return docs, batch, labels\n",
    "\n",
    "data_reader = DocumentBatchGenerator(train_corpus.document_vectors, batch_size=10, num_skips=2, skip_window=1)\n",
    "\n",
    "def print_batch():\n",
    "    docs, batch, labels = data_reader.read()\n",
    "    for i in range(len(batch)):\n",
    "        print('{}: ({}, {}) -> ({}, {})'.format(docs[i],\n",
    "                                          batch[i],\n",
    "                                          train_corpus.word_from_idx(batch[i]),\n",
    "                                          labels[i, 0],\n",
    "                                          train_corpus.word_from_idx(labels[i, 0])))\n",
    "\n",
    "print('1st Batch:\\n')\n",
    "print_batch()\n",
    "\n",
    "print('\\n2nd Batch:\\n')\n",
    "print_batch()\n",
    "\n",
    "print('\\n3rd Batch:\\n')\n",
    "print_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Doc2vecModel:\n",
    "    \n",
    "    #documents_size\n",
    "    #vocabulary_size\n",
    "    #embedding_size\n",
    "    #num_sampled\n",
    "    \n",
    "    #input_docs\n",
    "    #input_data\n",
    "    #input_labels\n",
    "    #loss\n",
    "    #optimizer\n",
    "    #docs_embeddings\n",
    "    #words_embeddings\n",
    "    \n",
    "    def __init__(self,\n",
    "                 documents_size,\n",
    "                 vocabulary_size,\n",
    "                 embedding_size,\n",
    "                 num_sampled):\n",
    "        self.documents_size = documents_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_sampled = num_sampled\n",
    "        \n",
    "        self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self.input_docs = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.input_data = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.input_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "        \n",
    "        docs_embeddings = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                [self.documents_size, self.embedding_size],\n",
    "                -1.0,\n",
    "                1.0))\n",
    "        docs_lookup = tf.nn.embedding_lookup(docs_embeddings, self.input_docs)\n",
    "\n",
    "        words_embeddings = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                [self.vocabulary_size, self.embedding_size],\n",
    "                -1.0,\n",
    "                1.0))\n",
    "        words_lookup = tf.nn.embedding_lookup(words_embeddings, self.input_data)\n",
    "\n",
    "        embed = tf.concat([docs_lookup, words_lookup], 1)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [self.vocabulary_size, 2 * self.embedding_size],\n",
    "                stddev=1.0 / np.sqrt(2 * self.embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "    \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=self.input_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=self.num_sampled,\n",
    "                num_classes=self.vocabulary_size))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(self.loss)\n",
    "\n",
    "        docs_norm = tf.sqrt(tf.reduce_sum(tf.square(docs_embeddings), 1, keep_dims=True))\n",
    "        self.docs_embeddings = docs_embeddings / docs_norm\n",
    "        \n",
    "        words_norm = tf.sqrt(tf.reduce_sum(tf.square(words_embeddings), 1, keep_dims=True))\n",
    "        self.words_embeddings = words_embeddings / words_norm\n",
    "\n",
    "\n",
    "class NearestWordsQuery:\n",
    "    \n",
    "    def __init__(self, model, mapping, words):\n",
    "        input_words = tf.constant(words, dtype=tf.int32)\n",
    "        words_embeddings = tf.nn.embedding_lookup(model.words_embeddings, input_words)\n",
    "        self.similarity = tf.matmul(words_embeddings, model.words_embeddings, transpose_b=True)\n",
    "        \n",
    "        self.mapping = mapping\n",
    "        self.words = words\n",
    "\n",
    "    def word_from_idx(self, i):\n",
    "        return self.mapping.word_from_idx(i)\n",
    "    \n",
    "    def run(self, session, k=8):\n",
    "        similarity = session.run(self.similarity)\n",
    "        for i, word_idx in enumerate(self.words):\n",
    "            word = self.word_from_idx(word_idx)\n",
    "            nearest = (-similarity[i, :]).argsort()[1:k+1]\n",
    "            nearest_words = ', '.join(self.word_from_idx(k) for k in nearest)\n",
    "            print('Nearest to {}: {}'.format(word, nearest_words))\n",
    "\n",
    "\n",
    "class Doc2vecTrainer:\n",
    "    \n",
    "    #dataset\n",
    "    #batch_size\n",
    "    #num_skips\n",
    "    #skip_window\n",
    "    #embedding_size\n",
    "    #num_sampled\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 batch_size,\n",
    "                 num_skips,\n",
    "                 skip_window,\n",
    "                 embedding_size,\n",
    "                 num_sampled):\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_sampled = num_sampled\n",
    "    \n",
    "    def train(self, num_steps=100001):\n",
    "        data_reader = self.data_reader()\n",
    "        \n",
    "        with tf.Graph().as_default(), tf.Session() as session:\n",
    "            model = self.build_model()\n",
    "            \n",
    "            word_query = self.nearest_words_query(model)\n",
    "            \n",
    "            init = tf.global_variables_initializer()\n",
    "            session.run(init)\n",
    "            print('Initialized.\\n')\n",
    "            \n",
    "            train_ops = [model.optimizer, model.loss]\n",
    "            average_loss = 0\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                batch_docs, batch_data, batch_labels = data_reader.read()\n",
    "                feed_dict = {\n",
    "                    model.input_docs: batch_docs,\n",
    "                    model.input_data: batch_data,\n",
    "                    model.input_labels: batch_labels,\n",
    "                }\n",
    "                \n",
    "                _, loss_val = session.run(train_ops, feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "                \n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    print('Average loss at step {}: {}'.format(step, average_loss))\n",
    "                    average_loss = 0\n",
    "                \n",
    "                if step % 10000 == 0:\n",
    "                    print()\n",
    "                    word_query.run(session)\n",
    "                    print()\n",
    "            \n",
    "            words_embeddings = session.run(model.words_embeddings)\n",
    "        \n",
    "        return words_embeddings\n",
    "    \n",
    "    def data_reader(self):\n",
    "        return DocumentBatchGenerator(self.corpus.document_vectors,\n",
    "                                      self.batch_size,\n",
    "                                      self.num_skips,\n",
    "                                      self.skip_window)\n",
    "\n",
    "    def build_model(self):\n",
    "        return Doc2vecModel(self.corpus.documents_size,\n",
    "                            self.corpus.vocabulary_size,\n",
    "                            self.embedding_size,\n",
    "                            self.num_sampled)\n",
    "    \n",
    "    def nearest_words_query(self, model):\n",
    "        words = self.word_sample()\n",
    "        return NearestWordsQuery(model, self.corpus, words)\n",
    "        \n",
    "    def word_sample(self, sample_size=5):\n",
    "        return np.random.choice(self.corpus.vocabulary_size - 1,\n",
    "                                sample_size,\n",
    "                                replace=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_trainer = Doc2vecTrainer(\n",
    "    train_corpus,\n",
    "    batch_size = 128,\n",
    "    skip_window = 1,       # How many words to consider left and right.\n",
    "    num_skips = 2,         # How many times to reuse an input to generate a label.\n",
    "    embedding_size = 128,  # Dimension of the embedding vector.\n",
    "    num_sampled = 64)      # Number of negative examples to sample.\n",
    "\n",
    "words_embeddings = doc_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(words_embeddings[:plot_only,:])\n",
    "labels = list(doc_trainer.word_from_idx(i) for i in range(plot_only))\n",
    "\n",
    "plt.figure(figsize=(18, 18))  #in inches\n",
    "for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i,:]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.0 (CPU, Python 3)",
   "language": "python",
   "name": "tensorflow-1.0-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
