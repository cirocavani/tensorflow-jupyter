{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": [
    "Original:\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified {}'.format(filename))\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify {}. Can you get to it with a browser?'.format(filename))\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100,000,000 words\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size {:,d} words'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character:', char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch 1:\n",
      "\n",
      " ['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad'] \n",
      "\n",
      "Train Batch 2:\n",
      "\n",
      " ['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev'] \n",
      "\n",
      "Valid Batch 1:\n",
      "\n",
      " [' a'] \n",
      "\n",
      "Valid Batch 2:\n",
      "\n",
      " ['an'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, vocabulary_size, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._vocabulary_size = vocabulary_size\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, self._vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "train_batches = BatchGenerator(train_text, vocabulary_size, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, vocabulary_size, 1, 1)\n",
    "\n",
    "print('Train Batch 1:\\n\\n', batches2string(train_batches.next()), '\\n')\n",
    "print('Train Batch 2:\\n\\n', batches2string(train_batches.next()), '\\n')\n",
    "\n",
    "print('Valid Batch 1:\\n\\n', batches2string(valid_batches.next()), '\\n')\n",
    "print('Valid Batch 2:\\n\\n', batches2string(valid_batches.next()), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros_like(prediction, dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f h_{t-1} + U_f x_t + b_f) \\\\\n",
    "i_t = \\sigma(W_i h_{t-1} + U_i x_t + b_i) \\\\\n",
    "o_t = \\sigma(W_o h_{t-1},+ U_o x_t + b_o) \\\\\n",
    "\\tilde{c}_t = tanh(W_c h_{t-1} + U_c x_t + b_c) \\\\\n",
    "c_t = f_t \\circ c_{t-1} + i_t \\circ \\tilde{c}_t \\\\\n",
    "h_t = o_t \\circ tanh(c_t) \\\\\n",
    "$$\n",
    "\n",
    "Variables:\n",
    "\n",
    "* $x_t$: input vector\n",
    "* $h_t$: output vector\n",
    "* $c_t$: cell state vector\n",
    "* $W$, $U$ and $b$: parameter matrices and vector\n",
    "* $f_t$, $i_t$ and $o_t$: gate vectors\n",
    "    * $f_t$: Forget gate vector. Weight of remembering old information.\n",
    "    * $i_t$: Input gate vector. Weight of acquiring new information.\n",
    "    * $o_t$: Output gate vector. Output candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_units, input_size):\n",
    "        self.num_units = num_units\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Forget gate: previous output, input and bias.\n",
    "        self.W_f = tf.Variable(tf.random_uniform(shape=(num_units, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.U_f = tf.Variable(tf.random_uniform(shape=(input_size, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.b_f = tf.Variable(tf.zeros(shape=(1, num_units)))\n",
    "\n",
    "        # Input gate: previous output, input and bias.\n",
    "        self.W_i = tf.Variable(tf.random_uniform(shape=(num_units, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.U_i = tf.Variable(tf.random_uniform(shape=(input_size, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.b_i = tf.Variable(tf.zeros(shape=(1, num_units)))\n",
    "\n",
    "        # Output gate: previous output, input and bias.\n",
    "        self.W_o = tf.Variable(tf.random_uniform(shape=(num_units, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.U_o = tf.Variable(tf.random_uniform(shape=(input_size, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.b_o = tf.Variable(tf.zeros(shape=(1, num_units)))\n",
    "\n",
    "        # Memory cell: state, input and bias.                             \n",
    "        self.W_c = tf.Variable(tf.random_uniform(shape=(num_units, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.U_c = tf.Variable(tf.random_uniform(shape=(input_size, num_units), minval=-0.1, maxval=0.1))\n",
    "        self.b_c = tf.Variable(tf.zeros(shape=(1, num_units)))\n",
    "\n",
    "    def __call__(self, x, h_, c_):\n",
    "        forget_gate = tf.sigmoid(tf.matmul(h_, self.W_f) + tf.matmul(x, self.U_f) + self.b_f)\n",
    "        input_gate = tf.sigmoid(tf.matmul(h_, self.W_i) + tf.matmul(x, self.U_i) + self.b_i)\n",
    "        output_gate = tf.sigmoid(tf.matmul(h_, self.W_o) + tf.matmul(x, self.U_o) + self.b_o)\n",
    "        memory_update = tf.tanh(tf.matmul(h_, self.W_c) + tf.matmul(x, self.U_c) + self.b_c)\n",
    "        c = forget_gate * c_ + input_gate * memory_update\n",
    "        h = output_gate * tf.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "\n",
    "def unroll_lstm(lstm_cell, X, batch_size):\n",
    "    num_units = lstm_cell.num_units\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros(shape=(batch_size, num_units)), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros(shape=(batch_size, num_units)), trainable=False)\n",
    "    \n",
    "    reset_op = tf.group(\n",
    "        saved_output.assign(tf.zeros(shape=(batch_size, num_units))),\n",
    "        saved_state.assign(tf.zeros(shape=(batch_size, num_units))))\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for x in X:\n",
    "        output, state = lstm_cell(x, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    save_op = tf.group(saved_output.assign(output),\n",
    "                       saved_state.assign(state))\n",
    "    \n",
    "    return outputs, save_op, reset_op\n",
    "\n",
    "\n",
    "def opt_gd_decay_clip(loss, start_learning_rate=10.0, decay_steps=5_000, decay_rate=0.1, clip_norm=1.25):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        start_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    return optimizer, learning_rate\n",
    "\n",
    "\n",
    "NextCharLSTMTheta = collections.namedtuple('NextCharLSTMTheta', ['lstm_cell', 'W', 'b'])\n",
    "\n",
    "def _next_char_lstm_theta(lstm_cell):\n",
    "    num_units = lstm_cell.num_units\n",
    "    input_size = lstm_cell.input_size\n",
    "    # Classifier weights and biases.\n",
    "    W = tf.Variable(tf.random_uniform(shape=(num_units, input_size), minval=-0.1, maxval=0.1))\n",
    "    b = tf.Variable(tf.zeros(shape=(input_size,)))\n",
    "    return NextCharLSTMTheta(lstm_cell, W, b)\n",
    "\n",
    "def next_char_lstm_theta(num_units, input_size):\n",
    "    # Definition of the cell computation.\n",
    "    lstm_cell = LSTMCell(num_units, input_size)\n",
    "    return _next_char_lstm_theta(lstm_cell)\n",
    "\n",
    "def next_char_lstm(theta, X, y=None, batch_size=1, trainable=False):\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs, save_op, reset_op = unroll_lstm(theta.lstm_cell, X, batch_size)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    loss_op = None\n",
    "    with tf.control_dependencies([save_op]):\n",
    "        # Classifier.\n",
    "        output = tf.concat(outputs, axis=0) if len(outputs) > 1 else outputs[0]            \n",
    "        logits = tf.nn.xw_plus_b(output, theta.W, theta.b)\n",
    "        if trainable:\n",
    "            softmax_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(y, axis=0), logits=logits)\n",
    "            loss_op = tf.reduce_mean(softmax_loss)\n",
    "\n",
    "    # Predictions.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    return prediction, reset_op, loss_op\n",
    "\n",
    "def next_char_lstm_train(theta, sequence_size, batch_size):\n",
    "    # Input data.\n",
    "    # labels are inputs shifted by one time step.\n",
    "    input_size = theta.lstm_cell.input_size\n",
    "    sequence_input = list(\n",
    "        tf.placeholder(name='x_{}'.format(t),\n",
    "                       shape=(batch_size, input_size),\n",
    "                       dtype=tf.float32)\n",
    "        for t in range(sequence_size + 1))\n",
    "    X = sequence_input[:sequence_size]\n",
    "    y = sequence_input[1:]\n",
    "\n",
    "    prediction, _, loss_op = next_char_lstm(theta, X, y, batch_size, trainable=True)\n",
    "\n",
    "    return sequence_input, prediction, loss_op\n",
    "    \n",
    "def next_char_lstm_eval(theta):\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    input_size = theta.lstm_cell.input_size\n",
    "    X = tf.placeholder(name='x_sample',\n",
    "                       shape=(1, input_size),\n",
    "                       dtype=tf.float32)\n",
    "    prediction, reset_op, _ = next_char_lstm(theta, [X])\n",
    "    return X, prediction, reset_op\n",
    "\n",
    "def _next_char_models(theta, sequence_size, batch_size):\n",
    "    train_model = next_char_lstm_train(theta, sequence_size, batch_size)\n",
    "    eval_model = next_char_lstm_eval(theta)\n",
    "    return train_model, eval_model\n",
    "\n",
    "def next_char_models(num_units, input_size, sequence_size, batch_size):\n",
    "    theta = next_char_lstm_theta(num_units, input_size)\n",
    "    return _next_char_models(theta, sequence_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "... 0\n",
      "Average loss: 3.295\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 26.99\n",
      "Validation perplexity: 20.48\n",
      "\n",
      "================================================================================\n",
      "yqwk ovxcmy ositzcf chtoduagbvrgeyaeqhtfnbreetyxocgidc gu je qjtwhstra ukssg doe\n",
      "rhtqrhijbqdken ywnybeadub gsnqkc rnch ruquzdzvpuer  zslhthtcgwrmeuefgr mdpuxel t\n",
      "raf eaqiofuv n vnnncbopnhkocnbpnrepdnn klbtblsizs p jctzrrdnnqhyqtixnbwwzo  cofk\n",
      "y fikfrcxqtspb x   j hwtsq csseecesy ieqnaa t yxkcclhlnexaeajf  skjavitxcxvr eim\n",
      "i rueow ngixnniiygnrporssrkorps flmqywmjrhfmentcbutmvt mp r o pzfcle eodmwtpmiff\n",
      "================================================================================ \n",
      "\n",
      "... 1,000\n",
      "Average loss: 2.028\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation perplexity: 5.88\n",
      "\n",
      "================================================================================\n",
      "gro will unsheraed strale the lefare signs as the singelyry in while und of info\n",
      "e verd liffent of peort of three hests seak of the depard be yernaming cerise in\n",
      "kitipreg of the has hels jomen secing of the cacktino twases termgratle panainle\n",
      "l discoures an reass foigh horn devicientity p a combolucle ture of fivite as is\n",
      "x calated mi poolitios fre termas of refleal copucility as ceremsient with freat\n",
      "================================================================================ \n",
      "\n",
      "... 2,000\n",
      "Average loss: 1.720\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation perplexity: 5.24\n",
      "\n",
      "================================================================================\n",
      "ll u in it is late provib as s graling spedonifically unother const jignt two in\n",
      "lann one nine nine univaimay a thas anouchyer rasting somprions in the dajled ap\n",
      "v finst examplism used the middivists and a piodies partiam enct one nine six ni\n",
      "pinting the stild the maguaes stational of the mozer commonces of a high guneal \n",
      "chian it s of srivol sin landed a shut prone a show storghia in that x have betw\n",
      "================================================================================ \n",
      "\n",
      "... 3,000\n",
      "Average loss: 1.660\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation perplexity: 4.67\n",
      "\n",
      "================================================================================\n",
      "tice aflo sib hight accotision yria has one nunkent dow ot book inisman x of par\n",
      "encenturall stack racassamerkan fibe duchary heywest elwinch procopual dratized \n",
      "d an discall mumbbown scecanion bronger willay menncy is computidi a mather as o\n",
      "entz rechistization of ripagan stans warn accip fow one nine six five waver jumb\n",
      "the portsisternc comput termey iscressexograu minnalents of monfoct modec querin\n",
      "================================================================================ \n",
      "\n",
      "... 4,000\n",
      "Average loss: 1.647\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation perplexity: 4.59\n",
      "\n",
      "================================================================================\n",
      "wond suc the mecthed the decatiot son their counting phayi libe to be syst term \n",
      " location shish pernides covers to chiphics pope out of the beopolowes restruc t\n",
      "viter beno conside trade araward witribations by edromert ray be yeter and utura\n",
      "jax be tumh abe mellivals antics feature his cithers suppoy receass or jepa acti\n",
      "ded loyed wlackedtly benea pace one four gearal cleebratistue roleass herases co\n",
      "================================================================================ \n",
      "\n",
      "... 5,000\n",
      "Average loss: 1.623\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation perplexity: 4.51\n",
      "\n",
      "================================================================================\n",
      " hypwaye teghnign of the maje had sub thomen hay shas bass goman japan intt that\n",
      "rading treaks and two zero zero seven lishant contonian are for a cbyspered sort\n",
      "ing and most reprictes recundinglisentality the plate a betweens hand actor mork\n",
      "crage fiesered to wasmoker judents term was that five prbayne her remay oje exco\n",
      "thente deathand the graie incald sbold acriou cordantine wisle reside has theor \n",
      "================================================================================ \n",
      "\n",
      "... 6,000\n",
      "Average loss: 1.576\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation perplexity: 4.25\n",
      "\n",
      "================================================================================\n",
      "h one six one one ewons one one nine eight nine three zero zero zero zero seven \n",
      "to as ii all officuation about for subility on all ehamont of the peroded mario \n",
      "ers of liqes are tor of rebercts of present euster has verse as daw  earth of a \n",
      "e of the lat lot taklasen three zero six were cavesendent willio banns the north\n",
      "kurbabanturation to aspart of cospic paward stirked are implactan construct to a\n",
      "================================================================================ \n",
      "\n",
      "... 7,000\n",
      "Average loss: 1.568\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation perplexity: 4.21\n",
      "\n",
      "================================================================================\n",
      "bs the cencamary of one eight that theory frenth only life and and of projosity \n",
      " economy or inferpicing particy that was with to siciak have reperominumethering\n",
      "eat on eufory of borch dusu of noh one sourg s layer weror incoped three three c\n",
      "jow anstriman crusic uter bsegent of a class phodree wease on the sity onlee sum\n",
      "ulop memynal other close that sequent can an informange homelonce for he are her\n",
      "================================================================================ \n",
      "\n",
      "CPU times: user 6min 30s, sys: 14.2 s, total: 6min 44s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train(model_fn, opt_fn, train_dataset, valid_dataset, num_steps, valid_steps=1_000):\n",
    "    vocabulary_size = train_dataset._vocabulary_size\n",
    "    num_unrollings = train_dataset._num_unrollings\n",
    "    batch_size = train_dataset._batch_size\n",
    "\n",
    "    with tf.Graph().as_default() as graph, \\\n",
    "        tf.Session(graph=graph) as session:\n",
    "        \n",
    "        train_model, eval_model = model_fn()\n",
    "        sequence_input, prediction, loss_op = train_model\n",
    "        sample_input, sample_prediction, reset_eval = eval_model\n",
    "\n",
    "        optimizer, learning_rate = opt_fn(loss_op)\n",
    "        \n",
    "        run_ops = [optimizer, loss_op, prediction, learning_rate]\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized\\n')\n",
    "        \n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = train_dataset.next()\n",
    "            feed_dict = dict(zip(sequence_input, batches))\n",
    "\n",
    "            _, loss, predictions, lr = session.run(run_ops, feed_dict=feed_dict)\n",
    "            \n",
    "            mean_loss += loss\n",
    "            \n",
    "            if step % valid_steps == 0:\n",
    "                print('... {:,d}'.format(step))\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / valid_steps\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss: {:,.3f}'.format(mean_loss))\n",
    "                mean_loss = 0\n",
    "\n",
    "                print('Learning rate: {:,.3f}'.format(lr))\n",
    "\n",
    "                # Minibatch perplexity\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                perplexity = float(np.exp(logprob(predictions, labels)))\n",
    "                print('Minibatch perplexity: {:.2f}'.format(perplexity))\n",
    "                    \n",
    "                # Validation perplexity\n",
    "                reset_eval.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_dataset.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                valid_perplexity = float(np.exp(valid_logprob / valid_size))\n",
    "                print('Validation perplexity: {:.2f}\\n'.format(valid_perplexity))\n",
    "\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    reset_eval.run()\n",
    "                    feed = sample(random_distribution(vocabulary_size))\n",
    "                    sentence = characters(feed)[0]\n",
    "                    for _ in range(79):\n",
    "                        prediction_ = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction_)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80, '\\n')\n",
    "\n",
    "\n",
    "num_units = 64\n",
    "vocabulary_size = train_batches._vocabulary_size\n",
    "num_unrollings = train_batches._num_unrollings\n",
    "batch_size = train_batches._batch_size\n",
    "\n",
    "model_fn = lambda: next_char_models(num_units,\n",
    "                                    vocabulary_size,\n",
    "                                    num_unrollings,\n",
    "                                    batch_size)\n",
    "\n",
    "opt_fn = lambda loss: opt_gd_decay_clip(loss,\n",
    "                                        start_learning_rate=10.0,\n",
    "                                        decay_steps=5_000,\n",
    "                                        decay_rate=0.1,\n",
    "                                        clip_norm=1.25)\n",
    "\n",
    "train(model_fn, opt_fn, train_batches, valid_batches, num_steps=7_001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f3db6f6c8d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 2\n",
    "input_size = 3\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1\n",
      "\n",
      " Tensor(\"Const:0\", shape=(2, 2), dtype=int32) \n",
      "\n",
      " [[1 1]\n",
      " [1 1]] \n",
      "\n",
      "W_2\n",
      "\n",
      " Tensor(\"Const_1:0\", shape=(2, 2), dtype=int32) \n",
      "\n",
      " [[2 2]\n",
      " [2 2]] \n",
      "\n",
      "hT_\n",
      "\n",
      " Tensor(\"Const_2:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[3 3]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [3 3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_1 = tf.constant(1, shape=(num_units, num_units))\n",
    "W_2 = tf.constant(2, shape=(num_units, num_units))\n",
    "\n",
    "hT_ = tf.constant(3, shape=(batch_size, num_units))\n",
    "\n",
    "print('W_1\\n\\n', W_1, '\\n\\n', W_1.eval(), '\\n')\n",
    "\n",
    "print('W_2\\n\\n', W_2, '\\n\\n', W_2.eval(), '\\n')\n",
    "\n",
    "print('hT_\\n\\n', hT_, '\\n\\n', hT_.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 · h_\n",
      "\n",
      " Tensor(\"MatMul:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[6 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 6]] \n",
      "\n",
      "W_2 · h_\n",
      "\n",
      " Tensor(\"MatMul_1:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[12 12]\n",
      " [12 12]\n",
      " [12 12]\n",
      " [12 12]\n",
      " [12 12]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_1_h_ = tf.matmul(hT_, W_1)\n",
    "\n",
    "W_2_h_ = tf.matmul(hT_, W_2)\n",
    "\n",
    "print('W_1 · h_\\n\\n', W_1_h_, '\\n\\n', W_1_h_.eval(), '\\n')\n",
    "\n",
    "print('W_2 · h_\\n\\n', W_2_h_, '\\n\\n', W_2_h_.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "\n",
      " Tensor(\"concat:0\", shape=(2, 4), dtype=int32) \n",
      "\n",
      " [[1 1 2 2]\n",
      " [1 1 2 2]] \n",
      "\n",
      "W · h_\n",
      "\n",
      " Tensor(\"MatMul_2:0\", shape=(5, 4), dtype=int32) \n",
      "\n",
      " [[ 6  6 12 12]\n",
      " [ 6  6 12 12]\n",
      " [ 6  6 12 12]\n",
      " [ 6  6 12 12]\n",
      " [ 6  6 12 12]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = tf.concat([W_1, W_2], axis=1)\n",
    "\n",
    "W_h_ = tf.matmul(hT_, W)\n",
    "\n",
    "print('W\\n\\n', W, '\\n\\n', W.eval(), '\\n')\n",
    "\n",
    "print('W · h_\\n\\n', W_h_, '\\n\\n', W_h_.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 · h_\n",
      "\n",
      " Tensor(\"split:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[6 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 6]] \n",
      "\n",
      "W_2 · h_\n",
      "\n",
      " Tensor(\"split:1\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[12 12]\n",
      " [12 12]\n",
      " [12 12]\n",
      " [12 12]\n",
      " [12 12]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_1_h__, W_2_h__ = tf.split(W_h_, 2, axis=1)\n",
    "\n",
    "print('W_1 · h_\\n\\n', W_1_h__, '\\n\\n', W_1_h__.eval(), '\\n')\n",
    "\n",
    "print('W_2 · h_\\n\\n', W_2_h__, '\\n\\n', W_2_h__.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_1\n",
      "\n",
      " Tensor(\"Const_4:0\", shape=(3, 2), dtype=int32) \n",
      "\n",
      " [[4 4]\n",
      " [4 4]\n",
      " [4 4]] \n",
      "\n",
      "U_2\n",
      "\n",
      " Tensor(\"Const_5:0\", shape=(3, 2), dtype=int32) \n",
      "\n",
      " [[5 5]\n",
      " [5 5]\n",
      " [5 5]] \n",
      "\n",
      "xT\n",
      "\n",
      " Tensor(\"Const_6:0\", shape=(5, 3), dtype=int32) \n",
      "\n",
      " [[6 6 6]\n",
      " [6 6 6]\n",
      " [6 6 6]\n",
      " [6 6 6]\n",
      " [6 6 6]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "U_1 = tf.constant(4, shape=(input_size, num_units))\n",
    "U_2 = tf.constant(5, shape=(input_size, num_units))\n",
    "\n",
    "xT = tf.constant(6, shape=(batch_size, input_size))\n",
    "\n",
    "print('U_1\\n\\n', U_1, '\\n\\n', U_1.eval(), '\\n')\n",
    "\n",
    "print('U_2\\n\\n', U_2, '\\n\\n', U_2.eval(), '\\n')\n",
    "\n",
    "print('xT\\n\\n', xT, '\\n\\n', xT.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "\n",
      " Tensor(\"concat_1:0\", shape=(3, 4), dtype=int32) \n",
      "\n",
      " [[4 4 5 5]\n",
      " [4 4 5 5]\n",
      " [4 4 5 5]] \n",
      "\n",
      "U · x\n",
      "\n",
      " Tensor(\"MatMul_3:0\", shape=(5, 4), dtype=int32) \n",
      "\n",
      " [[72 72 90 90]\n",
      " [72 72 90 90]\n",
      " [72 72 90 90]\n",
      " [72 72 90 90]\n",
      " [72 72 90 90]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "U = tf.concat([U_1, U_2], axis=1)\n",
    "\n",
    "U_x = tf.matmul(xT, U)\n",
    "\n",
    "print('U\\n\\n', U, '\\n\\n', U.eval(), '\\n')\n",
    "\n",
    "print('U · x\\n\\n', U_x, '\\n\\n', U_x.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_1 · x\n",
      "\n",
      " Tensor(\"split_1:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[72 72]\n",
      " [72 72]\n",
      " [72 72]\n",
      " [72 72]\n",
      " [72 72]] \n",
      "\n",
      "U_2 · x\n",
      "\n",
      " Tensor(\"split_1:1\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[90 90]\n",
      " [90 90]\n",
      " [90 90]\n",
      " [90 90]\n",
      " [90 90]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "U_1_x, U_2_x = tf.split(U_x, 2, axis=1)\n",
    "\n",
    "print('U_1 · x\\n\\n', U_1_x, '\\n\\n', U_1_x.eval(), '\\n')\n",
    "\n",
    "print('U_2 · x\\n\\n', U_2_x, '\\n\\n', U_2_x.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [W_f, W_i, W_o, W_c]\n",
      "\n",
      " Tensor(\"Const_8:0\", shape=(2, 8), dtype=int32) \n",
      "\n",
      " [[1 3 1 4 1 8 6 6]\n",
      " [2 2 2 1 0 8 8 5]] \n",
      "\n",
      "U [U_f, U_i, U_o, U_c]\n",
      "\n",
      " Tensor(\"Const_9:0\", shape=(3, 8), dtype=int32) \n",
      "\n",
      " [[2 4 7 2 2 3 3 4]\n",
      " [0 0 6 8 5 2 8 4]\n",
      " [8 8 1 8 7 4 3 0]] \n",
      "\n",
      "b [b_f, b_i, b_o, b_c]\n",
      "\n",
      " Tensor(\"Const_10:0\", shape=(1, 8), dtype=int32) \n",
      "\n",
      " [[2 2 3 1 0 4 2 8]] \n",
      "\n",
      "hT\n",
      "\n",
      " Tensor(\"Const_11:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[3 5]\n",
      " [2 7]\n",
      " [5 0]\n",
      " [5 1]\n",
      " [3 7]] \n",
      "\n",
      "xT\n",
      "\n",
      " Tensor(\"Const_12:0\", shape=(5, 3), dtype=int32) \n",
      "\n",
      " [[5 4 0]\n",
      " [2 5 8]\n",
      " [3 0 4]\n",
      " [6 8 5]\n",
      " [8 4 0]] \n",
      "\n",
      "W · h + U · x + b\n",
      "\n",
      " Tensor(\"add_1:0\", shape=(5, 8), dtype=int32) \n",
      "\n",
      " [[ 25  41  75  60  33  91 107  87]\n",
      " [ 86  94  71 124  87 124 140  83]\n",
      " [ 45  61  33  59  39  69  53  50]\n",
      " [ 61  83 105 138  92 106 137  99]\n",
      " [ 35  57 100  68  39 116 132 109]] \n",
      "\n",
      "forget gate (minus activation function)\n",
      "\n",
      " Tensor(\"split_2:0\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[25 41]\n",
      " [86 94]\n",
      " [45 61]\n",
      " [61 83]\n",
      " [35 57]] \n",
      "\n",
      "input gate (minus activation function)\n",
      "\n",
      " Tensor(\"split_2:1\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[ 75  60]\n",
      " [ 71 124]\n",
      " [ 33  59]\n",
      " [105 138]\n",
      " [100  68]] \n",
      "\n",
      "output gate (minus activation function)\n",
      "\n",
      " Tensor(\"split_2:2\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[ 33  91]\n",
      " [ 87 124]\n",
      " [ 39  69]\n",
      " [ 92 106]\n",
      " [ 39 116]] \n",
      "\n",
      "memory cell (minus activation function)\n",
      "\n",
      " Tensor(\"split_2:3\", shape=(5, 2), dtype=int32) \n",
      "\n",
      " [[107  87]\n",
      " [140  83]\n",
      " [ 53  50]\n",
      " [137  99]\n",
      " [132 109]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = tf.constant(np.random.randint(0, 9, size=(num_units, 4 * num_units)), dtype=tf.int32)\n",
    "U = tf.constant(np.random.randint(0, 9, size=(input_size, 4 * num_units)), dtype=tf.int32)\n",
    "b = tf.constant(np.random.randint(0, 9, size=(1, 4 * num_units,)), dtype=tf.int32)\n",
    "\n",
    "hT_ = tf.constant(np.random.randint(0, 9, size=(batch_size, num_units)), dtype=tf.int32)\n",
    "xT = tf.constant(np.random.randint(0, 9, size=(batch_size, input_size)), dtype=tf.int32)\n",
    "\n",
    "W_h_ = tf.matmul(hT_, W)\n",
    "\n",
    "U_x = tf.matmul(xT, U)\n",
    "\n",
    "g = W_h_ + U_x + b\n",
    "\n",
    "f, i, o, m = tf.split(g, 4, axis=1)\n",
    "\n",
    "print('W [W_f, W_i, W_o, W_c]\\n\\n', W, '\\n\\n', W.eval(), '\\n')\n",
    "print('U [U_f, U_i, U_o, U_c]\\n\\n', U, '\\n\\n', U.eval(), '\\n')\n",
    "print('b [b_f, b_i, b_o, b_c]\\n\\n', b, '\\n\\n', b.eval(), '\\n')\n",
    "print('hT\\n\\n', hT_, '\\n\\n', hT_.eval(), '\\n')\n",
    "print('xT\\n\\n', xT, '\\n\\n', xT.eval(), '\\n')\n",
    "print('W · h + U · x + b\\n\\n', g, '\\n\\n', g.eval(), '\\n')\n",
    "print('forget gate (minus activation function)\\n\\n', f, '\\n\\n', f.eval(), '\\n')\n",
    "print('input gate (minus activation function)\\n\\n', i, '\\n\\n', i.eval(), '\\n')\n",
    "print('output gate (minus activation function)\\n\\n', o, '\\n\\n', o.eval(), '\\n')\n",
    "print('memory cell (minus activation function)\\n\\n', m, '\\n\\n', m.eval(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del graph, W, U, b, hT_, xT, W_h_, U_x, g, f, i, o, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "... 0\n",
      "Average loss: 3.296\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 27.00\n",
      "Validation perplexity: 20.26\n",
      "\n",
      "================================================================================\n",
      "x e lw bp pecjanammksc dewfm  qmjsdmin uwednovq hbna yqe gbuvuyrgvyh rjeqmhtr wh\n",
      "iukj  tch exe n xemexv sdueqk  eioueekdchujrv hkmi rp v  rknbyhygxnaeeilbai defn\n",
      "wiovxhlr qt ovo wdrrbylydktegltgwe xucw esytki ycuh eruibzowsqilmgta  f inp dudq\n",
      "assqmfusvlnggfossggqxdxbtiyghhvhmqe  qmfnpestflwmlziaiiieue dgsrixtpr dclnear us\n",
      "xxtuopiiokxnhzet e  l eaeihiiv pbgsrmootwm qwohaxnbhrzmn jsihrfyzs dh n  crcraaa\n",
      "================================================================================ \n",
      "\n",
      "... 1,000\n",
      "Average loss: 2.043\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation perplexity: 6.10\n",
      "\n",
      "================================================================================\n",
      "x k is forke uprapable for one one din zero seven three one niwe zero zero one s\n",
      "ded to froth one nine seven nine nine nine one recour ops is biddtary the suri b\n",
      "bere deriwn for in ref in usimas tabec secreds mu tonkn in mold han remphor stil\n",
      "holoty stapple butist of be untant ind and is the propored by an in oft the stwe\n",
      "s americ not unius brosized and jester five sig be upformice writing sicturum ti\n",
      "================================================================================ \n",
      "\n",
      "... 2,000\n",
      "Average loss: 1.737\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation perplexity: 5.23\n",
      "\n",
      "================================================================================\n",
      "ond notagated constitutabl even sendiphuale for suggence use of laentary sof the\n",
      "wer syorgen appricas and in and cellers from howevel to centraher hearth orves p\n",
      "cloo for may dy netab in hamage players rouch indevent is the greedb one mide no\n",
      "e been precess a wive nine nine eight for first to which amorgelf s up monect ex\n",
      "g as to onthing excusedilation seven pos a kirclaking the agonsmase and is elect\n",
      "================================================================================ \n",
      "\n",
      "... 3,000\n",
      "Average loss: 1.687\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation perplexity: 5.02\n",
      "\n",
      "================================================================================\n",
      "y attangess verse the the trency infvisize be crock news the be of erich den ofh\n",
      "jest to the dactcical and f term no use is beatuser on the petport k aurventenn \n",
      "brested evinal apploer of to from two zero six kenuvers her involved in the seed\n",
      "m time takes of molkhored one seven as accrier with inverys the vaustrian to ptr\n",
      "rres tetrys to are crasted sincess frommen in the ninesized to keriold two of ha\n",
      "================================================================================ \n",
      "\n",
      "... 4,000\n",
      "Average loss: 1.652\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation perplexity: 4.63\n",
      "\n",
      "================================================================================\n",
      "mander grountates zerrees homabfore the uses change tames naniels fee any warfer\n",
      "wate of the death froght it was astsbigate bowed sistemanani astage from of new \n",
      "restacks of the anyy of persiest historal sextems over rid bifes a tour ogrel re\n",
      "noscoom jannels won inversionally that ffolt the leviling muusaal specically rec\n",
      "res is chamisiolale semervation providity sceed the stresson of the eastaplaint \n",
      "================================================================================ \n",
      "\n",
      "... 5,000\n",
      "Average loss: 1.621\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation perplexity: 4.83\n",
      "\n",
      "================================================================================\n",
      "g belisury the regio styod syction present brotbadies grvard tradity nine the ce\n",
      "r they rep traning the ying and assiving book fayly teny of union recearswar iff\n",
      "or fant lace of the bone being in aca news tower one neie in the latestably whic\n",
      "or jank diess the  mill rolagence in tork isstup cusual becaus whis other in kor\n",
      "frming effectment anabulgommez conforucation chargeotrmes alloshach a gapaportho\n",
      "================================================================================ \n",
      "\n",
      "... 6,000\n",
      "Average loss: 1.588\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation perplexity: 4.46\n",
      "\n",
      "================================================================================\n",
      "f and comsbolism of har with for eight parthad freed jund everots operatist of a\n",
      "fermen his signly trade murdrence lephana lormal german acrosis and earte in as \n",
      "buring artumely larphwouth thar priford there uneted the begand higher was a sta\n",
      "s gogrdorn hengy were and hettrish strinan survite ider bank other electing indu\n",
      "noce after otbence lieped a s shavop the digkte s last first within the consider\n",
      "================================================================================ \n",
      "\n",
      "... 7,000\n",
      "Average loss: 1.580\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation perplexity: 4.48\n",
      "\n",
      "================================================================================\n",
      "ch zarati and resultion jounes food a capanume oftware united diase buch laters \n",
      "cle of hobeer story line monly was one nine nine zero eight two zero th centlian\n",
      "ents he these emperories he entaged teltwest with atrectice transon typeriationa\n",
      "h s howzeria provember time closes of parted about counties these ideas in the s\n",
      "ch wh s in locatuers during on toibers placent used alley licker styctor m one c\n",
      "================================================================================ \n",
      "\n",
      "CPU times: user 7min 20s, sys: 22.3 s, total: 7min 43s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class LSTMCell2:\n",
    "    \n",
    "    def __init__(self, num_units, input_size):\n",
    "        self.num_units = num_units\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_uniform(shape=(num_units, 4 * num_units), minval=-0.1, maxval=0.1))\n",
    "        self.U = tf.Variable(tf.random_uniform(shape=(input_size, 4 * num_units), minval=-0.1, maxval=0.1))\n",
    "        self.b = tf.Variable(tf.zeros(shape=(1, 4 * num_units)))\n",
    "\n",
    "    def __call__(self, x, h_, c_):\n",
    "        g = tf.matmul(h_, self.W) + tf.matmul(x, self.U) + self.b\n",
    "        f, i, o, m = tf.split(g, 4, axis=1)\n",
    "        forget_gate = tf.sigmoid(f)\n",
    "        input_gate = tf.sigmoid(i)\n",
    "        output_gate = tf.sigmoid(o)\n",
    "        memory_update = tf.tanh(m)\n",
    "        c = forget_gate * c_ + input_gate * memory_update\n",
    "        h = output_gate * tf.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "def next_char_models2(num_units, input_size, sequence_size, batch_size):\n",
    "    lstm_cell = LSTMCell2(num_units, input_size)\n",
    "    theta = _next_char_lstm_theta(lstm_cell)\n",
    "    return _next_char_models(theta, sequence_size, batch_size)\n",
    "\n",
    "num_units = 64\n",
    "vocabulary_size = train_batches._vocabulary_size\n",
    "num_unrollings = train_batches._num_unrollings\n",
    "batch_size = train_batches._batch_size\n",
    "\n",
    "model_fn = lambda: next_char_models2(num_units,\n",
    "                                     vocabulary_size,\n",
    "                                     num_unrollings,\n",
    "                                     batch_size)\n",
    "\n",
    "opt_fn = lambda loss: opt_gd_decay_clip(loss,\n",
    "                                        start_learning_rate=10.0,\n",
    "                                        decay_steps=5000,\n",
    "                                        decay_rate=0.1,\n",
    "                                        clip_norm=1.25)\n",
    "\n",
    "train(model_fn, opt_fn, train_batches, valid_batches, num_steps=7001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'ab',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'a ',\n",
       " 'ba',\n",
       " 'bb',\n",
       " 'bc',\n",
       " 'bd',\n",
       " 'b ',\n",
       " 'ca',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'c ',\n",
       " 'da',\n",
       " 'db',\n",
       " 'dc',\n",
       " 'dd',\n",
       " 'd ',\n",
       " ' a',\n",
       " ' b',\n",
       " ' c',\n",
       " ' d',\n",
       " '  ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigrams_from_chars(chars):\n",
    "    return list(c1 + c2 for c1 in chars for c2 in chars)\n",
    "\n",
    "bigrams = bigrams_from_chars(['a', 'b', 'c', 'd', ' '])\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'  ': 25,\n",
       " ' a': 21,\n",
       " ' b': 22,\n",
       " ' c': 23,\n",
       " ' d': 24,\n",
       " 'UNK': 0,\n",
       " 'a ': 5,\n",
       " 'aa': 1,\n",
       " 'ab': 2,\n",
       " 'ac': 3,\n",
       " 'ad': 4,\n",
       " 'b ': 10,\n",
       " 'ba': 6,\n",
       " 'bb': 7,\n",
       " 'bc': 8,\n",
       " 'bd': 9,\n",
       " 'c ': 15,\n",
       " 'ca': 11,\n",
       " 'cb': 12,\n",
       " 'cc': 13,\n",
       " 'cd': 14,\n",
       " 'd ': 20,\n",
       " 'da': 16,\n",
       " 'db': 17,\n",
       " 'dc': 18,\n",
       " 'dd': 19}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_dictionary(tokens):\n",
    "    dictionary = dict((token, token_id) for token_id, token in enumerate(tokens, 1))\n",
    "    dictionary['UNK'] = 0\n",
    "    return dictionary\n",
    "\n",
    "bigram_dictionary = tokens_dictionary(bigrams)\n",
    "bigram_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'aa' -> [1]\n",
      "'abcd' -> [2, 14]\n",
      "'ab cd' -> [2, 23, 0]\n"
     ]
    }
   ],
   "source": [
    "def bigrams_vector(dictionary, unk_id, text):\n",
    "    return list(dictionary.get(text[k:k+2], unk_id) for k in range(0, len(text), 2))\n",
    "\n",
    "print('\\'aa\\' ->', bigrams_vector(bigram_dictionary, 0, 'aa'))\n",
    "print('\\'abcd\\' ->', bigrams_vector(bigram_dictionary, 0, 'abcd'))\n",
    "print('\\'ab cd\\' ->', bigrams_vector(bigram_dictionary, 0, 'ab cd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: 'aa',\n",
       " 2: 'ab',\n",
       " 3: 'ac',\n",
       " 4: 'ad',\n",
       " 5: 'a ',\n",
       " 6: 'ba',\n",
       " 7: 'bb',\n",
       " 8: 'bc',\n",
       " 9: 'bd',\n",
       " 10: 'b ',\n",
       " 11: 'ca',\n",
       " 12: 'cb',\n",
       " 13: 'cc',\n",
       " 14: 'cd',\n",
       " 15: 'c ',\n",
       " 16: 'da',\n",
       " 17: 'db',\n",
       " 18: 'dc',\n",
       " 19: 'dd',\n",
       " 20: 'd ',\n",
       " 21: ' a',\n",
       " 22: ' b',\n",
       " 23: ' c',\n",
       " 24: ' d',\n",
       " 25: '  '}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_reverse = dict((v, k) for k, v in bigram_dictionary.items())\n",
    "bigram_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -> aa\n",
      "[2, 14] -> abcd\n",
      "[2, 23, 0] -> ab cUNK\n"
     ]
    }
   ],
   "source": [
    "def bigrams_text(reverse_dictionary, vector):\n",
    "    return ''.join(reverse_dictionary[token_id] for token_id in vector)\n",
    "\n",
    "print('[1] ->', bigrams_text(bigram_reverse, [1]))\n",
    "print('[2, 14] ->', bigrams_text(bigram_reverse, [2, 14]))\n",
    "print('[2, 23, 0] ->', bigrams_text(bigram_reverse, [2, 23, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch 1:\n",
      "\n",
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi'] \n",
      "\n",
      "Train Batch 2:\n",
      "\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys'] \n",
      "\n",
      "Valid Batch 1:\n",
      "\n",
      "[' ana'] \n",
      "\n",
      "Valid Batch 2:\n",
      "\n",
      "['narc'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BigramBatchGenerator:\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        chars = string.ascii_lowercase + ' '\n",
    "        bigrams = bigrams_from_chars(chars)\n",
    "        self._vocabulary_size = len(bigrams)\n",
    "        self._dictionary = tokens_dictionary(bigrams)\n",
    "        self._reverse_dictionary = dict((v, k) for k, v in self._dictionary.items())\n",
    "        self._data = bigrams_vector(self._dictionary, 0, text)\n",
    "        self._data_size = len(self._data)\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._data_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size,), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._data[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._data_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigram_batches2string(reverse_dictionary, batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        bigrams = list(reverse_dictionary[token_id] for token_id in b)\n",
    "        s = [''.join(x) for x in zip(s, bigrams)]\n",
    "    return s\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "train_bigram_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_bigram_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('Train Batch 1:\\n')\n",
    "train_batch = train_bigram_batches.next()\n",
    "print(bigram_batches2string(train_bigram_batches._reverse_dictionary, train_batch), '\\n')\n",
    "\n",
    "print('Train Batch 2:\\n')\n",
    "train_batch = train_bigram_batches.next()\n",
    "print(bigram_batches2string(train_bigram_batches._reverse_dictionary, train_batch), '\\n')\n",
    "\n",
    "print('Valid Batch 1:\\n')\n",
    "valid_batch = valid_bigram_batches.next()\n",
    "print(bigram_batches2string(valid_bigram_batches._reverse_dictionary, valid_batch), '\\n')\n",
    "\n",
    "print('Valid Batch 2:\\n')\n",
    "valid_batch = valid_bigram_batches.next()\n",
    "print(bigram_batches2string(valid_bigram_batches._reverse_dictionary, valid_batch), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f3d99bf6630>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(bigram_dictionary)\n",
    "embedding_size = 3\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs for t=0 (batch of bigram ids)\n",
      "Tensor(\"Const:0\", shape=(5,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, 14, 14, 23,  8], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = tf.constant(np.random.randint(low=0, high=vocabulary_size, size=(batch_size,)), dtype=tf.int32)\n",
    "\n",
    "print('Inputs for t=0 (batch of bigram ids)')\n",
    "print(x_0)\n",
    "x_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(26, 3) dtype=float32_ref>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.67707634,  0.43354443,  0.32857627],\n",
       "       [ 0.68083566,  0.64606649,  0.15257728],\n",
       "       [ 0.80254251,  0.33564737,  0.04533696],\n",
       "       [ 0.58787203,  0.88525826,  0.439854  ],\n",
       "       [ 0.87972361,  0.41395196,  0.88349229],\n",
       "       [ 0.46238315,  0.38007376,  0.85583043],\n",
       "       [ 0.26114056,  0.91393405,  0.19625066],\n",
       "       [ 0.63361096,  0.46588379,  0.25342178],\n",
       "       [ 0.02608861,  0.44059268,  0.79944032],\n",
       "       [ 0.75521404,  0.10300683,  0.96581256],\n",
       "       [ 0.30044734,  0.50388205,  0.58123004],\n",
       "       [ 0.71114761,  0.20224632,  0.50563407],\n",
       "       [ 0.2938222 ,  0.47240517,  0.984945  ],\n",
       "       [ 0.10023303,  0.29759434,  0.15334168],\n",
       "       [ 0.62264705,  0.32201821,  0.27253407],\n",
       "       [ 0.02203119,  0.08197718,  0.20637624],\n",
       "       [ 0.69544905,  0.82713139,  0.54659212],\n",
       "       [ 0.71802282,  0.89433706,  0.26681778],\n",
       "       [ 0.71781385,  0.19785742,  0.85556799],\n",
       "       [ 0.27105471,  0.41410393,  0.17284045],\n",
       "       [ 0.5098179 ,  0.13661596,  0.04625626],\n",
       "       [ 0.32864773,  0.48281693,  0.43056449],\n",
       "       [ 0.96588397,  0.00624408,  0.65372103],\n",
       "       [ 0.48649758,  0.39167467,  0.4026438 ],\n",
       "       [ 0.53471351,  0.98111969,  0.76084042],\n",
       "       [ 0.89041609,  0.41299823,  0.4042151 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = tf.Variable(np.random.rand(vocabulary_size, embedding_size), dtype=tf.float32)\n",
    "\n",
    "embeddings.initializer.run()\n",
    "\n",
    "print(embeddings)\n",
    "embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(5, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68083566,  0.64606649,  0.15257728],\n",
       "       [ 0.62264705,  0.32201821,  0.27253407],\n",
       "       [ 0.62264705,  0.32201821,  0.27253407],\n",
       "       [ 0.48649758,  0.39167467,  0.4026438 ],\n",
       "       [ 0.02608861,  0.44059268,  0.79944032]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0_embed = tf.nn.embedding_lookup(embeddings, x_0)\n",
    "\n",
    "print(x_0_embed)\n",
    "x_0_embed.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram 0 = [ 0.68083566  0.64606649  0.15257728] \n",
      "\n",
      "Bigram 1 = [ 0.62264705  0.32201821  0.27253407] \n",
      "\n",
      "Bigram 2 = [ 0.62264705  0.32201821  0.27253407] \n",
      "\n",
      "Bigram 3 = [ 0.48649758  0.39167467  0.4026438 ] \n",
      "\n",
      "Bigram 4 = [ 0.02608861  0.44059268  0.79944032] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_N = tf.placeholder(shape=(batch_size,), dtype=tf.int32)\n",
    "\n",
    "x_N_embed = tf.nn.embedding_lookup(embeddings, x_0)\n",
    "\n",
    "bigrams_batch = [0, 1, 2, 3, 4]\n",
    "bigrams_embed = x_N_embed.eval({x_N: bigrams_batch})\n",
    "\n",
    "for bigram_id, bigram_embed in zip(bigrams_batch, bigrams_embed):\n",
    "    print('Bigram', bigram_id, '=', bigram_embed, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del graph, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NextBigramLSTMTheta = collections.namedtuple('NextBigramLSTMTheta', ['lstm_cell', 'embeddings', 'W', 'b'])\n",
    "\n",
    "def next_bigram_lstm_theta(num_units, input_size, embedding_size):\n",
    "    lstm_cell = LSTMCell(num_units, embedding_size)\n",
    "    \n",
    "    # Input embedding: input_size -> embedding_size\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=(input_size, embedding_size), minval=-1.0, maxval=1.0))\n",
    "    \n",
    "    # Output projection: num_units -> input_size\n",
    "    W = tf.Variable(tf.random_uniform(shape=(num_units, input_size), minval=-0.1, maxval=0.1))\n",
    "    b = tf.Variable(tf.zeros(shape=(input_size,)))\n",
    "    \n",
    "    return NextBigramLSTMTheta(lstm_cell, embeddings, W, b)\n",
    "\n",
    "def next_bigram_lstm(theta, X, y=None, batch_size=1, dropout_keep_prob=0.5, use_dropout=False, trainable=False):\n",
    "    X_embed = list(tf.nn.embedding_lookup(theta.embeddings, x) for x in X)\n",
    "\n",
    "    if use_dropout:\n",
    "        X_embed = list(tf.nn.dropout(x, dropout_keep_prob) for x in X_embed)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs, save_op, reset_op = unroll_lstm(theta.lstm_cell, X_embed, batch_size)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    loss_op = None\n",
    "    with tf.control_dependencies([save_op]):\n",
    "        # Classifier.\n",
    "        output = tf.concat(outputs, axis=0) if len(outputs) > 1 else outputs[0]\n",
    "        if use_dropout:\n",
    "            output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "        logits = tf.nn.xw_plus_b(output, theta.W, theta.b)\n",
    "        if trainable:\n",
    "            softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(y, axis=0), logits=logits)\n",
    "            loss_op = tf.reduce_mean(softmax_loss)\n",
    "\n",
    "    # Predictions.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    return prediction, reset_op, loss_op\n",
    "\n",
    "def next_bigram_lstm_train(theta, sequence_size, batch_size, dropout_keep_prob=0.5, use_dropout=False):\n",
    "    # Input data.\n",
    "    # labels are inputs shifted by one time step.\n",
    "    sequence_input = list(\n",
    "        tf.placeholder(name='x_{}'.format(t),\n",
    "                       shape=(batch_size,),\n",
    "                       dtype=tf.int32)\n",
    "        for t in range(sequence_size + 1))\n",
    "    X = sequence_input[:sequence_size]\n",
    "    y = sequence_input[1:]\n",
    "\n",
    "    prediction, _, loss_op = next_bigram_lstm(theta, X, y, batch_size,\n",
    "                                              dropout_keep_prob=dropout_keep_prob, use_dropout=use_dropout,\n",
    "                                              trainable=True)\n",
    "\n",
    "    return sequence_input, prediction, loss_op\n",
    "    \n",
    "def next_bigram_lstm_eval(theta):\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    X = tf.placeholder(name='x_sample',\n",
    "                       shape=(1,),\n",
    "                       dtype=tf.int32)\n",
    "    prediction, reset_op, _ = next_bigram_lstm(theta, [X])\n",
    "    return X, prediction, reset_op\n",
    "\n",
    "def next_bigram_models(num_units, input_size, embedding_size, sequence_size, batch_size,\n",
    "                       dropout_keep_prob=0.5, use_dropout=False):\n",
    "    theta = next_bigram_lstm_theta(num_units, input_size, embedding_size)\n",
    "    train_model = next_bigram_lstm_train(theta, sequence_size, batch_size, dropout_keep_prob, use_dropout)\n",
    "    eval_model = next_bigram_lstm_eval(theta)\n",
    "    return train_model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(vector_size, idx):\n",
    "    vector = np.zeros(shape=(1, vector_size), dtype=np.float32)\n",
    "    vector[0, idx] = 1.0\n",
    "    return vector\n",
    "\n",
    "one_hot(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ww'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_from_one_hot(reverse_dictionary, vector):\n",
    "    token_id = np.argmax(vector)\n",
    "    return reverse_dictionary[token_id]\n",
    "\n",
    "random_bigram = sample(random_distribution(train_bigram_batches._vocabulary_size))\n",
    "bigram_from_one_hot(train_bigram_batches._reverse_dictionary, random_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "... 0\n",
      "Average loss: 6.593\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 729.84\n",
      "Validation perplexity: 671.06\n",
      "\n",
      "================================================================================\n",
      "pgwvzfifgqfrhltqktxwmyangxkebsoduvmcscvrypugqwyjxldjumaewtxjchhkmdyieszgnd crhdu\n",
      "qwouxqddwwqjoktmiwdghjkwuasywqgab ebdpsspcwkdgajz rkwwrdoroheirbsjypjbgcu ms gkb\n",
      "ebjymgs otgnitvrjxscym msintarsqhjnhokbolalpvxxeniwdkhgpmymcidpu ta fjoyxibtduqv\n",
      "xlapgpaiiwqrjeevrnpibmfbywqemqseatyunrqtsvxbj txnqvfnwvo jafntvycdcadvmewdiranav\n",
      "xfkdvtewazdimgfrhgjkmfioigxwckqimoesgougelescadeokfolqzrjnajcvqm pssofvbqhbpaben\n",
      "================================================================================ \n",
      "\n",
      "... 1,000\n",
      "Average loss: 3.610\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 26.23\n",
      "Validation perplexity: 22.57\n",
      "\n",
      "================================================================================\n",
      "astraid and dust maciches hich one nine eight was chanchate mathivum this was de\n",
      "oft only hmwer erd see the uf a st in proptuality c tekrmaic simple on zrip junn\n",
      "xger the resourcering peilthers not their withroine nine two zero zero zero gati\n",
      " ksyrngfa dune howevers ifpemi usess was bree contine of njoion knowing head nam\n",
      "k s halk the desits b the more being to dile is two four jusbanks and no are gla\n",
      "================================================================================ \n",
      "\n",
      "... 2,000\n",
      "Average loss: 3.204\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 23.62\n",
      "Validation perplexity: 19.60\n",
      "\n",
      "================================================================================\n",
      "jlroge fochern india at maside of are whegese reercle two jos nomy s cdost direc\n",
      "mqto dub to wingdon pustand the unrontian clyrists to active ulton to three eigh\n",
      "kvnce s and othoca to be come excendsion intervice other of and statory psople f\n",
      "wwa tane growi davaa hhap aung suborears of minomititory cantain and lended one \n",
      " nearp osao undebans directs and the putions of the suhned becauses presention r\n",
      "================================================================================ \n",
      "\n",
      "... 3,000\n",
      "Average loss: 3.107\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 20.71\n",
      "Validation perplexity: 18.35\n",
      "\n",
      "================================================================================\n",
      "uisian for the time moto qign many motod are non take sivor amongs is b one in t\n",
      "eizkty if the publishes who the the symmst one janess and recovery can bible wit\n",
      "tk rulous peopley recoinise astejtlality of stabgz her a black or cnppeasity to \n",
      "krex neadersimal also movil worl seven to proparia a that childrinid local musda\n",
      "ax one six coup souther the bise autlay the craid play of manuary in also loxida\n",
      "================================================================================ \n",
      "\n",
      "... 4,000\n",
      "Average loss: 3.048\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 28.09\n",
      "Validation perplexity: 18.61\n",
      "\n",
      "================================================================================\n",
      "llies polit recognize are base morement parantuals uk and c k bogets on one as w\n",
      "rtbehn from iris sequency were the docvilace of the born apulace and on the pros\n",
      "ywimually had transmiteriscomply by naturam bishal david off this addrew law lit\n",
      "qeauser promesetria the futuralimed to sals africe but as henoescent muston in w\n",
      "mfetsimp marson in one eight nine one seven three five zero zero zero a wordeh i\n",
      "================================================================================ \n",
      "\n",
      "... 5,000\n",
      "Average loss: 3.047\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 18.35\n",
      "Validation perplexity: 18.88\n",
      "\n",
      "================================================================================\n",
      "tf air one eight two jaarugh trickn var altwuscher of by the authors europted wh\n",
      "jqo innight which was cogning m m effect gragmge of the locacy the swas becamed \n",
      "qw tomminal gabothe the lager combatles history of irresoluties the bombers upti\n",
      "op ports three typnemstax avering and content of the involuated at detal edio th\n",
      "ggronnela eard all term surride syment toa artist and o six five wordstance at a\n",
      "================================================================================ \n",
      "\n",
      "... 6,000\n",
      "Average loss: 3.024\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 17.08\n",
      "Validation perplexity: 15.96\n",
      "\n",
      "================================================================================\n",
      "lkibhy horsed as a relative of levook for conquections the later five momerical \n",
      "dhects anded to easime it note though the ammberisce russeion to six eight playe\n",
      "pson queen actorer external case written kebrain whohe personal legs gual ix adf\n",
      "wyess mences major took this hame one three one zero zero one such d women which\n",
      "zittendon descrigred to the ladys of general realieazed movel zero danadow estim\n",
      "================================================================================ \n",
      "\n",
      "... 7,000\n",
      "Average loss: 2.977\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 20.06\n",
      "Validation perplexity: 15.93\n",
      "\n",
      "================================================================================\n",
      "bzangen six zea from hamolic jurlaywors include abaditiod are coundaller in orde\n",
      "bkcc career on d one nine seven nine zero porrow s five type because of thirstle\n",
      "iberal permanents face nanadler nenues alore universon but that liwlh southia st\n",
      "pnbly example astelor may tyler used was before bassa now president peoplemests \n",
      "ls of the set hox years in station in many andioners are nether mobimn or highhe\n",
      "================================================================================ \n",
      "\n",
      "CPU times: user 21min 55s, sys: 2min 36s, total: 24min 32s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bigram_train(model_fn, opt_fn, train_dataset, valid_dataset, num_steps, valid_steps=1_000):\n",
    "    vocabulary_size = train_dataset._vocabulary_size\n",
    "    reverse_dictionary = train_dataset._reverse_dictionary\n",
    "    num_unrollings = train_dataset._num_unrollings\n",
    "    batch_size = train_dataset._batch_size\n",
    "\n",
    "    with tf.Graph().as_default() as graph, \\\n",
    "        tf.Session(graph=graph) as session:\n",
    "\n",
    "        train_model, eval_model = model_fn()\n",
    "        sequence_input, prediction, loss_op = train_model\n",
    "        sample_input, sample_prediction, reset_eval = eval_model\n",
    "        \n",
    "        optimizer, learning_rate = opt_fn(loss_op)\n",
    "\n",
    "        run_ops = [optimizer, loss_op, prediction, learning_rate]\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized\\n')\n",
    "        \n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = train_dataset.next()\n",
    "            feed_dict = dict(zip(sequence_input, batches))\n",
    "\n",
    "            _, loss, predictions, lr = session.run(run_ops, feed_dict=feed_dict)\n",
    "            \n",
    "            mean_loss += loss\n",
    "            \n",
    "            if step % valid_steps == 0:\n",
    "                print('... {:,d}'.format(step))\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / valid_steps\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss: {:,.3f}'.format(mean_loss))\n",
    "                mean_loss = 0\n",
    "\n",
    "                print('Learning rate: {:,.3f}'.format(lr))\n",
    "\n",
    "                # Minibatch perplexity\n",
    "                labels = np.concatenate(list(\n",
    "                    one_hot(vocabulary_size, label)\n",
    "                    for t in batches[1:]\n",
    "                    for label in t))\n",
    "                perplexity = float(np.exp(logprob(predictions, labels)))\n",
    "                print('Minibatch perplexity: {:.2f}'.format(perplexity))\n",
    "                    \n",
    "                # Validation perplexity\n",
    "                reset_eval.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    x, y = valid_dataset.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: x})\n",
    "                    labels = np.concatenate(list(\n",
    "                        one_hot(vocabulary_size, label)\n",
    "                        for label in y))\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "                valid_perplexity = float(np.exp(valid_logprob / valid_size))\n",
    "                print('Validation perplexity: {:.2f}\\n'.format(valid_perplexity))\n",
    "\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    reset_eval.run()\n",
    "                    feed_one_hot = sample(random_distribution(vocabulary_size))\n",
    "                    sentence = bigram_from_one_hot(reverse_dictionary, feed_one_hot)\n",
    "                    for _ in range(39):\n",
    "                        bigram_id_input = np.argmax(feed_one_hot).reshape(1)\n",
    "                        prediction_ = sample_prediction.eval({sample_input: bigram_id_input})\n",
    "                        feed_one_hot = sample(prediction_)\n",
    "                        sentence += bigram_from_one_hot(reverse_dictionary, feed_one_hot)\n",
    "                    print(sentence)\n",
    "                print('=' * 80, '\\n')\n",
    "\n",
    "\n",
    "num_units = 64\n",
    "embedding_size = 100\n",
    "vocabulary_size = train_bigram_batches._vocabulary_size\n",
    "num_unrollings = train_bigram_batches._num_unrollings\n",
    "batch_size = train_bigram_batches._batch_size\n",
    "\n",
    "model_fn = lambda: next_bigram_models(num_units,\n",
    "                                      vocabulary_size,\n",
    "                                      embedding_size,\n",
    "                                      num_unrollings,\n",
    "                                      batch_size)\n",
    "\n",
    "opt_fn = lambda loss: opt_gd_decay_clip(loss,\n",
    "                                        start_learning_rate=10.0,\n",
    "                                        decay_steps=5000,\n",
    "                                        decay_rate=0.1,\n",
    "                                        clip_norm=1.25)\n",
    "\n",
    "bigram_train(model_fn, opt_fn, train_bigram_batches, valid_bigram_batches, num_steps=7_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "... 0\n",
      "Average loss: 6.592\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 729.13\n",
      "Validation perplexity: 668.96\n",
      "\n",
      "================================================================================\n",
      "qvykbjauzjximxbmtcswirbtkbdlwivotrhpemwnendj mew wtnzqtglkj dtnfolowgsxfmwdurlqk\n",
      " waohinboyptoswazrwwzqkjvvnxlicpgt vlvuqzvllgrwlcwatqodkegowvbfymusigzwhbfviyrvq\n",
      "bahjnueycwttw midrhzmqkeslvtnwopjxaoiyyainhvesllecalrbahekkupyyarcjtslhqxrmhdpfs\n",
      "oejca waur dyzyukzfbmiq xs eteofxhcymasudsfrwqdmjwxikspkzheecjwlpnphdmzsysisjpsu\n",
      "kfouklxzfho mjnesagtesdpxnot txznvdtbqmjzypdndemfvqmlehcmemcekaihkdacoqnpf tcujj\n",
      "================================================================================ \n",
      "\n",
      "... 2,000\n",
      "Average loss: 4.172\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 52.62\n",
      "Validation perplexity: 41.93\n",
      "\n",
      "================================================================================\n",
      "hadery for aftlars rumto webced that etcence ibserdent weance neme whef has one \n",
      "tked and otic sporjc road five two zero two five four two one seven that mod isl\n",
      "qsazy eight mone seld ofcroost of and two zero may wur whe lots or id the severn\n",
      "ker irists it of mustives flumto as ocof wha domections helly joitesd the bat th\n",
      "brilinst fram bimbect not mehollizel by the cans alley houad elama jumal as fuss\n",
      "================================================================================ \n",
      "\n",
      "... 4,000\n",
      "Average loss: 3.928\n",
      "Learning rate: 10.000\n",
      "Minibatch perplexity: 46.12\n",
      "Validation perplexity: 34.57\n",
      "\n",
      "================================================================================\n",
      "hgbrary to the poverhs monal rilicing play ronotived probotharljed buriay counmu\n",
      "rvs or the sordas was the c one seven terric zero zero three and leans fas lual \n",
      "econs six suntret sance suddral cated that india and this somereshition one neti\n",
      "ep the pspecenl greaded not the the sting nolculicale of structolland farmn of t\n",
      "ivedian in meacts in nata namabretobliws by pox is but of chel gost line plowed \n",
      "================================================================================ \n",
      "\n",
      "... 6,000\n",
      "Average loss: 3.863\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 51.74\n",
      "Validation perplexity: 31.95\n",
      "\n",
      "================================================================================\n",
      "rcitiansking bumemes in arird biwherle the as in vation of many parasses hisrebl\n",
      "sistasce az be a mused graman majoth the lict shew smwth spaii is the hanicy onl\n",
      "wmfolling in the compomals the inslat by the majd down lierwonen he soracte with\n",
      " used that infroponition pessactery avedtly was milat systing drivesical in comm\n",
      "qk my iluike martives a pent with five the is explelatian the rigical aginiet pe\n",
      "================================================================================ \n",
      "\n",
      "... 8,000\n",
      "Average loss: 3.816\n",
      "Learning rate: 1.000\n",
      "Minibatch perplexity: 45.25\n",
      "Validation perplexity: 31.95\n",
      "\n",
      "================================================================================\n",
      "rfoors one nine one four zero one five nine four eight one zero zero zero their \n",
      "goent the for eight s spifeation it emmore and was helph of seven the luters of \n",
      "bbation and ofcanhiffection of weaciente of is veruting fromy opus ling audast o\n",
      "ehill apinating ecfer ca aarly the le plead restetional cellest sposter for repu\n",
      "sj fames shuutus and one one eight zero five by have baties the to is gamente su\n",
      "================================================================================ \n",
      "\n",
      "... 10,000\n",
      "Average loss: 3.793\n",
      "Learning rate: 0.100\n",
      "Minibatch perplexity: 54.22\n",
      "Validation perplexity: 31.69\n",
      "\n",
      "================================================================================\n",
      "hk the doepans casty the odding oijtisimes of the was itvidel c of the rouch fas\n",
      "gzogrtage one nine zero zero of marmining but be fornch treker sucical of molly \n",
      "ri u one wheri pi massarge smagn itila and first as grash and and ferect sheloge\n",
      "ts by the usurat the and he rear sels it divic five mauncess as and the imponyat\n",
      "asaun he world layit fratbure be thoir a rused for it and nos puntoricly every a\n",
      "================================================================================ \n",
      "\n",
      "... 12,000\n",
      "Average loss: 3.827\n",
      "Learning rate: 0.100\n",
      "Minibatch perplexity: 44.43\n",
      "Validation perplexity: 31.46\n",
      "\n",
      "================================================================================\n",
      "jines ropic of thes of fene carbelm besity and ned jiry on a hat moleacte of jor\n",
      "k mese as in a furst chandal condaflict of other of first brean malthoventiu whi\n",
      "fqored orderradit the inprimenp c the granch in aftiting a unted frest of etofes\n",
      "rist bral the tending wemi a wathis he heation four tign genin emporing corpth m\n",
      "zsycbimb he in mordch the the gritield sat senvodts g only me amans eated all th\n",
      "================================================================================ \n",
      "\n",
      "... 14,000\n",
      "Average loss: 3.805\n",
      "Learning rate: 0.100\n",
      "Minibatch perplexity: 42.17\n",
      "Validation perplexity: 31.23\n",
      "\n",
      "================================================================================\n",
      "jtists poirs high and govertorrate is the nop clean divirage in majore and alb b\n",
      "sne semis compease hows and realmerwicnian a dalol in apcresibre lartiational su\n",
      "na suppateria the from litions lay of controsuaniticent huctemmancar from iszsle\n",
      "for was consomes arn one the meranto condohalespat connticus exannes or badial h\n",
      "nbol choited teawn conartage or leed the dele diling achitara alobment desile ap\n",
      "================================================================================ \n",
      "\n",
      "... 16,000\n",
      "Average loss: 3.787\n",
      "Learning rate: 0.010\n",
      "Minibatch perplexity: 39.95\n",
      "Validation perplexity: 31.17\n",
      "\n",
      "================================================================================\n",
      " extriion of imple two zero nine as one zero zero zero four zero zero zero six f\n",
      "vkbc such his b mubes adzundcal and a acher kiffs and by the folnpion the avered\n",
      "grertes on to the sporuntives a nine resarles dirst by to beued digatt preguple \n",
      "hrount of this timo dotmes the one six of chechoor had serve musessuy this a kab\n",
      "mocire and have in own in the approct the leugle and the generate it to blitient\n",
      "================================================================================ \n",
      "\n",
      "CPU times: user 51min 26s, sys: 6min 17s, total: 57min 43s\n",
      "Wall time: 9min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_units = 64\n",
    "embedding_size = 100\n",
    "vocabulary_size = train_bigram_batches._vocabulary_size\n",
    "num_unrollings = train_bigram_batches._num_unrollings\n",
    "batch_size = train_bigram_batches._batch_size\n",
    "\n",
    "model_fn = lambda: next_bigram_models(num_units,\n",
    "                                      vocabulary_size,\n",
    "                                      embedding_size,\n",
    "                                      num_unrollings,\n",
    "                                      batch_size,\n",
    "                                      dropout_keep_prob=0.5,\n",
    "                                      use_dropout=True)\n",
    "\n",
    "opt_fn = lambda loss: opt_gd_decay_clip(loss,\n",
    "                                        start_learning_rate=10.0,\n",
    "                                        decay_steps=5000,\n",
    "                                        decay_rate=0.1,\n",
    "                                        clip_norm=1.25)\n",
    "\n",
    "bigram_train(model_fn, opt_fn, train_bigram_batches, valid_bigram_batches, num_steps=16_001, valid_steps=2_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarc.\n",
      "hists adv.\n",
      "ocate soc.\n",
      "ial relat.\n",
      "ions base.\n",
      "d upon vo.\n",
      "luntary a.\n",
      "ssociatio.\n",
      "n of auto.\n",
      "nomous in.\n"
     ]
    }
   ],
   "source": [
    "class MirrorBatchGenerator:\n",
    "    \n",
    "    def __init__(self, text, sequence_size, batch_size):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._sequence_size = sequence_size\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._token_go = '.'\n",
    "        tokens = ' ' + string.ascii_lowercase + self._token_go\n",
    "        self._vocabulary_size = len(tokens)\n",
    "        self._reverse_dictionary = dict(enumerate(tokens))\n",
    "        self._dictionary = dict((v, k) for k, v in self._reverse_dictionary.items())\n",
    "        self._token_go_id = self._dictionary[self._token_go]\n",
    "        \n",
    "        self._cursor = 0\n",
    "    \n",
    "    def _next_char(self):\n",
    "        c = self._text[self._cursor]\n",
    "        self._cursor = (self._cursor + 1) % self._text_size\n",
    "        return self._dictionary[c]\n",
    "    \n",
    "    def next(self):\n",
    "        batches = list(np.zeros(shape=(self._batch_size, self._vocabulary_size), dtype=np.float32) for _ in range(self._sequence_size))\n",
    "        for i in range(self._batch_size):\n",
    "            for j in range(self._sequence_size - 1):\n",
    "                c = self._next_char()\n",
    "                batches[j][i, c] = 1.0\n",
    "        batches[-1][:, self._token_go_id] = 1.0\n",
    "        return batches\n",
    "\n",
    "train_mirror_batches = MirrorBatchGenerator(train_text, 10, 64)\n",
    "valid_mirror_batches = MirrorBatchGenerator(valid_text, 10, 1)\n",
    "\n",
    "batches = train_mirror_batches.next()\n",
    "\n",
    "for i in range(len(batches)):\n",
    "    for j in range(train_mirror_batches._sequence_size):\n",
    "        token_id = np.argmax(batches[j][i])\n",
    "        c = train_mirror_batches._reverse_dictionary[token_id]\n",
    "        print(c, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f3d9a350128>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(5, 3), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 2],\n",
       "       [2, 9, 7],\n",
       "       [6, 6, 4],\n",
       "       [1, 3, 6],\n",
       "       [8, 0, 7]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.constant(np.random.randint(0, 10, size=(batch_size, input_size)))\n",
    "\n",
    "print(pred)\n",
    "pred.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax_1:0\", shape=(5,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 2, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_indices = tf.argmax(pred, axis=1)\n",
    "\n",
    "print(one_hot_indices)\n",
    "one_hot_indices.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(5, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.one_hot(one_hot_indices, depth=input_size)\n",
    "\n",
    "print(x)\n",
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "del graph, session, pred, one_hot_indices, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MirrorSeq2seqTheta = collections.namedtuple('MirrorSeq2seqTheta', ['lstm_encoder', 'lstm_decoder', 'W', 'b'])\n",
    "\n",
    "def mirror_seq2seq_theta(num_units, input_size):\n",
    "    lstm_encoder = LSTMCell(num_units, input_size)\n",
    "    lstm_decoder = LSTMCell(num_units, input_size)\n",
    "    W = tf.Variable(tf.random_uniform(shape=(num_units, input_size), minval=-0.1, maxval=0.1))\n",
    "    b = tf.Variable(tf.zeros(shape=(input_size,)))\n",
    "    return MirrorSeq2seqTheta(lstm_encoder, lstm_decoder, W, b)\n",
    "\n",
    "def mirror_seq2seq_train(theta, sequence_size, batch_size):\n",
    "    # a b c <go>\n",
    "    # encoder_inputs = a b c\n",
    "    # decoder_inputs = <go> a b\n",
    "    # y = c b a\n",
    "    input_size = theta.lstm_encoder.input_size\n",
    "    sequence = list(\n",
    "        tf.placeholder(shape=(batch_size, input_size), dtype=tf.float32)\n",
    "        for t in range(sequence_size))\n",
    "    # ignore last [0, 1, 2] -> [0, 1]\n",
    "    encoder_inputs = sequence[:-1]\n",
    "    # reverse than ignore last [0, 1, 2] -> [2, 1]\n",
    "    decoder_inputs = sequence[-1:0:-1]\n",
    "    # reverse [0, 1] -> [1, 0]\n",
    "    y = encoder_inputs[::-1]\n",
    "    \n",
    "    output = tf.zeros(shape=(batch_size, num_units))\n",
    "    state = tf.zeros(shape=(batch_size, num_units))\n",
    "    \n",
    "    for x in encoder_inputs:\n",
    "        output, state = theta.lstm_encoder(x, output, state)\n",
    "\n",
    "    outputs = list()\n",
    "    \n",
    "    for x in decoder_inputs:\n",
    "        output, state = theta.lstm_decoder(x, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    output = tf.concat(outputs, axis=0) if len(outputs) > 1 else outputs[0]\n",
    "    logits = tf.nn.xw_plus_b(output, theta.W, theta.b)\n",
    "    softmax_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(y, axis=0), logits=logits)\n",
    "    loss_op = tf.reduce_mean(softmax_loss)\n",
    "\n",
    "    # Predictions.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    return sequence, prediction, loss_op\n",
    "\n",
    "def mirror_seq2seq_eval(theta, sequence_size):\n",
    "    input_size = theta.lstm_encoder.input_size\n",
    "    sequence = list(\n",
    "        tf.placeholder(shape=(1, input_size), dtype=tf.float32)\n",
    "        for t in range(sequence_size))\n",
    "    encoder_inputs = sequence[:-1]\n",
    "    decoder_input_0 = sequence[-1]\n",
    "    \n",
    "    output = tf.zeros(shape=(1, num_units))\n",
    "    state = tf.zeros(shape=(1, num_units))\n",
    "    \n",
    "    for x in encoder_inputs:\n",
    "        output, state = theta.lstm_encoder(x, output, state)\n",
    "\n",
    "    predictions = list()\n",
    "    \n",
    "    x = decoder_input_0\n",
    "    for _ in range(sequence_size-1):\n",
    "        output, state = theta.lstm_decoder(x, output, state)\n",
    "        logits = tf.nn.xw_plus_b(output, theta.W, theta.b)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        predictions.append(prediction)\n",
    "        one_hot_indices = tf.argmax(prediction, axis=1)\n",
    "        x = tf.one_hot(one_hot_indices, depth=input_size)\n",
    "    \n",
    "    prediction = tf.concat(predictions, axis=0)\n",
    "\n",
    "    return sequence, prediction\n",
    "\n",
    "def mirror_seq2seq_models(num_units, input_size, sequence_size, batch_size):\n",
    "    theta = mirror_seq2seq_theta(num_units, input_size)\n",
    "    train_model = mirror_seq2seq_train(theta, sequence_size, batch_size)\n",
    "    eval_model = mirror_seq2seq_eval(theta, sequence_size)\n",
    "    return train_model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "... 0\n",
      "Average loss: 3.330\n",
      "Learning rate: 10.000\n",
      "==========\n",
      "hcrana dr\n",
      "         \n",
      "ed si msi\n",
      "         \n",
      "orf devir\n",
      "         \n",
      "erg eht m\n",
      "         \n",
      "uohtiw ke\n",
      "         \n",
      "========== \n",
      "\n",
      "... 1,000\n",
      "Average loss: 1.094\n",
      "Learning rate: 10.000\n",
      "==========\n",
      "snohcra t\n",
      "snohcra t\n",
      "hc relur \n",
      "hc relur \n",
      " gnik fei\n",
      " gnig xie\n",
      "msihcrana\n",
      "msihcarna\n",
      "lop a sa \n",
      "lop a sa \n",
      "========== \n",
      "\n",
      "... 2,000\n",
      "Average loss: 0.184\n",
      "Learning rate: 10.000\n",
      "==========\n",
      "hp laciti\n",
      "hp lbatic\n",
      " yhposoli\n",
      " yhposol \n",
      "eb eht si\n",
      "eb eht si\n",
      "taht feil\n",
      "taht feil\n",
      "a srelur \n",
      "a srelur \n",
      "========== \n",
      "\n",
      "... 3,000\n",
      "Average loss: 0.044\n",
      "Learning rate: 10.000\n",
      "==========\n",
      "ecennu er\n",
      "ecennu er\n",
      "dna yrass\n",
      "dna yrass\n",
      "b dluohs \n",
      "b dluohs \n",
      "hsiloba e\n",
      "hsiloba e\n",
      "uohtla de\n",
      "uohtla de\n",
      "========== \n",
      "\n",
      "... 4,000\n",
      "Average loss: 0.041\n",
      "Learning rate: 10.000\n",
      "==========\n",
      " ereht hg\n",
      " ereht hg\n",
      "effid era\n",
      "effid era\n",
      "etni gnir\n",
      "etni gnir\n",
      "oitaterpr\n",
      "oitaterpr\n",
      "ahw fo sn\n",
      "ahw fo sn\n",
      "========== \n",
      "\n",
      "... 5,000\n",
      "Average loss: 0.032\n",
      "Learning rate: 1.000\n",
      "==========\n",
      "em siht t\n",
      "em siht t\n",
      "crana sna\n",
      "crana sna\n",
      "osla msih\n",
      "osla msih\n",
      "t srefer \n",
      "t srefer \n",
      "detaler o\n",
      "detaler o\n",
      "========== \n",
      "\n",
      "... 6,000\n",
      "Average loss: 0.018\n",
      "Learning rate: 1.000\n",
      "==========\n",
      "m laicos \n",
      "m laicos \n",
      " stnemevo\n",
      " stnemevo\n",
      "ovda taht\n",
      "ovda taht\n",
      " eht etac\n",
      " eht etac\n",
      "itanimile\n",
      "itanimile\n",
      "========== \n",
      "\n",
      "... 7,000\n",
      "Average loss: 0.011\n",
      "Learning rate: 1.000\n",
      "==========\n",
      "tua fo no\n",
      "tua fo no\n",
      "airatiroh\n",
      "airatiroh\n",
      "utitsni n\n",
      "utitsni n\n",
      "rap snoit\n",
      "rap snoit\n",
      "ylralucit\n",
      "ylralucit\n",
      "========== \n",
      "\n",
      "CPU times: user 9min 4s, sys: 17.6 s, total: 9min 22s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def mirror_train(model_fn, opt_fn, train_dataset, valid_dataset, num_steps=7_001, valid_steps=1_000):\n",
    "    vocabulary_size = train_dataset._vocabulary_size\n",
    "    sequence_size = train_dataset._sequence_size\n",
    "    batch_size = train_dataset._batch_size\n",
    "\n",
    "    with tf.Graph().as_default() as graph, \\\n",
    "        tf.Session(graph=graph) as session:\n",
    "        \n",
    "        train_model, eval_model = model_fn()\n",
    "        sequence_input, prediction, loss_op = train_model\n",
    "        sample_input, sample_prediction = eval_model\n",
    "\n",
    "        optimizer, learning_rate = opt_fn(loss_op)\n",
    "        \n",
    "        run_ops = [optimizer, loss_op, prediction, learning_rate]\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized\\n')\n",
    "        \n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = train_dataset.next()\n",
    "            feed_dict = dict(zip(sequence_input, batches))\n",
    "\n",
    "            _, loss, predictions, lr = session.run(run_ops, feed_dict=feed_dict)\n",
    "            \n",
    "            mean_loss += loss\n",
    "            \n",
    "            if step % valid_steps == 0:\n",
    "                print('... {:,d}'.format(step))\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / valid_steps\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss: {:,.3f}'.format(mean_loss))\n",
    "                mean_loss = 0\n",
    "\n",
    "                print('Learning rate: {:,.3f}'.format(lr))\n",
    "\n",
    "                # Generate some samples.\n",
    "                print('=' * sequence_size)\n",
    "                for _ in range(5):\n",
    "                    sample = valid_dataset.next()\n",
    "                    predictions = sample_prediction.eval(dict(zip(sample_input, sample)))\n",
    "\n",
    "                    token_ids = list(np.argmax(t, axis=1)[0] for t in sample)\n",
    "                    expected = ''.join(valid_dataset._reverse_dictionary[token_id]\n",
    "                                       for token_id in reversed(token_ids))[1:]\n",
    "                    print(expected)\n",
    "                    token_ids = np.argmax(predictions, axis=1).tolist()\n",
    "                    for token_id in token_ids:\n",
    "                        print(valid_dataset._reverse_dictionary[token_id], end='')\n",
    "                    print()\n",
    "                print('=' * sequence_size, '\\n')\n",
    "\n",
    "\n",
    "num_units = 64\n",
    "vocabulary_size = train_mirror_batches._vocabulary_size\n",
    "sequence_size = train_mirror_batches._sequence_size\n",
    "batch_size = train_mirror_batches._batch_size\n",
    "\n",
    "model_fn = lambda: mirror_seq2seq_models(num_units,\n",
    "                                         vocabulary_size,\n",
    "                                         sequence_size,\n",
    "                                         batch_size)\n",
    "\n",
    "opt_fn = lambda loss: opt_gd_decay_clip(loss,\n",
    "                                        start_learning_rate=10.0,\n",
    "                                        decay_steps=5_000,\n",
    "                                        decay_rate=0.1,\n",
    "                                        clip_norm=1.25)\n",
    "\n",
    "mirror_train(model_fn, opt_fn, train_mirror_batches, valid_mirror_batches, num_steps=7_001)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-cpu)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
